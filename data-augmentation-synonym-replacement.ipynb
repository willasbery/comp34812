{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "from gensim.downloader import load as glove_embeddings_loader\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path configuration\n",
    "DATA_DIR = Path('./data')\n",
    "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
    "DEV_PATH = DATA_DIR / 'dev.csv'\n",
    "AUGMENTED_DATA_PATH = DATA_DIR / 'train_augmented.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 18:40:51 - loading projection weights from C:\\Users\\willi/gensim-data\\glove-wiki-gigaword-300\\glove-wiki-gigaword-300.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 18:41:56 - KeyedVectors lifecycle event {'msg': 'loaded (400000, 300) matrix of type float32 from C:\\\\Users\\\\willi/gensim-data\\\\glove-wiki-gigaword-300\\\\glove-wiki-gigaword-300.gz', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-03-27T18:41:56.186043', 'gensim': '4.3.3', 'python': '3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "glove_embeddings = glove_embeddings_loader('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word: str, noise_level: float=0.005, topn: int=10):\n",
    "    # Check if the word exists in the embeddings\n",
    "    if word not in glove_embeddings:\n",
    "        return []\n",
    "    \n",
    "    # Get the word's embedding vector\n",
    "    original_vec = glove_embeddings[word]\n",
    "    \n",
    "    # Add random Gaussian noise to the vector\n",
    "    noise = np.random.normal(loc=0.0, scale=noise_level, size=original_vec.shape)\n",
    "    noisy_vec = original_vec + noise\n",
    "    \n",
    "    # Retrieve the topn + 1 words (as one of them will be the word itself) closest to the noisy vector\n",
    "    # This will return a list of tuples (word, similarity)\n",
    "    similar_words = glove_embeddings.most_similar(positive=[noisy_vec], topn=topn + 1)\n",
    "    \n",
    "    # Return just the words from the list of tuples\n",
    "    return [syn for syn, similarity in similar_words if syn != word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove any non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # Split hyphenated words\n",
    "    text = re.sub(r'-', ' ', text)\n",
    "    \n",
    "    # Remove any double spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    return ' '.join([word for word in text.split() if word not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "augmented_df = pd.read_csv(AUGMENTED_DATA_PATH)\n",
    "dev_df = pd.read_csv(DEV_PATH)\n",
    "\n",
    "train_df['POS'] = train_df['Evidence'].apply(lambda x: nltk.pos_tag(nltk.word_tokenize(x)))\n",
    "original_evidences_pos = train_df['POS'].tolist()\n",
    "original_evidences = train_df['Evidence'].tolist()\n",
    "\n",
    "preprocessed_evidences = train_df['Evidence'].apply(remove_stopwords).tolist()\n",
    "corresponding_claim = train_df['Claim'].apply(remove_stopwords).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_evidence_words(claim_words: set, \n",
    "                           evidence_words: list, \n",
    "                           original_pos_tags: dict) -> list:\n",
    "    \"\"\"\n",
    "    Filter evidence words to find potential replacement candidates.\n",
    "    \n",
    "    Args:\n",
    "        claim_words (set): Set of words in the claim.\n",
    "        evidence_words (list): List of words in the evidence.\n",
    "        original_pos_tags (dict): Dictionary of POS tags for the original evidence.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of potential replacement candidates.\n",
    "    \"\"\"\n",
    "    common_words = set(evidence_words) & claim_words\n",
    "    potential_replacements = []\n",
    "    \n",
    "    for word in evidence_words:\n",
    "        # Skip if word is:\n",
    "        # 1. Common between claim and evidence\n",
    "        # 2. Substring of any claim word or vice versa\n",
    "        # 3. Not in POS tags dictionary\n",
    "        if (word in common_words or\n",
    "            any(word in claim_word or claim_word in word for claim_word in claim_words) or\n",
    "            word not in original_pos_tags):\n",
    "            continue\n",
    "        potential_replacements.append(word)\n",
    "    \n",
    "    return potential_replacements\n",
    "\n",
    "def find_valid_replacements(word_to_replace: str, \n",
    "                            synonyms: list, \n",
    "                            original_evidence: str, \n",
    "                            original_pos_tags: dict) -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Find a valid synonym replacement that maintains POS tag.\n",
    "    \n",
    "    Args:\n",
    "        word_to_replace (str): The word to replace.\n",
    "        synonyms (list): List of synonyms to choose from.\n",
    "        original_evidence (str): The original evidence.\n",
    "        original_pos_tags_dict (dict): Dictionary of POS tags for the original evidence.\n",
    "        \n",
    "    Returns:\n",
    "        tuple[bool, str]: A tuple containing a boolean indicating if a valid replacement was found and the replacement word.\n",
    "    \"\"\"\n",
    "    for synonym in synonyms:\n",
    "        # Replace word in evidence\n",
    "        pattern = r'\\b' + re.escape(word_to_replace) + r'\\b'\n",
    "        new_evidence = re.sub(pattern, synonym, original_evidence)\n",
    "        \n",
    "        # Get POS tags for new evidence\n",
    "        new_evidence_pos = nltk.pos_tag(nltk.word_tokenize(new_evidence))\n",
    "        new_evidence_pos_dict = {word.lower(): [] for word, _ in new_evidence_pos}\n",
    "        for word, tag in new_evidence_pos:\n",
    "            new_evidence_pos_dict[word.lower()].append(tag)\n",
    "        \n",
    "        # Check if POS tags match\n",
    "        if (word_to_replace in original_pos_tags and \n",
    "            synonym.lower() in new_evidence_pos_dict and \n",
    "            original_pos_tags[word_to_replace] == new_evidence_pos_dict[synonym.lower()]):\n",
    "            return True, synonym\n",
    "            \n",
    "    return False, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4456it [12:40,  5.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 103\u001b[39m\n\u001b[32m    100\u001b[39m         synyonm_replaced_df.to_csv(file_name, index=\u001b[38;5;28;01mFalse\u001b[39;00m, mode=mode, header=header)\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Run the augmentation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[43maugment_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m             \u001b[49m\u001b[43mpreprocessed_evidences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m             \u001b[49m\u001b[43mcorresponding_claim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m             \u001b[49m\u001b[43moriginal_evidences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m             \u001b[49m\u001b[43moriginal_evidences_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m             \u001b[49m\u001b[43madd_original_evidence\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m             \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/train_augmented_synonyms_with_original_evidence.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36maugment_data\u001b[39m\u001b[34m(train_df, preprocessed_evidences, corresponding_claim, original_evidences, original_pos_tags, file_name, add_original_evidence, batch_size)\u001b[39m\n\u001b[32m     51\u001b[39m final_word_replacement_map = {}\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words_to_replace:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     synonyms = \u001b[43mget_synonyms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_level\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     found, synonym = find_valid_replacements(\n\u001b[32m     56\u001b[39m         word, synonyms, original_evidences[idx], pos_tags_dict\n\u001b[32m     57\u001b[39m     )\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m found:\n\u001b[32m     59\u001b[39m         \u001b[38;5;66;03m# print(f\"Replacement found: {word} -> {synonym}\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mget_synonyms\u001b[39m\u001b[34m(word, noise_level, topn)\u001b[39m\n\u001b[32m     11\u001b[39m noisy_vec = original_vec + noise\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Retrieve the topn + 1 words (as one of them will be the word itself) closest to the noisy vector\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# This will return a list of tuples (word, similarity)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m similar_words = \u001b[43mglove_embeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositive\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnoisy_vec\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopn\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Return just the words from the list of tuples\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [syn \u001b[38;5;28;01mfor\u001b[39;00m syn, similarity \u001b[38;5;129;01min\u001b[39;00m similar_words \u001b[38;5;28;01mif\u001b[39;00m syn != word]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\willi\\Desktop\\Uni\\3rd_year\\NLU\\comp38412-not-broken\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:849\u001b[39m, in \u001b[36mKeyedVectors.most_similar\u001b[39m\u001b[34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[39m\n\u001b[32m    846\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    847\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m indexer.most_similar(mean, topn)\n\u001b[32m--> \u001b[39m\u001b[32m849\u001b[39m dists = \u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclip_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mclip_end\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m / \u001b[38;5;28mself\u001b[39m.norms[clip_start:clip_end]\n\u001b[32m    850\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m topn:\n\u001b[32m    851\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dists\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def augment_data(train_df: pd.DataFrame, \n",
    "                 preprocessed_evidences: list, \n",
    "                 corresponding_claim: list, \n",
    "                 original_evidences: list, \n",
    "                 original_pos_tags: list, \n",
    "                 file_name: str,\n",
    "                 add_original_evidence: bool = False,\n",
    "                 batch_size: int = 1000):\n",
    "    \"\"\"\n",
    "    Main function to create augmented dataset with synonym replacements.\n",
    "    \n",
    "    Args:\n",
    "        train_df (pd.DataFrame): The training dataframe.\n",
    "        preprocessed_evidences (list): List of preprocessed evidences.\n",
    "        corresponding_claim (list): List of claims corresponding to the preprocessed evidences.\n",
    "        original_evidences (list): List of original evidences.\n",
    "        original_pos_tags (list): List of original POS tags.\n",
    "        batch_size (int): The batch size for saving to CSV.\n",
    "    \"\"\"\n",
    "    if Path(file_name).exists():\n",
    "        overwrite = input(f\"File {file_name} already exists. Would you like to overwrite it? (y/n) \")\n",
    "        if overwrite != 'y':\n",
    "            return\n",
    "        \n",
    "    cols = [\"Claim\", \"Evidence\", \"label\"]\n",
    "    if add_original_evidence:\n",
    "        cols.append(\"Original Evidence\")\n",
    "    \n",
    "    synyonm_replaced_df = pd.DataFrame(columns=cols)\n",
    "    batch_counter = 0\n",
    "    \n",
    "    for idx, (claim, evidence) in tqdm(enumerate(zip(corresponding_claim, preprocessed_evidences))):\n",
    "        # Prepare POS tags dictionary\n",
    "        pos_tags = original_pos_tags[idx]\n",
    "        pos_tags_dict = {word.lower(): [] for word, _ in pos_tags}\n",
    "        for word, tag in pos_tags:\n",
    "            pos_tags_dict[word.lower()].append(tag)\n",
    "        \n",
    "        # Get potential words to replace\n",
    "        claim_words = set(claim.split())\n",
    "        evidence_words = evidence.split()\n",
    "        potential_replacements = process_evidence_words(claim_words, evidence_words, pos_tags_dict)\n",
    "        \n",
    "        # Skip if not enough words to replace\n",
    "        number_of_replacements = len(potential_replacements) // 3\n",
    "        if number_of_replacements < 1:\n",
    "            continue\n",
    "        \n",
    "        # Find replacements\n",
    "        words_to_replace = random.sample(potential_replacements, k=number_of_replacements)\n",
    "        final_word_replacement_map = {}\n",
    "        \n",
    "        for word in words_to_replace:\n",
    "            synonyms = get_synonyms(word, noise_level=0.0001, topn=20)\n",
    "            found, synonym = find_valid_replacements(\n",
    "                word, synonyms, original_evidences[idx], pos_tags_dict\n",
    "            )\n",
    "            if found:\n",
    "                # print(f\"Replacement found: {word} -> {synonym}\")\n",
    "                final_word_replacement_map[word] = synonym\n",
    "        \n",
    "        # Skip if not enough valid replacements found\n",
    "        if len(final_word_replacement_map) < len(words_to_replace) * 0.6:\n",
    "            # print(f\"Insufficient replacements found for {words_to_replace}\")\n",
    "            continue\n",
    "        \n",
    "        # Create new evidence with replacements\n",
    "        new_evidence = original_evidences[idx]\n",
    "        for word, replacement in final_word_replacement_map.items():\n",
    "            pattern = r'\\b' + re.escape(word) + r'\\b'\n",
    "            new_evidence = re.sub(pattern, replacement, new_evidence)\n",
    "        \n",
    "        # Add to dataframe\n",
    "        new_row = {\n",
    "            \"Claim\": [train_df['Claim'][idx]],\n",
    "            \"Evidence\": [new_evidence],\n",
    "            \"label\": [train_df['label'][idx]]\n",
    "        }\n",
    "        \n",
    "        if add_original_evidence:\n",
    "            new_row[\"Original Evidence\"] = [original_evidences[idx]]\n",
    "            \n",
    "        new_row = pd.DataFrame(new_row)\n",
    "        \n",
    "        synyonm_replaced_df = pd.concat([synyonm_replaced_df, new_row], ignore_index=True)\n",
    "        \n",
    "        # Save batch if size threshold reached\n",
    "        if len(synyonm_replaced_df) >= batch_size:\n",
    "            mode = 'w' if batch_counter == 0 else 'a'\n",
    "            header = batch_counter == 0\n",
    "            synyonm_replaced_df.to_csv(file_name, index=False, mode=mode, header=header)\n",
    "            synyonm_replaced_df = pd.DataFrame(columns=cols)\n",
    "            batch_counter += 1\n",
    "    \n",
    "    # Save any remaining data\n",
    "    if len(synyonm_replaced_df) > 0:\n",
    "        mode = 'w' if batch_counter == 0 else 'a'\n",
    "        header = batch_counter == 0\n",
    "        \n",
    "        synyonm_replaced_df.to_csv(file_name, index=False, mode=mode, header=header)\n",
    "\n",
    "# Run the augmentation\n",
    "augment_data(train_df, \n",
    "             preprocessed_evidences, \n",
    "             corresponding_claim, \n",
    "             original_evidences, \n",
    "             original_evidences_pos,\n",
    "             add_original_evidence=True,\n",
    "             file_name='data/train_augmented_synonyms_with_original_evidence.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
