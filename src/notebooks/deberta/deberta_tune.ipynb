{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11351353,"sourceType":"datasetVersion","datasetId":7013730}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets huggingface_hub optuna tensorboard peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T18:09:16.810936Z","iopub.execute_input":"2025-04-08T18:09:16.811378Z","iopub.status.idle":"2025-04-08T18:09:21.832577Z","shell.execute_reply.started":"2025-04-08T18:09:16.811339Z","shell.execute_reply":"2025-04-08T18:09:21.831622Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import logging\nimport os\nimport pandas as pd\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport optuna\nfrom optuna.samplers import TPESampler\nimport json\nfrom pathlib import Path\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    EarlyStoppingCallback,\n    DataCollatorWithPadding\n)\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_recall_fscore_support,\n    matthews_corrcoef,\n    confusion_matrix,\n)\nfrom datasets import Dataset as HFDataset\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nfrom tqdm import tqdm\n\n# Configure logging\nlogging.basicConfig(\n    format='%(asctime)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    level=logging.INFO\n)\n\n# Disable wandb\nos.environ['WANDB_DISABLED'] = 'true'\n\n# Path configuration\nDATA_DIR = Path(\"/kaggle/working/\")\nTRAIN_FILE = \"/kaggle/input/ed-uom/train.csv\"\nDEV_FILE = \"/kaggle/input/ed-uom/dev.csv\"\nAUG_TRAIN_FILE = \"/kaggle/input/ed-uom/train_augmented.csv\"\nNEW_AUG = \"/kaggle/input/ed-uom/train_augmented_new.csv\"\nAUG_TRAIN_HIGH_REPLACEMENT_FILE = DATA_DIR / \"train_augmented_high_replacement_fraction.csv\"\nSAVE_DIR = DATA_DIR / \"results\" / \"transformer\"\nSAVE_DIR.mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T18:09:28.847756Z","iopub.execute_input":"2025-04-08T18:09:28.848111Z","iopub.status.idle":"2025-04-08T18:09:54.397088Z","shell.execute_reply.started":"2025-04-08T18:09:28.848080Z","shell.execute_reply":"2025-04-08T18:09:54.396258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training parameters\nBATCH_SIZE = 8\nNUM_EPOCHS = 3\nLEARNING_RATE = 5e-5\nWEIGHT_DECAY = 0.05\nWARMUP_RATIO = 0.07\nDROPOUT_RATE = 0.05\nFF_DROPOUT_RATE = 0.05\nMAX_SEQ_LENGTH = 512\nBASE_MODEL = 'microsoft/deberta-v3-large'\n\n\n# Optuna parameters\nN_TRIALS = 10\n\nWEIGHT_DECAYS = [0.001, 0.1]\nWARMUP_RATIOS = [0.05, 0.15]\nDROPOUT_RATES = [0.05]\nFF_DROPOUT_RATES = [0.05]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T18:09:26.411966Z","iopub.execute_input":"2025-04-08T18:09:26.412341Z","iopub.status.idle":"2025-04-08T18:09:26.417720Z","shell.execute_reply.started":"2025-04-08T18:09:26.412312Z","shell.execute_reply":"2025-04-08T18:09:26.416701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_device() -> torch.device:\n    \"\"\"Determine the device to use for computations.\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n        return torch.device('mps')\n    return torch.device('cpu')\n\ndef preprocess_function(examples, tokenizer, max_seq_length):\n    \"\"\"Process examples for BERT/DeBERTa classification.\"\"\"\n    # Combine claim and evidence\n    claims = []\n    evidences = []\n\n    # Create inputs and targets\n    for claim, evidence in zip(examples['Claim'], examples['Evidence']):\n        formatted_claim = f\"Claim: {claim}\"\n        formatted_evidence = f\"Evidence: {evidence}\"\n        claims.append(formatted_claim)\n        evidences.append(formatted_evidence)\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(\n        claims,\n        evidences,\n        max_length=max_seq_length,\n        padding=False,\n        truncation=True,\n    )\n    \n    # Add labels (binary classification)\n    model_inputs[\"labels\"] = examples['label']\n    return model_inputs\n\ndef convert_to_hf_dataset(dataframe):\n    \"\"\"Convert pandas dataframe to HuggingFace dataset format.\"\"\"\n    return HFDataset.from_pandas(dataframe)\n\ndef load_data(tokenizer, max_seq_length):\n    \"\"\"Load and prepare the training and development datasets.\"\"\"\n    logging.info(\"Loading datasets...\")\n    \n    # Load CSV files into pandas dataframes\n    train_df = pd.read_csv(AUG_TRAIN_FILE)\n    dev_df = pd.read_csv(DEV_FILE)\n\n    # try:\n    #     train_augmented_df = pd.read_csv(AUG_TRAIN_FILE)\n    #     another_aug_df = pd.read_csv(ANOTHER_AUG_FILE)\n    #     train_df = pd.concat([train_df, train_augmented_df, another_aug_df])\n    # except Exception as e:\n    #     logging.error(f\"Error loading or concatenating augmented training data: {e}\")\n    #     raise\n    \n    print(f\"Training data shape: {train_df.shape}\")\n    print(f\"Development data shape: {dev_df.shape}\")\n    \n    # Check and report class distribution\n    train_positive = (train_df['label'] == 1).sum()\n    train_negative = (train_df['label'] == 0).sum()\n    dev_positive = (dev_df['label'] == 1).sum()\n    dev_negative = (dev_df['label'] == 0).sum()\n    \n    print(f\"Training data distribution: Positive: {train_positive} ({train_positive/len(train_df)*100:.1f}%), \"\n                 f\"Negative: {train_negative} ({train_negative/len(train_df)*100:.1f}%)\")\n    print(f\"Dev data distribution: Positive: {dev_positive} ({dev_positive/len(dev_df)*100:.1f}%), \"\n                 f\"Negative: {dev_negative} ({dev_negative/len(dev_df)*100:.1f}%)\")\n    \n    # Add a sequential index to keep track of original order (if not already present)\n    if 'original_index' not in dev_df.columns:\n        dev_df['original_index'] = list(range(len(dev_df)))\n    \n    # Convert to HuggingFace datasets\n    train_dataset = convert_to_hf_dataset(train_df)\n    dev_dataset = convert_to_hf_dataset(dev_df)\n    \n    # Apply preprocessing (tokenization)\n    train_dataset = train_dataset.map(\n        lambda examples: preprocess_function(examples, tokenizer, max_seq_length),\n        batched=True,\n        batch_size=1000,\n        remove_columns=['Claim', 'Evidence', 'label']\n    )\n    \n    # For dev dataset, keep track of original indices but remove other columns\n    columns_to_remove = [col for col in dev_df.columns if col not in ['original_index']]\n    dev_dataset = dev_dataset.map(\n        lambda examples: preprocess_function(examples, tokenizer, max_seq_length),\n        batched=True,\n        batch_size=1000,\n        remove_columns=columns_to_remove\n    )\n    \n    # Set format for pytorch\n    train_dataset.set_format(type='torch')\n    dev_dataset.set_format(type='torch')\n    \n    return train_dataset, dev_dataset, dev_df\n\ndef compute_metrics(eval_pred):\n    \"\"\"Calculate evaluation metrics for classification.\"\"\"\n    predictions, labels = eval_pred\n    \n    # For binary classification, get the predicted class (0 or 1)\n    predictions = predictions.argmax(axis=1)\n    \n    # Calculate metrics with more focus on positive class\n    accuracy = accuracy_score(labels, predictions)\n    \n    # Get more detailed metrics for both classes\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, predictions, average=None, zero_division=0\n    )\n    \n    # Weighted metrics\n    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n        labels, predictions, average='weighted', zero_division=0\n    )\n    \n    # Matthews Correlation Coefficient\n    mcc = matthews_corrcoef(labels, predictions)\n    \n    # Return both class-specific and overall metrics\n    metrics = {\n        'Accuracy': accuracy,\n        'Positive_Precision': precision[1] if len(precision) > 1 else 0,\n        'Positive_Recall': recall[1] if len(recall) > 1 else 0,\n        'Positive_F1': f1[1] if len(f1) > 1 else 0,\n        'W Macro-P': weighted_precision,\n        'W Macro-R': weighted_recall,\n        'W Macro-F1': weighted_f1,\n        'MCC': mcc\n    }\n    \n    return metrics\n\ndef plot_confusion_matrix(y_true, y_pred, save_path):\n    \"\"\"Plot and save confusion matrix.\"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    \n    classes = ['Negative', 'Positive']\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    \n    # Normalize confusion matrix\n    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    # Add text annotations\n    thresh = cm.max() / 2.\n    for i, j in np.ndindex(cm.shape):\n        plt.text(j, i, f'{cm[i, j]}\\n({cm_norm[i, j]:.2f})',\n                horizontalalignment=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\")\n    \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.savefig(save_path)\n    plt.close()\n\ndef train_model(\n    model,\n    train_dataset,\n    eval_dataset,\n    output_dir,\n    tokenizer,\n    **kwargs\n):\n    \"\"\"Train the classification model.\"\"\"\n    logging.info(\"Starting training...\")\n    \n    # Free up CUDA memory before training\n    torch.cuda.empty_cache()\n    \n    # Create data collator for dynamic padding\n    data_collator = DataCollatorWithPadding(\n        tokenizer=tokenizer,\n        padding='longest'\n    )\n    \n    training_args = TrainingArguments(\n        output_dir=str(output_dir),\n        greater_is_better=True,\n        **kwargs\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        processing_class=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=8)]\n    )\n    \n    trainer.train()\n    \n    # Evaluate on dev set and plot confusion matrix\n    eval_results = trainer.evaluate()\n    dev_preds = trainer.predict(eval_dataset)\n    y_true = dev_preds.label_ids\n    y_pred = dev_preds.predictions.argmax(axis=1)\n\n    # Save predictions to a CSV file with original dev data for alignment\n    # First, load the original dev CSV to maintain alignment\n    dev_df = pd.read_csv(DEV_FILE)\n    \n    # Create a dataframe with predictions\n    predictions_df = pd.DataFrame({'prediction': y_pred})\n\n    \n    \n    # Check if the evaluation dataset has original indices\n    if hasattr(eval_dataset, 'original_index') or 'original_index' in eval_dataset.features:\n        # Get original indices if present\n        try:\n            original_indices = [item['original_index'] for item in eval_dataset]\n            # Sort predictions by original index\n            predictions_df['original_index'] = original_indices\n            predictions_df = predictions_df.sort_values('original_index')\n            del predictions_df['original_index']  # Remove after sorting\n        except Exception as e:\n            logging.warning(f\"Couldn't use original indices: {e}\")\n    \n    # Ensure the predictions align with the original data\n    if len(dev_df) == len(predictions_df):\n        # Add predictions to the original dev dataframe\n        dev_df['prediction'] = predictions_df['prediction'].values\n        predictions_csv_path = os.path.join(output_dir, \"predictions_with_data.csv\")\n        dev_df.to_csv(predictions_csv_path, index=False)\n        print(f\"Predictions with original data saved to {predictions_csv_path}\")\n        \n        # Also save just the predictions for convenience\n        predictions_only_path = os.path.join(output_dir, \"predictions.csv\")\n        predictions_df.to_csv(predictions_only_path, index=False)\n    else:\n        print(f\"Prediction count ({len(predictions_df)}) doesn't match dev data count ({len(dev_df)})\")\n        # Save just the predictions\n        predictions_csv_path = os.path.join(output_dir, \"predictions.csv\")\n        predictions_df.to_csv(predictions_csv_path, index=False)\n        print(f\"Predictions saved to {predictions_csv_path}\")\n    \n    # Plot and save confusion matrix\n    cm_save_path = os.path.join(output_dir, \"confusion_matrix.png\")\n    plot_confusion_matrix(y_true, y_pred, cm_save_path)\n    \n    trainer.save_model()\n    logging.info(f\"Model saved to {output_dir}\")\n\n    print(eval_results)\n    \n    return eval_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T18:09:56.273979Z","iopub.execute_input":"2025-04-08T18:09:56.274648Z","iopub.status.idle":"2025-04-08T18:09:56.297419Z","shell.execute_reply.started":"2025-04-08T18:09:56.274618Z","shell.execute_reply":"2025-04-08T18:09:56.296494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def objective(trial):\n    \"\"\"Optuna objective function for hyperparameter optimization.\"\"\"\n    # Get hyperparameters from trial\n    weight_decay = trial.suggest_float(\"weight_decay\", WEIGHT_DECAYS[0], WEIGHT_DECAYS[1], log=True)\n    warmup_ratio = trial.suggest_float(\"warmup_ratio\", WARMUP_RATIOS[0], WARMUP_RATIOS[1])\n    \n    device = get_device()\n    logging.info(f\"Trial {trial.number}: Using device: {device}\")\n    \n    # Free GPU memory\n    torch.cuda.empty_cache()\n\n    peft_config = LoraConfig(\n        task_type=TaskType.SEQ_CLS,\n        inference_mode=False,\n        r=8,  # Increased rank\n        lora_alpha=16,  # Higher scale\n        lora_dropout=0.1,\n        target_modules=[\"query_proj\", \"key_proj\", \"value_proj\", \"dense\"],  # Target both attention and FFN\n        init_lora_weights='pissa',\n        layers_to_transform=[i for i in range(6, 24)]\n    )\n\n    # Initialize tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        BASE_MODEL, \n        num_labels=2,\n        hidden_dropout_prob=DROPOUT_RATE,\n        attention_probs_dropout_prob=DROPOUT_RATE,\n    )\n\n    hidden_size = model.classifier.in_features\n    model.classifier = nn.Sequential(\n        nn.Linear(hidden_size, hidden_size),\n        nn.GELU(),\n        nn.LayerNorm(hidden_size),\n        nn.Dropout(FF_DROPOUT_RATE),\n        nn.Linear(hidden_size, 2)\n    )\n    model.config.num_labels = 2\n\n    model = get_peft_model(model, peft_config)\n    \n    model.to(device)\n    \n    # Load data with current max_seq_length\n    train_dataset, dev_dataset, dev_df = load_data(tokenizer, MAX_SEQ_LENGTH)\n    \n    # Training parameters\n    training_params = {\n        'per_device_train_batch_size': BATCH_SIZE,\n        'per_device_eval_batch_size': BATCH_SIZE,\n        'learning_rate': LEARNING_RATE,\n        'weight_decay': weight_decay,\n        'num_train_epochs': NUM_EPOCHS,\n        'warmup_ratio': warmup_ratio,\n        'lr_scheduler_type': 'cosine',\n        'evaluation_strategy': 'steps',\n        'eval_steps': 1000,\n        'save_strategy': 'steps',\n        'save_steps': 1000,\n        'save_total_limit': 1,\n        'load_best_model_at_end': True,\n        'metric_for_best_model': 'MCC',\n        'fp16': torch.cuda.is_available(),\n        'optim': 'adamw_torch',\n        'logging_steps': 100,\n        'logging_first_step': True,\n        'group_by_length': True,\n        'seed': 42,\n        'label_smoothing_factor': 0.1,\n    }\n    \n    # Set trial output directory\n    trial_dir = SAVE_DIR / f\"trial_{trial.number}\"\n    \n    try:\n        # Train with current hyperparameters\n        eval_results = train_model(\n            model,\n            train_dataset,\n            dev_dataset,\n            trial_dir,\n            tokenizer,\n            **training_params\n        )\n        \n        # Log the hyperparameters and results\n        params = {\n            \"weight_decay\": weight_decay,\n            \"warmup_ratio\": warmup_ratio,\n        }\n        \n        with open(trial_dir / \"hyperparameters.json\", \"w\") as f:\n            json.dump({**params, **eval_results}, f, indent=2)\n        \n        # Return Matthews Correlation Coefficient as the objective value\n        return eval_results[\"eval_MCC\"]\n    \n    except Exception as e:\n        print(f\"Trial {trial.number} failed with error: {e}\")\n        # Return very bad score for failed trials\n        return -1.0\n\ndef run_optuna_experiment():\n    \"\"\"Run Optuna hyperparameter optimization experiment.\"\"\"\n    logging.info(\"Starting hyperparameter optimization with Optuna...\")\n    \n    # Create output directory for study\n    study_dir = SAVE_DIR / \"optuna_study\"\n    study_dir.mkdir(exist_ok=True)\n    \n    # Create a pruner to terminate unpromising trials\n    pruner = optuna.pruners.MedianPruner()\n    \n    # Create a storage for the study\n    storage_name = f\"sqlite:///{study_dir}/optuna_study.db\"\n    \n    # Create TPE sampler for Bayesian optimization\n    sampler = TPESampler(seed=42)\n    \n    # Create the study\n    study = optuna.create_study(\n        direction=\"maximize\",\n        pruner=pruner,\n        storage=storage_name,\n        study_name=\"deberta_claim_evidence\",\n        load_if_exists=True,\n        sampler=sampler\n    )\n    \n    # Run optimization\n    study.optimize(objective, n_trials=N_TRIALS)\n    \n    # Get best trial\n    best_trial = study.best_trial\n    \n    # Log additional information about the Bayesian optimization\n    logging.info(f\"Using Bayesian optimization with TPE sampler\")\n    logging.info(f\"Best trial: {best_trial.number}\")\n    logging.info(f\"Best value: {best_trial.value}\")\n    logging.info(\"Best hyperparameters:\")\n    \n    for param, value in best_trial.params.items():\n        logging.info(f\"\\t{param}: {value}\")\n    \n    # Save best parameters\n    best_params = {\n        \"weight_decay\": best_trial.params[\"weight_decay\"],\n        \"warmup_ratio\": best_trial.params[\"warmup_ratio\"],\n    }\n    \n    with open(study_dir / \"best_params.json\", \"w\") as f:\n        json.dump(best_params, f, indent=2)\n    \n    # Plot optimization history\n    fig = optuna.visualization.plot_optimization_history(study)\n    fig.write_html(str(study_dir / \"optimization_history.html\"))\n    \n    # Plot parameter importance\n    fig = optuna.visualization.plot_param_importances(study)\n    fig.write_html(str(study_dir / \"param_importances.html\"))\n    \n    # Plot parameter relationships\n    fig = optuna.visualization.plot_parallel_coordinate(study)\n    fig.write_html(str(study_dir / \"parallel_coordinate.html\"))\n    \n    # Plot high-dimensional parameter relationships\n    fig = optuna.visualization.plot_contour(study)\n    fig.write_html(str(study_dir / \"contour.html\"))\n    \n    return best_params\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T18:10:02.698044Z","iopub.execute_input":"2025-04-08T18:10:02.698370Z","iopub.status.idle":"2025-04-08T18:10:02.713829Z","shell.execute_reply.started":"2025-04-08T18:10:02.698347Z","shell.execute_reply":"2025-04-08T18:10:02.712644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    \"\"\"Main execution function.\"\"\"\n    device = get_device()\n    logging.info(f\"Using device: {device}\")\n\n    best_params = run_optuna_experiment()\n    \n    # Free GPU memory\n    torch.cuda.empty_cache()\n\n    peft_config = LoraConfig(\n        task_type=TaskType.SEQ_CLS,\n        inference_mode=False,\n        r=8,  # Increased rank\n        lora_alpha=16,  # Higher scale\n        lora_dropout=0.1,\n        target_modules=[\"query_proj\", \"key_proj\", \"value_proj\", \"dense\"],  # Target both attention and FFN\n        init_lora_weights='pissa',\n        layers_to_transform=[i for i in range(6, 24)]\n    )\n\n    # Initialize tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        BASE_MODEL, \n        num_labels=2,\n        hidden_dropout_prob=DROPOUT_RATE,\n        attention_probs_dropout_prob=DROPOUT_RATE,\n    )\n\n    hidden_size = model.classifier.in_features\n    model.classifier = nn.Sequential(\n        nn.Linear(hidden_size, hidden_size),\n        nn.GELU(),\n        nn.LayerNorm(hidden_size),\n        nn.Dropout(FF_DROPOUT_RATE),\n        nn.Linear(hidden_size, 2)\n    )\n    model.config.num_labels = 2\n\n    # model = get_peft_model(model, peft_config)\n    \n    model.to(device)\n\n    # Load data\n    train_dataset, dev_dataset, dev_df = load_data(tokenizer, MAX_SEQ_LENGTH)\n    \n    # Training parameters with focus on preventing overfitting\n    training_params = {\n        'per_device_train_batch_size': BATCH_SIZE,\n        'per_device_eval_batch_size': BATCH_SIZE,\n        'learning_rate': 5e-6,\n        'weight_decay': best_params[\"weight_decay\"],\n        'num_train_epochs': 10,\n        'warmup_ratio': best_params[\"warmup_ratio\"],\n        'lr_scheduler_type': 'cosine',\n        'evaluation_strategy': 'steps',\n        'eval_steps': 1000,\n        'save_strategy': 'steps',\n        'save_steps': 1000,\n        'save_total_limit': 5,\n        'load_best_model_at_end': True,\n        'metric_for_best_model': 'MCC',\n        'fp16': torch.cuda.is_available(),\n        'optim': 'adamw_torch',\n        'logging_steps': 100,\n        'logging_first_step': True,\n        'group_by_length': True,\n        'seed': 42,\n        'label_smoothing_factor': 0.1,\n    }\n    \n    # Train with default parameters\n    model_save_path = SAVE_DIR / BASE_MODEL.split('/')[-1]\n    train_model(\n        model,\n        train_dataset,\n        dev_dataset,\n        model_save_path,\n        tokenizer,\n        **training_params\n    )\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T18:10:06.821925Z","iopub.execute_input":"2025-04-08T18:10:06.822397Z","iopub.status.idle":"2025-04-08T18:42:01.828337Z","shell.execute_reply.started":"2025-04-08T18:10:06.822359Z","shell.execute_reply":"2025-04-08T18:42:01.826510Z"}},"outputs":[],"execution_count":null}]}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets huggingface_hub optuna tensorboard peft\n",
    "!pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    matthews_corrcoef,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Disable wandb\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "\n",
    "# Path configuration\n",
    "DATA_DIR = Path(\"/kaggle/working/\")\n",
    "\n",
    "TRAIN_FILE = \"/kaggle/input/ed-uom/train.csv\"\n",
    "DEV_FILE = \"/kaggle/input/ed-uom/dev.csv\"\n",
    "\n",
    "SAVE_DIR = DATA_DIR / \"results\" / \"transformer\"\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 0.0015\n",
    "WARMUP_RATIO = 0.12\n",
    "DROPOUT_RATE = 0.05\n",
    "MAX_SEQ_LENGTH = 512\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "BASE_MODEL = 'microsoft/deberta-v2-xlarge-mnli'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calculate evaluation metrics for classification.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # For binary classification, get the predicted class (0 or 1)\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    \n",
    "    # Calculate metrics with more focus on positive class\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Get more detailed metrics for both classes\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Weighted metrics\n",
    "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Matthews Correlation Coefficient\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "    \n",
    "    # Return both class-specific and overall metrics\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Positive_Precision': precision[1] if len(precision) > 1 else 0,\n",
    "        'Positive_Recall': recall[1] if len(recall) > 1 else 0,\n",
    "        'Positive_F1': f1[1] if len(f1) > 1 else 0,\n",
    "        'W Macro-P': weighted_precision,\n",
    "        'W Macro-R': weighted_recall,\n",
    "        'W Macro-F1': weighted_f1,\n",
    "        'MCC': mcc\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    \"\"\"Determine the device to use for computations.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, save_path):\n",
    "    \"\"\"Plot and save confusion matrix.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    classes = ['Negative', 'Positive']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, f'{cm[i, j]}\\n({cm_norm[i, j]:.2f})',\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer):\n",
    "    \"\"\"Process examples for BERT/DeBERTa classification.\"\"\"\n",
    "    # Combine claim and evidence\n",
    "    inputs = []\n",
    "\n",
    "    # Create inputs and targets\n",
    "    for claim, evidence in zip(examples['Claim'], examples['Evidence']):\n",
    "        # Simplify the instruction and make it more direct\n",
    "        formatted_input = f\"Claim: {claim}\\n\\nEvidence: {evidence}\"\n",
    "        inputs.append(formatted_input)        \n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    # Add labels (binary classification)\n",
    "    model_inputs[\"labels\"] = examples['label']\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def load_data(tokenizer, max_seq_length):\n",
    "    \"\"\"Load and prepare the training and development datasets.\"\"\"\n",
    "    logging.info(\"Loading datasets...\")\n",
    "    \n",
    "    # Load CSV files into pandas dataframes\n",
    "    train_df = pd.read_csv(TRAIN_FILE)\n",
    "    dev_df = pd.read_csv(DEV_FILE)\n",
    "\n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Development data shape: {dev_df.shape}\")\n",
    "    \n",
    "    # Check and report class distribution\n",
    "    train_positive = (train_df['label'] == 1).sum()\n",
    "    train_negative = (train_df['label'] == 0).sum()\n",
    "    dev_positive = (dev_df['label'] == 1).sum()\n",
    "    dev_negative = (dev_df['label'] == 0).sum()\n",
    "    \n",
    "    print(f\"Training data distribution: Positive: {train_positive} ({train_positive/len(train_df)*100:.1f}%), \"\n",
    "                 f\"Negative: {train_negative} ({train_negative/len(train_df)*100:.1f}%)\")\n",
    "    print(f\"Dev data distribution: Positive: {dev_positive} ({dev_positive/len(dev_df)*100:.1f}%), \"\n",
    "                 f\"Negative: {dev_negative} ({dev_negative/len(dev_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Add a sequential index to keep track of original order (if not already present)\n",
    "    if 'original_index' not in dev_df.columns:\n",
    "        dev_df['original_index'] = list(range(len(dev_df)))\n",
    "    \n",
    "    # Convert to HuggingFace datasets\n",
    "    train_dataset = HFDataset.from_pandas(train_df)\n",
    "    dev_dataset = HFDataset.from_pandas(dev_df)\n",
    "    \n",
    "    # Apply preprocessing (tokenization)\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda examples: preprocess_function(examples, tokenizer, max_seq_length),\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        remove_columns=['Claim', 'Evidence', 'label']\n",
    "    )\n",
    "        \n",
    "    dev_dataset = dev_dataset.map(\n",
    "        lambda examples: preprocess_function(examples, tokenizer, max_seq_length),\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        remove_columns=['Claim', 'Evidence', 'label']\n",
    "    )\n",
    "    \n",
    "    # Set format for pytorch\n",
    "    train_dataset.set_format(type='torch')\n",
    "    dev_dataset.set_format(type='torch')\n",
    "    \n",
    "    return train_dataset, dev_dataset, dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    output_dir,\n",
    "    tokenizer,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Train the classification model.\"\"\"\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # Free up CUDA memory before training\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Create data collator for dynamic padding\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding='longest'\n",
    "        # max_length=MAX_SEQ_LENGTH\n",
    "    )\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        greater_is_better=True,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate on dev set and plot confusion matrix\n",
    "    eval_results = trainer.evaluate()\n",
    "    dev_preds = trainer.predict(eval_dataset)\n",
    "    y_true = dev_preds.label_ids\n",
    "    y_pred = dev_preds.predictions.argmax(axis=1)\n",
    "\n",
    "    # Save predictions to a CSV file with original dev data for alignment\n",
    "    # First, load the original dev CSV to maintain alignment\n",
    "    dev_df = pd.read_csv(DEV_FILE)\n",
    "    \n",
    "    # Create a dataframe with predictions\n",
    "    predictions_df = pd.DataFrame({'prediction': y_pred})\n",
    "    \n",
    "    # Check if the evaluation dataset has original indices\n",
    "    if hasattr(eval_dataset, 'original_index') or 'original_index' in eval_dataset.features:\n",
    "        print(\"Checking the original indices\")\n",
    "        # Get original indices if present\n",
    "        try:\n",
    "            original_indices = [item['original_index'] for item in eval_dataset]\n",
    "            # Sort predictions by original index\n",
    "            predictions_df['original_index'] = original_indices\n",
    "            predictions_df = predictions_df.sort_values('original_index')\n",
    "            del predictions_df['original_index']  # Remove after sorting\n",
    "            print(\"Predictions sorted by original indices\")\n",
    "        except Exception as e:\n",
    "            print(f\"Predictions matching: couldn't use original indices: {e}\")\n",
    "    \n",
    "    # Ensure the predictions align with the original data\n",
    "    if len(dev_df) == len(predictions_df):\n",
    "        # Add predictions to the original dev dataframe\n",
    "        dev_df['prediction'] = predictions_df['prediction'].values\n",
    "        predictions_csv_path = os.path.join(output_dir, \"predictions_with_data.csv\")\n",
    "        dev_df.to_csv(predictions_csv_path, index=False)\n",
    "        print(f\"Predictions with original data saved to {predictions_csv_path}\")\n",
    "        \n",
    "        # Also save just the predictions for convenience\n",
    "        predictions_only_path = os.path.join(output_dir, \"predictions.csv\")\n",
    "        predictions_df.to_csv(predictions_only_path, index=False)\n",
    "    else:\n",
    "        print(f\"Prediction count ({len(predictions_df)}) doesn't match dev data count ({len(dev_df)})\")\n",
    "        # Save just the predictions\n",
    "        predictions_csv_path = os.path.join(output_dir, \"predictions.csv\")\n",
    "        predictions_df.to_csv(predictions_csv_path, index=False)\n",
    "        print(f\"Predictions saved to {predictions_csv_path}\")\n",
    "    \n",
    "    # Plot and save confusion matrix\n",
    "    cm_save_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
    "    plot_confusion_matrix(y_true, y_pred, cm_save_path)\n",
    "    \n",
    "    trainer.save_model()\n",
    "    print(f\"Model saved to {output_dir}\")\n",
    "    \n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     \"\"\"Optuna objective function for hyperparameter optimization.\"\"\"\n",
    "#     # Get hyperparameters from trial\n",
    "#     weight_decay = trial.suggest_float(\"weight_decay\", WEIGHT_DECAYS[0], WEIGHT_DECAYS[-1], log=True)\n",
    "#     warmup_ratio = trial.suggest_float(\"warmup_ratio\", WARMUP_RATIOS[0], WARMUP_RATIOS[-1])\n",
    "#     dropout = trial.suggest_float(\"dropout\", DROPOUT_RATES[0], DROPOUT_RATES[-1])\n",
    "    \n",
    "#     device = get_device()\n",
    "#     logging.info(f\"Trial {trial.number}: Using device: {device}\")\n",
    "    \n",
    "#     # Free GPU memory\n",
    "#     torch.cuda.empty_cache()\n",
    "    \n",
    "#     # LoRA configuration (fixed for all trials)\n",
    "#     peft_config = LoraConfig(\n",
    "#         task_type=TaskType.SEQ_CLS,\n",
    "#         inference_mode=False,\n",
    "#         r=8,\n",
    "#         lora_alpha=16,\n",
    "#         lora_dropout=0.1,\n",
    "#     )\n",
    "    \n",
    "#     # Initialize tokenizer and model\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#         BASE_MODEL, \n",
    "#         num_labels=2,\n",
    "#         hidden_dropout_prob=dropout,\n",
    "#         attention_probs_dropout_prob=dropout,\n",
    "#         ignore_mismatched_sizes=True,\n",
    "#     )\n",
    "    \n",
    "#     model = get_peft_model(model, peft_config)\n",
    "#     model.to(device)\n",
    "    \n",
    "#     # Load data with current max_seq_length\n",
    "#     train_dataset, dev_dataset, dev_df = load_data(tokenizer, MAX_SEQ_LENGTH)\n",
    "    \n",
    "#     # Training parameters\n",
    "#     training_params = {\n",
    "#         'per_device_train_batch_size': BATCH_SIZE,\n",
    "#         'per_device_eval_batch_size': BATCH_SIZE,\n",
    "#         'learning_rate': LEARNING_RATE,\n",
    "#         'weight_decay': weight_decay,\n",
    "#         'num_train_epochs': NUM_EPOCHS,\n",
    "#         'warmup_ratio': warmup_ratio,\n",
    "#         'lr_scheduler_type': 'cosine_with_restarts',\n",
    "#         'eval_strategy': 'steps',\n",
    "#         'eval_steps': 500,\n",
    "#         'save_strategy': 'steps',\n",
    "#         'save_steps': 500,\n",
    "#         'save_total_limit': 2,\n",
    "#         'load_best_model_at_end': True,\n",
    "#         'metric_for_best_model': 'MCC',\n",
    "#         'fp16': device.type == 'cuda', \n",
    "#         'optim': 'adamw_torch',\n",
    "#         'logging_steps': 100,\n",
    "#         'logging_first_step': True,\n",
    "#         'group_by_length': True,\n",
    "#         'seed': 42,\n",
    "#         'dataloader_num_workers': 4,\n",
    "#         'label_smoothing_factor': 0.05,\n",
    "#         'max_grad_norm': 1.0,\n",
    "#         'gradient_checkpointing': True,\n",
    "#     }\n",
    "    \n",
    "#     # Set trial output directory\n",
    "#     trial_dir = SAVE_DIR / f\"trial_{trial.number}\"\n",
    "    \n",
    "#     try:\n",
    "#         # Train with current hyperparameters\n",
    "#         eval_results = train_model(\n",
    "#             model,\n",
    "#             train_dataset,\n",
    "#             dev_dataset,\n",
    "#             trial_dir,\n",
    "#             tokenizer,\n",
    "#             **training_params\n",
    "#         )\n",
    "        \n",
    "#         # Log the hyperparameters and results\n",
    "#         params = {\n",
    "#             \"weight_decay\": weight_decay,\n",
    "#             \"warmup_ratio\": warmup_ratio,\n",
    "#             \"dropout\": dropout,\n",
    "#         }\n",
    "        \n",
    "#         with open(trial_dir / \"hyperparameters.json\", \"w\") as f:\n",
    "#             json.dump({**params, **eval_results}, f, indent=2)\n",
    "        \n",
    "#         # Return Matthews Correlation Coefficient as the objective value\n",
    "#         return eval_results[\"eval_MCC\"]\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Trial {trial.number} failed with error: {e}\")\n",
    "#         # Return very bad score for failed trials\n",
    "#         return -1.0\n",
    "\n",
    "# def run_optuna_experiment():\n",
    "#     \"\"\"Run Optuna hyperparameter optimization experiment.\"\"\"\n",
    "#     logging.info(\"Starting hyperparameter optimization with Optuna...\")\n",
    "    \n",
    "#     # Create output directory for study\n",
    "#     study_dir = SAVE_DIR / \"optuna_study\"\n",
    "#     study_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "#     # Create a pruner to terminate unpromising trials\n",
    "#     pruner = optuna.pruners.MedianPruner()\n",
    "    \n",
    "#     # Create a storage for the study\n",
    "#     storage_name = f\"sqlite:///{study_dir}/optuna_study.db\"\n",
    "    \n",
    "#     # Create TPE sampler for Bayesian optimization\n",
    "#     sampler = TPESampler(seed=42)\n",
    "    \n",
    "#     # Create the study\n",
    "#     study = optuna.create_study(\n",
    "#         direction=\"maximize\",\n",
    "#         pruner=pruner,\n",
    "#         storage=storage_name,\n",
    "#         study_name=\"deberta_claim_evidence\",\n",
    "#         load_if_exists=True,\n",
    "#         sampler=sampler\n",
    "#     )\n",
    "    \n",
    "#     # Run optimization\n",
    "#     study.optimize(objective, n_trials=N_TRIALS)\n",
    "    \n",
    "#     # Get best trial\n",
    "#     best_trial = study.best_trial\n",
    "    \n",
    "#     # Log additional information about the Bayesian optimization\n",
    "#     logging.info(f\"Using Bayesian optimization with TPE sampler\")\n",
    "#     logging.info(f\"Best trial: {best_trial.number}\")\n",
    "#     logging.info(f\"Best value: {best_trial.value}\")\n",
    "#     logging.info(\"Best hyperparameters:\")\n",
    "    \n",
    "#     for param, value in best_trial.params.items():\n",
    "#         logging.info(f\"\\t{param}: {value}\")\n",
    "    \n",
    "#     # Save best parameters\n",
    "#     best_params = {\n",
    "#         \"weight_decay\": best_trial.params[\"weight_decay\"],\n",
    "#         \"warmup_ratio\": best_trial.params[\"warmup_ratio\"],\n",
    "#         \"dropout\": best_trial.params[\"dropout\"],\n",
    "#         \"mcc_score\": best_trial.value\n",
    "#     }\n",
    "    \n",
    "#     with open(study_dir / \"best_params.json\", \"w\") as f:\n",
    "#         json.dump(best_params, f, indent=2)\n",
    "    \n",
    "#     # Plot optimization history\n",
    "#     fig = optuna.visualization.plot_optimization_history(study)\n",
    "#     fig.write_html(str(study_dir / \"optimization_history.html\"))\n",
    "    \n",
    "#     # Plot parameter importance\n",
    "#     fig = optuna.visualization.plot_param_importances(study)\n",
    "#     fig.write_html(str(study_dir / \"param_importances.html\"))\n",
    "    \n",
    "#     # Plot parameter relationships\n",
    "#     fig = optuna.visualization.plot_parallel_coordinate(study)\n",
    "#     fig.write_html(str(study_dir / \"parallel_coordinate.html\"))\n",
    "    \n",
    "#     # Plot high-dimensional parameter relationships\n",
    "#     fig = optuna.visualization.plot_contour(study)\n",
    "#     fig.write_html(str(study_dir / \"contour.html\"))\n",
    "    \n",
    "#     return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Run Optuna hyperparameter optimization\n",
    "    # best_params = run_optuna_experiment()\n",
    "    \n",
    "    # Optional: Train final model with best parameters\n",
    "    print(\"Training final model with best parameters...\")\n",
    "    \n",
    "    # Free GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "    )\n",
    "\n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        BASE_MODEL, \n",
    "        num_labels=2,\n",
    "        hidden_dropout_prob=DROPOUT_RATE,\n",
    "        attention_probs_dropout_prob=DROPOUT_RATE,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.to(device)\n",
    "\n",
    "    # Load data with best max_seq_length\n",
    "    train_dataset, dev_dataset, dev_df = load_data(tokenizer, MAX_SEQ_LENGTH)\n",
    "    \n",
    "    # Training parameters with best hyperparameters\n",
    "    training_params = {\n",
    "        'per_device_train_batch_size': BATCH_SIZE,\n",
    "        'per_device_eval_batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'num_train_epochs': NUM_EPOCHS,\n",
    "        'warmup_ratio': WARMUP_RATIO,\n",
    "        'lr_scheduler_type': 'cosine_with_restarts',\n",
    "        'eval_strategy': 'steps',\n",
    "        'eval_steps': 500,\n",
    "        'save_strategy': 'steps',\n",
    "        'save_steps': 500,\n",
    "        'save_total_limit': 5,\n",
    "        'load_best_model_at_end': True,\n",
    "        'metric_for_best_model': 'MCC',\n",
    "        'fp16': device.type == 'cuda', \n",
    "        'optim': 'adamw_torch',\n",
    "        'logging_steps': 100,\n",
    "        'logging_first_step': True,\n",
    "        'group_by_length': True,\n",
    "        'seed': 42,\n",
    "        'dataloader_num_workers': 4,\n",
    "        'label_smoothing_factor': 0.05,\n",
    "        'max_grad_norm': 1.0,\n",
    "        'gradient_checkpointing': True,\n",
    "    }\n",
    "    \n",
    "    # Train with best parameters\n",
    "    model_save_path = SAVE_DIR / f\"{BASE_MODEL.split('/')[-1]}_best\"\n",
    "    eval_results = train_model(\n",
    "        model,\n",
    "        train_dataset,\n",
    "        dev_dataset,\n",
    "        model_save_path,\n",
    "        tokenizer,\n",
    "        **training_params\n",
    "    )\n",
    "    \n",
    "    print(f\"Final evaluation results: {eval_results}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7040057,
     "sourceId": 11263413,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
