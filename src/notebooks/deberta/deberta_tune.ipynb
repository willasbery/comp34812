{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets huggingface_hub optuna tensorboard peft\n",
    "!pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    matthews_corrcoef,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Disable wandb\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "\n",
    "# Path configuration\n",
    "DATA_DIR = Path(\"/kaggle/working/\")\n",
    "TRAIN_FILE = \"/kaggle/input/ed-uom/train.csv\"\n",
    "DEV_FILE = \"/kaggle/input/ed-uom/dev.csv\"\n",
    "AUG_TRAIN_FILE = \"/kaggle/input/ed-uom/train_augmented.csv\"\n",
    "ANOTHER_AUG_FILE = \"/kaggle/input/ed-uom/positive_examples.csv\"\n",
    "AUG_TRAIN_HIGH_REPLACEMENT_FILE = DATA_DIR / \"train_augmented_high_replacement_fraction.csv\"\n",
    "SAVE_DIR = DATA_DIR / \"results\" / \"transformer\"\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.0015\n",
    "WARMUP_RATIO = 0.12\n",
    "DROPOUT_RATE = 0.05\n",
    "MAX_SEQ_LENGTH = 512\n",
    "# BASE_MODEL = 'cross-encoder/nli-deberta-v3-large'\n",
    "BASE_MODEL = 'microsoft/deberta-v2-xlarge-mnli'\n",
    "\n",
    "# Optuna parameters\n",
    "N_TRIALS = 6\n",
    "\n",
    "# Hyperparameter search space\n",
    "BATCH_SIZES = [8]\n",
    "LEARNING_RATES = [1e-4]\n",
    "WEIGHT_DECAYS = [0.0001, 0.001, 0.01]\n",
    "WARMUP_RATIOS = [0.1, 0.15, 0.2]\n",
    "DROPOUT_RATES = [0.05, 0.1]\n",
    "MAX_SEQ_LENGTHS = [512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    \"\"\"Determine the device to use for computations.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def preprocess_function(examples, tokenizer, max_seq_length):\n",
    "    \"\"\"Process examples for BERT/DeBERTa classification.\"\"\"\n",
    "    # Combine claim and evidence\n",
    "    claims = []\n",
    "    evidences = []\n",
    "\n",
    "    # Create inputs and targets\n",
    "    for claim, evidence in zip(examples['Claim'], examples['Evidence']):\n",
    "        formatted_claim = f\"Claim: {claim}\"\n",
    "        formatted_evidence = f\"Evidence: {evidence}\"\n",
    "        claims.append(formatted_claim)\n",
    "        evidences.append(formatted_evidence)\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        claims,\n",
    "        evidences,\n",
    "        max_length=max_seq_length,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    # Add labels (binary classification)\n",
    "    model_inputs[\"labels\"] = examples['label']\n",
    "    return model_inputs\n",
    "\n",
    "def convert_to_hf_dataset(dataframe):\n",
    "    \"\"\"Convert pandas dataframe to HuggingFace dataset format.\"\"\"\n",
    "    return HFDataset.from_pandas(dataframe)\n",
    "\n",
    "def load_data(tokenizer, max_seq_length):\n",
    "    \"\"\"Load and prepare the training and development datasets.\"\"\"\n",
    "    logging.info(\"Loading datasets...\")\n",
    "    \n",
    "    # Load CSV files into pandas dataframes\n",
    "    train_df = pd.read_csv(AUG_TRAIN_FILE)\n",
    "    dev_df = pd.read_csv(DEV_FILE)\n",
    "\n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Development data shape: {dev_df.shape}\")\n",
    "    \n",
    "    # Check and report class distribution\n",
    "    train_positive = (train_df['label'] == 1).sum()\n",
    "    train_negative = (train_df['label'] == 0).sum()\n",
    "    dev_positive = (dev_df['label'] == 1).sum()\n",
    "    dev_negative = (dev_df['label'] == 0).sum()\n",
    "    \n",
    "    print(f\"Training data distribution: Positive: {train_positive} ({train_positive/len(train_df)*100:.1f}%), \"\n",
    "                 f\"Negative: {train_negative} ({train_negative/len(train_df)*100:.1f}%)\")\n",
    "    print(f\"Dev data distribution: Positive: {dev_positive} ({dev_positive/len(dev_df)*100:.1f}%), \"\n",
    "                 f\"Negative: {dev_negative} ({dev_negative/len(dev_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Add a sequential index to keep track of original order (if not already present)\n",
    "    if 'original_index' not in dev_df.columns:\n",
    "        dev_df['original_index'] = list(range(len(dev_df)))\n",
    "    \n",
    "    # Convert to HuggingFace datasets\n",
    "    train_dataset = convert_to_hf_dataset(train_df)\n",
    "    dev_dataset = convert_to_hf_dataset(dev_df)\n",
    "    \n",
    "    # Apply preprocessing (tokenization)\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda examples: preprocess_function(examples, tokenizer, max_seq_length),\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        remove_columns=['Claim', 'Evidence', 'label']\n",
    "    )\n",
    "    \n",
    "    # For dev dataset, keep track of original indices but remove other columns\n",
    "    columns_to_remove = [col for col in dev_df.columns if col not in ['original_index']]\n",
    "    dev_dataset = dev_dataset.map(\n",
    "        lambda examples: preprocess_function(examples, tokenizer, max_seq_length),\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        remove_columns=columns_to_remove\n",
    "    )\n",
    "    \n",
    "    # Set format for pytorch\n",
    "    train_dataset.set_format(type='torch')\n",
    "    dev_dataset.set_format(type='torch')\n",
    "    \n",
    "    return train_dataset, dev_dataset, dev_df\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calculate evaluation metrics for classification.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # For binary classification, get the predicted class (0 or 1)\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    \n",
    "    # Calculate metrics with more focus on positive class\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Get more detailed metrics for both classes\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Weighted metrics\n",
    "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Matthews Correlation Coefficient\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "    \n",
    "    # Return both class-specific and overall metrics\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Positive_Precision': precision[1] if len(precision) > 1 else 0,\n",
    "        'Positive_Recall': recall[1] if len(recall) > 1 else 0,\n",
    "        'Positive_F1': f1[1] if len(f1) > 1 else 0,\n",
    "        'W Macro-P': weighted_precision,\n",
    "        'W Macro-R': weighted_recall,\n",
    "        'W Macro-F1': weighted_f1,\n",
    "        'MCC': mcc\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, save_path):\n",
    "    \"\"\"Plot and save confusion matrix.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    classes = ['Negative', 'Positive']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, f'{cm[i, j]}\\n({cm_norm[i, j]:.2f})',\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    output_dir,\n",
    "    tokenizer,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Train the classification model.\"\"\"\n",
    "    logging.info(\"Starting training...\")\n",
    "    \n",
    "    # Free up CUDA memory before training\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Create data collator for dynamic padding\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding='longest'\n",
    "    )\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        greater_is_better=True,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate on dev set and plot confusion matrix\n",
    "    eval_results = trainer.evaluate()\n",
    "    dev_preds = trainer.predict(eval_dataset)\n",
    "    y_true = dev_preds.label_ids\n",
    "    y_pred = dev_preds.predictions.argmax(axis=1)\n",
    "\n",
    "    # Save predictions to a CSV file with original dev data for alignment\n",
    "    # First, load the original dev CSV to maintain alignment\n",
    "    dev_df = pd.read_csv(DEV_FILE)\n",
    "    \n",
    "    # Create a dataframe with predictions\n",
    "    predictions_df = pd.DataFrame({'prediction': y_pred})\n",
    "\n",
    "    \n",
    "    \n",
    "    # Check if the evaluation dataset has original indices\n",
    "    if hasattr(eval_dataset, 'original_index') or 'original_index' in eval_dataset.features:\n",
    "        # Get original indices if present\n",
    "        try:\n",
    "            original_indices = [item['original_index'] for item in eval_dataset]\n",
    "            # Sort predictions by original index\n",
    "            predictions_df['original_index'] = original_indices\n",
    "            predictions_df = predictions_df.sort_values('original_index')\n",
    "            del predictions_df['original_index']  # Remove after sorting\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Couldn't use original indices: {e}\")\n",
    "    \n",
    "    # Ensure the predictions align with the original data\n",
    "    if len(dev_df) == len(predictions_df):\n",
    "        # Add predictions to the original dev dataframe\n",
    "        dev_df['prediction'] = predictions_df['prediction'].values\n",
    "        predictions_csv_path = os.path.join(output_dir, \"predictions_with_data.csv\")\n",
    "        dev_df.to_csv(predictions_csv_path, index=False)\n",
    "        print(f\"Predictions with original data saved to {predictions_csv_path}\")\n",
    "        \n",
    "        # Also save just the predictions for convenience\n",
    "        predictions_only_path = os.path.join(output_dir, \"predictions.csv\")\n",
    "        predictions_df.to_csv(predictions_only_path, index=False)\n",
    "    else:\n",
    "        print(f\"Prediction count ({len(predictions_df)}) doesn't match dev data count ({len(dev_df)})\")\n",
    "        # Save just the predictions\n",
    "        predictions_csv_path = os.path.join(output_dir, \"predictions.csv\")\n",
    "        predictions_df.to_csv(predictions_csv_path, index=False)\n",
    "        print(f\"Predictions saved to {predictions_csv_path}\")\n",
    "    \n",
    "    # Plot and save confusion matrix\n",
    "    cm_save_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
    "    plot_confusion_matrix(y_true, y_pred, cm_save_path)\n",
    "    \n",
    "    trainer.save_model()\n",
    "    logging.info(f\"Model saved to {output_dir}\")\n",
    "    \n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function for hyperparameter optimization.\"\"\"\n",
    "    # Get hyperparameters from trial\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", WEIGHT_DECAYS[0], WEIGHT_DECAYS[-1], log=True)\n",
    "    warmup_ratio = trial.suggest_float(\"warmup_ratio\", WARMUP_RATIOS[0], WARMUP_RATIOS[-1])\n",
    "    dropout = trial.suggest_float(\"dropout\", DROPOUT_RATES[0], DROPOUT_RATES[-1])\n",
    "    \n",
    "    device = get_device()\n",
    "    logging.info(f\"Trial {trial.number}: Using device: {device}\")\n",
    "    \n",
    "    # Free GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # LoRA configuration (fixed for all trials)\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "    )\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        BASE_MODEL, \n",
    "        num_labels=2,\n",
    "        hidden_dropout_prob=dropout,\n",
    "        attention_probs_dropout_prob=dropout,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Load data with current max_seq_length\n",
    "    train_dataset, dev_dataset, dev_df = load_data(tokenizer, MAX_SEQ_LENGTH)\n",
    "    \n",
    "    # Training parameters\n",
    "    training_params = {\n",
    "        'per_device_train_batch_size': BATCH_SIZE,\n",
    "        'per_device_eval_batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'weight_decay': weight_decay,\n",
    "        'num_train_epochs': NUM_EPOCHS,\n",
    "        'warmup_ratio': warmup_ratio,\n",
    "        'lr_scheduler_type': 'cosine_with_restarts',\n",
    "        'eval_strategy': 'steps',\n",
    "        'eval_steps': 500,\n",
    "        'save_strategy': 'steps',\n",
    "        'save_steps': 500,\n",
    "        'save_total_limit': 2,\n",
    "        'load_best_model_at_end': True,\n",
    "        'metric_for_best_model': 'MCC',\n",
    "        'fp16': device.type == 'cuda', \n",
    "        'optim': 'adamw_torch',\n",
    "        'logging_steps': 100,\n",
    "        'logging_first_step': True,\n",
    "        'group_by_length': True,\n",
    "        'seed': 42,\n",
    "        'dataloader_num_workers': 4,\n",
    "        'label_smoothing_factor': 0.05,\n",
    "        'max_grad_norm': 1.0,\n",
    "        'gradient_checkpointing': True,\n",
    "    }\n",
    "    \n",
    "    # Set trial output directory\n",
    "    trial_dir = SAVE_DIR / f\"trial_{trial.number}\"\n",
    "    \n",
    "    try:\n",
    "        # Train with current hyperparameters\n",
    "        eval_results = train_model(\n",
    "            model,\n",
    "            train_dataset,\n",
    "            dev_dataset,\n",
    "            trial_dir,\n",
    "            tokenizer,\n",
    "            **training_params\n",
    "        )\n",
    "        \n",
    "        # Log the hyperparameters and results\n",
    "        params = {\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"warmup_ratio\": warmup_ratio,\n",
    "            \"dropout\": dropout,\n",
    "        }\n",
    "        \n",
    "        with open(trial_dir / \"hyperparameters.json\", \"w\") as f:\n",
    "            json.dump({**params, **eval_results}, f, indent=2)\n",
    "        \n",
    "        # Return Matthews Correlation Coefficient as the objective value\n",
    "        return eval_results[\"eval_MCC\"]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number} failed with error: {e}\")\n",
    "        # Return very bad score for failed trials\n",
    "        return -1.0\n",
    "\n",
    "def run_optuna_experiment():\n",
    "    \"\"\"Run Optuna hyperparameter optimization experiment.\"\"\"\n",
    "    logging.info(\"Starting hyperparameter optimization with Optuna...\")\n",
    "    \n",
    "    # Create output directory for study\n",
    "    study_dir = SAVE_DIR / \"optuna_study\"\n",
    "    study_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create a pruner to terminate unpromising trials\n",
    "    pruner = optuna.pruners.MedianPruner()\n",
    "    \n",
    "    # Create a storage for the study\n",
    "    storage_name = f\"sqlite:///{study_dir}/optuna_study.db\"\n",
    "    \n",
    "    # Create TPE sampler for Bayesian optimization\n",
    "    sampler = TPESampler(seed=42)\n",
    "    \n",
    "    # Create the study\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        pruner=pruner,\n",
    "        storage=storage_name,\n",
    "        study_name=\"deberta_claim_evidence\",\n",
    "        load_if_exists=True,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # Run optimization\n",
    "    study.optimize(objective, n_trials=N_TRIALS)\n",
    "    \n",
    "    # Get best trial\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    # Log additional information about the Bayesian optimization\n",
    "    logging.info(f\"Using Bayesian optimization with TPE sampler\")\n",
    "    logging.info(f\"Best trial: {best_trial.number}\")\n",
    "    logging.info(f\"Best value: {best_trial.value}\")\n",
    "    logging.info(\"Best hyperparameters:\")\n",
    "    \n",
    "    for param, value in best_trial.params.items():\n",
    "        logging.info(f\"\\t{param}: {value}\")\n",
    "    \n",
    "    # Save best parameters\n",
    "    best_params = {\n",
    "        \"weight_decay\": best_trial.params[\"weight_decay\"],\n",
    "        \"warmup_ratio\": best_trial.params[\"warmup_ratio\"],\n",
    "        \"dropout\": best_trial.params[\"dropout\"],\n",
    "        \"mcc_score\": best_trial.value\n",
    "    }\n",
    "    \n",
    "    with open(study_dir / \"best_params.json\", \"w\") as f:\n",
    "        json.dump(best_params, f, indent=2)\n",
    "    \n",
    "    # Plot optimization history\n",
    "    fig = optuna.visualization.plot_optimization_history(study)\n",
    "    fig.write_html(str(study_dir / \"optimization_history.html\"))\n",
    "    \n",
    "    # Plot parameter importance\n",
    "    fig = optuna.visualization.plot_param_importances(study)\n",
    "    fig.write_html(str(study_dir / \"param_importances.html\"))\n",
    "    \n",
    "    # Plot parameter relationships\n",
    "    fig = optuna.visualization.plot_parallel_coordinate(study)\n",
    "    fig.write_html(str(study_dir / \"parallel_coordinate.html\"))\n",
    "    \n",
    "    # Plot high-dimensional parameter relationships\n",
    "    fig = optuna.visualization.plot_contour(study)\n",
    "    fig.write_html(str(study_dir / \"contour.html\"))\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    device = get_device()\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Run Optuna hyperparameter optimization\n",
    "    # best_params = run_optuna_experiment()\n",
    "    \n",
    "    # Optional: Train final model with best parameters\n",
    "    logging.info(\"Training final model with best parameters...\")\n",
    "    \n",
    "    # Free GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "    )\n",
    "\n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        BASE_MODEL, \n",
    "        num_labels=2,\n",
    "        hidden_dropout_prob=DROPOUT_RATE,\n",
    "        attention_probs_dropout_prob=DROPOUT_RATE,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.to(device)\n",
    "\n",
    "    # Load data with best max_seq_length\n",
    "    train_dataset, dev_dataset, dev_df = load_data(tokenizer, MAX_SEQ_LENGTH)\n",
    "    \n",
    "    # Training parameters with best hyperparameters\n",
    "    training_params = {\n",
    "        'per_device_train_batch_size': BATCH_SIZE,\n",
    "        'per_device_eval_batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'num_train_epochs': NUM_EPOCHS,\n",
    "        'warmup_ratio': WARMUP_RATIO,\n",
    "        'lr_scheduler_type': 'cosine_with_restarts',\n",
    "        'eval_strategy': 'steps',\n",
    "        'eval_steps': 500,\n",
    "        'save_strategy': 'steps',\n",
    "        'save_steps': 500,\n",
    "        'save_total_limit': 5,\n",
    "        'load_best_model_at_end': True,\n",
    "        'metric_for_best_model': 'MCC',\n",
    "        'fp16': device.type == 'cuda', \n",
    "        'optim': 'adamw_torch',\n",
    "        'logging_steps': 100,\n",
    "        'logging_first_step': True,\n",
    "        'group_by_length': True,\n",
    "        'seed': 42,\n",
    "        'dataloader_num_workers': 4,\n",
    "        'label_smoothing_factor': 0.05,\n",
    "        'max_grad_norm': 1.0,\n",
    "        'gradient_checkpointing': True,\n",
    "    }\n",
    "    \n",
    "    # Train with best parameters\n",
    "    model_save_path = SAVE_DIR / f\"{BASE_MODEL.split('/')[-1]}_best\"\n",
    "    eval_results = train_model(\n",
    "        model,\n",
    "        train_dataset,\n",
    "        dev_dataset,\n",
    "        model_save_path,\n",
    "        tokenizer,\n",
    "        **training_params\n",
    "    )\n",
    "    \n",
    "    print(f\"Final evaluation results: {eval_results}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7040057,
     "sourceId": 11263413,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
