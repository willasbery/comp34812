{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 26 - Deep learning with transformers -  Demo Code\n",
    "## DeBERTa\n",
    "\n",
    "#### Harvey Dennis and William Asbery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "__PLEASE RUN THE CELLS BELOW__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets huggingface_hub optuna tensorboard peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    matthews_corrcoef,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from safetensors.torch import load_file as load_safetensors_file\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    level=print\n",
    ")\n",
    "\n",
    "# Disable wandb\n",
    "os.environ['WANDB_DISABLED'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "\n",
    "This config specifies the configuration for the paths of the files being used to train, validate and test the model.\n",
    "\n",
    "Please add the relative or absolute paths (from Kaggle) to the train, dev, test and augmented train files. We provide you with an augmented train file as the augmentation pipeline takes about 2 hours to run.\n",
    "\n",
    "__PLEASE RUN THE CELL BELOW.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path configuration\n",
    "DATA_DIR = Path(\"/kaggle/working/\")\n",
    "TRAIN_FILE = \"/kaggle/input/ed-uom/train.csv\"\n",
    "DEV_FILE = \"/kaggle/input/ed-uom/dev.csv\"\n",
    "AUG_TRAIN_FILE = \"/kaggle/input/ed-uom/train_augmented.csv\"\n",
    "NEW_AUG = \"/kaggle/input/ed-uom/train_augmented_new.csv\"\n",
    "\n",
    "SAVE_DIR = DATA_DIR / \"results\" / \"transformer\"\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best params from tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 0.03\n",
    "WARMUP_RATIO = 0.11\n",
    "DROPOUT_RATE = 0.05\n",
    "FF_DROPOUT_RATE = 0.05\n",
    "MAX_SEQ_LENGTH = 512\n",
    "BASE_MODEL = 'microsoft/deberta-v3-large'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo code: run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"Determine the device to use for computations.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def prepare_input(claim: str, evidence: str, tokenizer, max_length: int, device: torch.device):\n",
    "    \"\"\"Formats and tokenizes a single claim-evidence pair.\"\"\"\n",
    "    # --- Reuses the formatting logic from preprocess_function ---\n",
    "    formatted_claim = f\"Claim: {claim}\"\n",
    "    formatted_evidence = f\"Evidence: {evidence}\"\n",
    "\n",
    "    # --- Reuses the tokenization logic ---\n",
    "    inputs = tokenizer(\n",
    "        formatted_claim,\n",
    "        formatted_evidence,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\", # Or another appropriate padding strategy\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    return inputs\n",
    "\n",
    "# --- Main Prediction Logic ---\n",
    "def run_predictions(model_path: str, input_csv_path: str, output_csv_path: str):\n",
    "    \"\"\"Loads model MANUALLY, reads CSV, makes predictions, and saves results.\"\"\"\n",
    "\n",
    "    # 1. Load Tokenizer and Config (as before)\n",
    "    print(f\"Loading tokenizer from: {model_path}\")\n",
    "    device = get_device()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    config = AutoConfig.from_pretrained(model_path) # Load config separately\n",
    "\n",
    "    # 2. *** Manually Construct the Model Architecture *** (same as before)\n",
    "    print(\"Constructing model architecture...\")\n",
    "    model = AutoModelForSequenceClassification.from_config(config)\n",
    "    hidden_size = model.classifier.in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(hidden_size, hidden_size),\n",
    "        nn.GELU(),\n",
    "        nn.LayerNorm(hidden_size),\n",
    "        nn.Dropout(FF_DROPOUT_RATE),\n",
    "        nn.Linear(hidden_size, config.num_labels)\n",
    "    )\n",
    "    print(\"Custom classifier head applied.\")\n",
    "\n",
    "    # 3. *** Load the Saved Weights (State Dictionary) - MODIFIED ***\n",
    "    safetensors_path = os.path.join(model_path, \"model.safetensors\")\n",
    "    pytorch_bin_path = os.path.join(model_path, \"pytorch_model.bin\")\n",
    "\n",
    "    state_dict = None\n",
    "    weights_loaded_from = None\n",
    "\n",
    "    if os.path.exists(safetensors_path):\n",
    "        print(f\"Loading weights from SafeTensors file: {safetensors_path}...\")\n",
    "        try:\n",
    "            state_dict = load_safetensors_file(safetensors_path, device='cpu') # Load using safetensors library\n",
    "            weights_loaded_from = safetensors_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading safetensors file: {e}\")\n",
    "            # Optionally, try pytorch_model.bin if safetensors fails\n",
    "            if os.path.exists(pytorch_bin_path):\n",
    "                 print(f\"Attempting to load pytorch_model.bin instead...\")\n",
    "            else:\n",
    "                 return # Stop if neither format seems to work\n",
    "\n",
    "    if state_dict is None and os.path.exists(pytorch_bin_path):\n",
    "        print(f\"Loading weights from PyTorch bin file: {pytorch_bin_path}...\")\n",
    "        try:\n",
    "            # Use weights_only=True for security as recommended by the warning\n",
    "            state_dict = torch.load(pytorch_bin_path, map_location='cpu', weights_only=True)\n",
    "            weights_loaded_from = pytorch_bin_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pytorch_model.bin file: {e}. This might indicate corruption.\")\n",
    "            print(\"Please ensure the model saving process completed successfully.\")\n",
    "            return # Stop if loading fails\n",
    "\n",
    "    if state_dict is None:\n",
    "        print(f\"Error: No weight file (model.safetensors or pytorch_model.bin) found or loaded successfully in {model_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Weights loaded successfully from {weights_loaded_from}\")\n",
    "\n",
    "    # Load the state dict into the manually constructed model\n",
    "    try:\n",
    "        model.load_state_dict(state_dict)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error loading state dict into model: {e}\")\n",
    "        print(\"This often means the manually constructed architecture doesn't match the keys in the weights file.\")\n",
    "        print(\"Ensure the custom classifier definition EXACTLY matches the one used during training.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    model.to(device) # Move the complete model to the target device\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    print(f\"Model constructed and weights loaded. Using device: {device}\")\n",
    "\n",
    "    # 4. Read Input CSV (same as before)\n",
    "    print(f\"Reading input CSV: {input_csv_path}\")\n",
    "    try:\n",
    "        input_df = pd.read_csv(input_csv_path)\n",
    "        if 'Claim' not in input_df.columns or 'Evidence' not in input_df.columns:\n",
    "            raise ValueError(\"Input CSV must contain 'Claim' and 'Evidence' columns.\")\n",
    "        print(f\"Loaded {len(input_df)} rows from {input_csv_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input CSV file not found at {input_csv_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return\n",
    "\n",
    "    # 5. Make Predictions (same as before)\n",
    "    predictions = []\n",
    "    print(\"Making predictions...\")\n",
    "    for index, row in tqdm(input_df.iterrows(), total=input_df.shape[0], desc=\"Predicting\"):\n",
    "        claim = str(row['Claim'])\n",
    "        evidence = str(row['Evidence'])\n",
    "        if not claim or not evidence:\n",
    "             print(f\"Warning: Skipping row {index} due to empty Claim or Evidence.\")\n",
    "             predictions.append(None)\n",
    "             continue\n",
    "        inputs = prepare_input(claim, evidence, tokenizer, MAX_SEQ_LENGTH, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted_class_id = torch.argmax(logits, dim=-1).item()\n",
    "            predictions.append(predicted_class_id)\n",
    "\n",
    "    # 6. Save Predictions (same as before)\n",
    "    output_df = pd.DataFrame({'prediction': predictions})\n",
    "    print(f\"Saving predictions to: {output_csv_path}\")\n",
    "    try:\n",
    "        output_df.to_csv(output_csv_path, index=False)\n",
    "        print(\"Predictions saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving predictions: {e}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model_save_path = \"/kaggle/working/results/transformer/deberta-v3-large\"\n",
    "\n",
    "run_predictions(model_save_path, DEV_FILE, 'predictions.csv')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
