{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 26 - Deep learning with transformers\n",
    "## DeBERTa\n",
    "\n",
    "#### Harvey Dennis and William Asbery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "__PLEASE RUN THE CELLS BELOW__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets huggingface_hub optuna tensorboard peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    matthews_corrcoef,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from safetensors.torch import load_file as load_safetensors_file\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    level=print\n",
    ")\n",
    "\n",
    "# Disable wandb\n",
    "os.environ['WANDB_DISABLED'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "\n",
    "This config specifies the configuration for the paths of the files being used to train, validate and test the model.\n",
    "\n",
    "Please add the relative or absolute paths (from Kaggle) to the train, dev, test and augmented train files. We provide you with an augmented train file as the augmentation pipeline takes about 2 hours to run.\n",
    "\n",
    "__PLEASE RUN THE CELL BELOW.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path configuration\n",
    "DATA_DIR = Path(\"/kaggle/working/\")\n",
    "TRAIN_FILE = \"/kaggle/input/ed-uom/train.csv\"\n",
    "DEV_FILE = \"/kaggle/input/ed-uom/dev.csv\"\n",
    "AUG_TRAIN_FILE = \"/kaggle/input/ed-uom/train_augmented.csv\"\n",
    "NEW_AUG = \"/kaggle/input/ed-uom/train_augmented_new.csv\"\n",
    "\n",
    "SAVE_DIR = DATA_DIR / \"results\" / \"transformer\"\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna parameters\n",
    "N_TRIALS = 10\n",
    "\n",
    "BATCH_SIZES = [8, 16, 32, 64]\n",
    "LEARNING_RATES = [1e-5, 2e-5, 3e-5, 4e-5, 5e-5]\n",
    "WEIGHT_DECAYS = [0.001, 0.1]\n",
    "WARMUP_RATIOS = [0.05, 0.15]\n",
    "DROPOUT_RATES = [0.05, 0.2]\n",
    "FF_DROPOUT_RATES = [0.05, 0.2]\n",
    "RANK = [8, 16, 32, 64]\n",
    "ALPHA = [16, 32, 64, 128]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best params from tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 0.03\n",
    "WARMUP_RATIO = 0.11\n",
    "DROPOUT_RATE = 0.05\n",
    "FF_DROPOUT_RATE = 0.05\n",
    "MAX_SEQ_LENGTH = 512\n",
    "BASE_MODEL = 'microsoft/deberta-v3-large'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo code: run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    \"\"\"Determine the device to use for computations.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def prepare_input(claim: str, evidence: str, tokenizer, max_length: int, device: torch.device):\n",
    "    \"\"\"Formats and tokenizes a single claim-evidence pair.\"\"\"\n",
    "    # --- Reuses the formatting logic from preprocess_function ---\n",
    "    formatted_claim = f\"Claim: {claim}\"\n",
    "    formatted_evidence = f\"Evidence: {evidence}\"\n",
    "\n",
    "    # --- Reuses the tokenization logic ---\n",
    "    inputs = tokenizer(\n",
    "        formatted_claim,\n",
    "        formatted_evidence,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\", # Or another appropriate padding strategy\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    return inputs\n",
    "\n",
    "# --- Main Prediction Logic ---\n",
    "def run_predictions(model_path: str, input_csv_path: str, output_csv_path: str):\n",
    "    \"\"\"Loads model MANUALLY, reads CSV, makes predictions, and saves results.\"\"\"\n",
    "\n",
    "    # 1. Load Tokenizer and Config (as before)\n",
    "    print(f\"Loading tokenizer from: {model_path}\")\n",
    "    device = get_device()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    config = AutoConfig.from_pretrained(model_path) # Load config separately\n",
    "\n",
    "    # 2. *** Manually Construct the Model Architecture *** (same as before)\n",
    "    print(\"Constructing model architecture...\")\n",
    "    model = AutoModelForSequenceClassification.from_config(config)\n",
    "    hidden_size = model.classifier.in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(hidden_size, hidden_size),\n",
    "        nn.GELU(),\n",
    "        nn.LayerNorm(hidden_size),\n",
    "        nn.Dropout(FF_DROPOUT_RATE),\n",
    "        nn.Linear(hidden_size, config.num_labels)\n",
    "    )\n",
    "    print(\"Custom classifier head applied.\")\n",
    "\n",
    "    # 3. *** Load the Saved Weights (State Dictionary) - MODIFIED ***\n",
    "    safetensors_path = os.path.join(model_path, \"model.safetensors\")\n",
    "    pytorch_bin_path = os.path.join(model_path, \"pytorch_model.bin\")\n",
    "\n",
    "    state_dict = None\n",
    "    weights_loaded_from = None\n",
    "\n",
    "    if os.path.exists(safetensors_path):\n",
    "        print(f\"Loading weights from SafeTensors file: {safetensors_path}...\")\n",
    "        try:\n",
    "            state_dict = load_safetensors_file(safetensors_path, device='cpu') # Load using safetensors library\n",
    "            weights_loaded_from = safetensors_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading safetensors file: {e}\")\n",
    "            # Optionally, try pytorch_model.bin if safetensors fails\n",
    "            if os.path.exists(pytorch_bin_path):\n",
    "                 print(f\"Attempting to load pytorch_model.bin instead...\")\n",
    "            else:\n",
    "                 return # Stop if neither format seems to work\n",
    "\n",
    "    if state_dict is None and os.path.exists(pytorch_bin_path):\n",
    "        print(f\"Loading weights from PyTorch bin file: {pytorch_bin_path}...\")\n",
    "        try:\n",
    "            # Use weights_only=True for security as recommended by the warning\n",
    "            state_dict = torch.load(pytorch_bin_path, map_location='cpu', weights_only=True)\n",
    "            weights_loaded_from = pytorch_bin_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pytorch_model.bin file: {e}. This might indicate corruption.\")\n",
    "            print(\"Please ensure the model saving process completed successfully.\")\n",
    "            return # Stop if loading fails\n",
    "\n",
    "    if state_dict is None:\n",
    "        print(f\"Error: No weight file (model.safetensors or pytorch_model.bin) found or loaded successfully in {model_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Weights loaded successfully from {weights_loaded_from}\")\n",
    "\n",
    "    # Load the state dict into the manually constructed model\n",
    "    try:\n",
    "        model.load_state_dict(state_dict)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error loading state dict into model: {e}\")\n",
    "        print(\"This often means the manually constructed architecture doesn't match the keys in the weights file.\")\n",
    "        print(\"Ensure the custom classifier definition EXACTLY matches the one used during training.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    model.to(device) # Move the complete model to the target device\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    print(f\"Model constructed and weights loaded. Using device: {device}\")\n",
    "\n",
    "    # 4. Read Input CSV (same as before)\n",
    "    print(f\"Reading input CSV: {input_csv_path}\")\n",
    "    try:\n",
    "        input_df = pd.read_csv(input_csv_path)\n",
    "        if 'Claim' not in input_df.columns or 'Evidence' not in input_df.columns:\n",
    "            raise ValueError(\"Input CSV must contain 'Claim' and 'Evidence' columns.\")\n",
    "        print(f\"Loaded {len(input_df)} rows from {input_csv_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input CSV file not found at {input_csv_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return\n",
    "\n",
    "    # 5. Make Predictions (same as before)\n",
    "    predictions = []\n",
    "    print(\"Making predictions...\")\n",
    "    for index, row in tqdm(input_df.iterrows(), total=input_df.shape[0], desc=\"Predicting\"):\n",
    "        claim = str(row['Claim'])\n",
    "        evidence = str(row['Evidence'])\n",
    "        if not claim or not evidence:\n",
    "             print(f\"Warning: Skipping row {index} due to empty Claim or Evidence.\")\n",
    "             predictions.append(None)\n",
    "             continue\n",
    "        inputs = prepare_input(claim, evidence, tokenizer, MAX_SEQ_LENGTH, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted_class_id = torch.argmax(logits, dim=-1).item()\n",
    "            predictions.append(predicted_class_id)\n",
    "\n",
    "    # 6. Save Predictions (same as before)\n",
    "    output_df = pd.DataFrame({'prediction': predictions})\n",
    "    print(f\"Saving predictions to: {output_csv_path}\")\n",
    "    try:\n",
    "        output_df.to_csv(output_csv_path, index=False)\n",
    "        print(\"Predictions saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving predictions: {e}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model_save_path = \"/kaggle/working/results/transformer/deberta-v3-large\"\n",
    "\n",
    "run_predictions(model_save_path, DEV_FILE, 'predictions.csv')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calculate evaluation metrics for classification.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # For binary classification, get the predicted class (0 or 1)\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    \n",
    "    # Calculate metrics with more focus on positive class\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Get more detailed metrics for both classes\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Weighted metrics\n",
    "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Matthews Correlation Coefficient\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "    \n",
    "    # Return both class-specific and overall metrics\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Positive_Precision': precision[1] if len(precision) > 1 else 0,\n",
    "        'Positive_Recall': recall[1] if len(recall) > 1 else 0,\n",
    "        'Positive_F1': f1[1] if len(f1) > 1 else 0,\n",
    "        'W Macro-P': weighted_precision,\n",
    "        'W Macro-R': weighted_recall,\n",
    "        'W Macro-F1': weighted_f1,\n",
    "        'MCC': mcc\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, save_path):\n",
    "    \"\"\"Plot and save confusion matrix.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    classes = ['Negative', 'Positive']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, f'{cm[i, j]}\\n({cm_norm[i, j]:.2f})',\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, max_seq_length):\n",
    "    \"\"\"Process examples for BERT/DeBERTa classification.\"\"\"\n",
    "    # Combine claim and evidence\n",
    "    claims = []\n",
    "    evidences = []\n",
    "\n",
    "    # Create inputs and targets\n",
    "    for claim, evidence in zip(examples['Claim'], examples['Evidence']):\n",
    "        formatted_claim = f\"Claim: {claim}\"\n",
    "        formatted_evidence = f\"Evidence: {evidence}\"\n",
    "        claims.append(formatted_claim)\n",
    "        evidences.append(formatted_evidence)\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        claims,\n",
    "        evidences,\n",
    "        max_length=max_seq_length,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    # Add labels (binary classification)\n",
    "    model_inputs[\"labels\"] = examples['label']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(tokenizer, max_seq_length):\n",
    "    \"\"\"Load and prepare the training and development datasets.\"\"\"\n",
    "    print(\"Loading datasets...\")\n",
    "    \n",
    "    # Load CSV files into pandas dataframes\n",
    "    train_df = pd.read_csv(AUG_TRAIN_FILE)\n",
    "    dev_df = pd.read_csv(DEV_FILE)\n",
    "    \n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Development data shape: {dev_df.shape}\")\n",
    "    \n",
    "    # Check and report class distribution\n",
    "    train_positive = (train_df['label'] == 1).sum()\n",
    "    train_negative = (train_df['label'] == 0).sum()\n",
    "    dev_positive = (dev_df['label'] == 1).sum()\n",
    "    dev_negative = (dev_df['label'] == 0).sum()\n",
    "    \n",
    "    print(f\"Training data distribution: Positive: {train_positive} ({train_positive/len(train_df)*100:.1f}%), \"\n",
    "                 f\"Negative: {train_negative} ({train_negative/len(train_df)*100:.1f}%)\")\n",
    "    print(f\"Dev data distribution: Positive: {dev_positive} ({dev_positive/len(dev_df)*100:.1f}%), \"\n",
    "                 f\"Negative: {dev_negative} ({dev_negative/len(dev_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Add a sequential index to keep track of original order (if not already present)\n",
    "    if 'original_index' not in dev_df.columns:\n",
    "        dev_df['original_index'] = list(range(len(dev_df)))\n",
    "    \n",
    "    # Convert to HuggingFace datasets\n",
    "    train_dataset = HFDataset.from_pandas(train_df)\n",
    "    dev_dataset = HFDataset.from_pandas(dev_df)\n",
    "    \n",
    "    # Apply preprocessing (tokenization)\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda examples: preprocess_function(examples, tokenizer, max_seq_length),\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        remove_columns=['Claim', 'Evidence', 'label']\n",
    "    )\n",
    "    \n",
    "    # For dev dataset, keep track of original indices but remove other columns\n",
    "    columns_to_remove = [col for col in dev_df.columns if col not in ['original_index']]\n",
    "    dev_dataset = dev_dataset.map(\n",
    "        lambda examples: preprocess_function(examples, tokenizer, max_seq_length),\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        remove_columns=columns_to_remove\n",
    "    )\n",
    "    \n",
    "    # Set format for pytorch\n",
    "    train_dataset.set_format(type='torch')\n",
    "    dev_dataset.set_format(type='torch')\n",
    "    \n",
    "    return train_dataset, dev_dataset, dev_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    output_dir,\n",
    "    tokenizer,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Train the classification model.\"\"\"\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # Free up CUDA memory before training\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Create data collator for dynamic padding\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding='longest'\n",
    "    )\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        greater_is_better=True,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=8)]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Get the final model from the trainer\n",
    "    model = trainer.model\n",
    "\n",
    "    # If using PEFT, merge adapters before saving\n",
    "    if isinstance(model, PeftModel):\n",
    "        print(\"Merging PEFT adapters into the base model...\")\n",
    "        model = model.merge_and_unload()\n",
    "        print(\"Adapters merged.\")\n",
    "\n",
    "    # Save the potentially merged model and tokenizer\n",
    "    print(f\"Saving final model to {output_dir}\")\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved to {output_dir}\")\n",
    "\n",
    "    eval_results = trainer.evaluate(eval_dataset)\n",
    "    dev_preds = trainer.predict(eval_dataset)\n",
    "    y_true = dev_preds.label_ids\n",
    "    y_pred = dev_preds.predictions.argmax(axis=1)\n",
    "\n",
    "    # Save predictions to a CSV file with original dev data for alignment\n",
    "    # First, load the original dev CSV to maintain alignment\n",
    "    dev_df = pd.read_csv(DEV_FILE)\n",
    "    \n",
    "    # Create a dataframe with predictions\n",
    "    predictions_df = pd.DataFrame({'prediction': y_pred})\n",
    "    \n",
    "    # Check if the evaluation dataset has original indices\n",
    "    if hasattr(eval_dataset, 'original_index') or 'original_index' in eval_dataset.features:\n",
    "        # Get original indices if present\n",
    "        try:\n",
    "            original_indices = [item['original_index'] for item in eval_dataset]\n",
    "            # Sort predictions by original index\n",
    "            predictions_df['original_index'] = original_indices\n",
    "            predictions_df = predictions_df.sort_values('original_index')\n",
    "            del predictions_df['original_index']  # Remove after sorting\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Couldn't use original indices: {e}\")\n",
    "    \n",
    "    # Ensure the predictions align with the original data\n",
    "    if len(dev_df) == len(predictions_df):\n",
    "        # Add predictions to the original dev dataframe\n",
    "        dev_df['prediction'] = predictions_df['prediction'].values\n",
    "        predictions_csv_path = os.path.join(output_dir, \"predictions_with_data.csv\")\n",
    "        dev_df.to_csv(predictions_csv_path, index=False)\n",
    "        print(f\"Predictions with original data saved to {predictions_csv_path}\")\n",
    "        \n",
    "        # Also save just the predictions for convenience\n",
    "        predictions_only_path = os.path.join(output_dir, \"predictions.csv\")\n",
    "        predictions_df.to_csv(predictions_only_path, index=False)\n",
    "    else:\n",
    "        print(f\"Prediction count ({len(predictions_df)}) doesn't match dev data count ({len(dev_df)})\")\n",
    "        # Save just the predictions\n",
    "        predictions_csv_path = os.path.join(output_dir, \"predictions.csv\")\n",
    "        predictions_df.to_csv(predictions_csv_path, index=False)\n",
    "        print(f\"Predictions saved to {predictions_csv_path}\")\n",
    "    \n",
    "    # Plot and save confusion matrix\n",
    "    cm_save_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
    "    plot_confusion_matrix(y_true, y_pred, cm_save_path)\n",
    "\n",
    "    print(eval_results)\n",
    "    \n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna hyperparameter optimisation objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function for hyperparameter optimization.\"\"\"\n",
    "    # Get hyperparameters from trial\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", WEIGHT_DECAYS[0], WEIGHT_DECAYS[1], log=True)\n",
    "    warmup_ratio = trial.suggest_float(\"warmup_ratio\", WARMUP_RATIOS[0], WARMUP_RATIOS[1])\n",
    "    \n",
    "    device = get_device()\n",
    "    print(f\"Trial {trial.number}: Using device: {device}\")\n",
    "    \n",
    "    # Free GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        r=8,  # Increased rank\n",
    "        lora_alpha=16,  # Higher scale\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"query_proj\", \"key_proj\", \"value_proj\", \"dense\"],  # Target both attention and FFN\n",
    "        init_lora_weights='pissa',\n",
    "        layers_to_transform=[i for i in range(6, 24)]\n",
    "    )\n",
    "\n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        BASE_MODEL, \n",
    "        num_labels=2,\n",
    "        hidden_dropout_prob=DROPOUT_RATE,\n",
    "        attention_probs_dropout_prob=DROPOUT_RATE,\n",
    "    )\n",
    "\n",
    "    hidden_size = model.classifier.in_features\n",
    "    # Add custom classification head\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(hidden_size, hidden_size),\n",
    "        nn.GELU(),\n",
    "        nn.LayerNorm(hidden_size),\n",
    "        nn.Dropout(FF_DROPOUT_RATE),\n",
    "        nn.Linear(hidden_size, 2)\n",
    "    )\n",
    "    model.config.num_labels = 2\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Load data with current max_seq_length\n",
    "    train_dataset, dev_dataset, dev_df = load_data(tokenizer, MAX_SEQ_LENGTH)\n",
    "    \n",
    "    # Training parameters\n",
    "    training_params = {\n",
    "        'per_device_train_batch_size': BATCH_SIZE,\n",
    "        'per_device_eval_batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'weight_decay': weight_decay,\n",
    "        'num_train_epochs': NUM_EPOCHS,\n",
    "        'warmup_ratio': warmup_ratio,\n",
    "        'lr_scheduler_type': 'cosine',\n",
    "        'eval_strategy': 'steps',\n",
    "        'eval_steps': 1000,\n",
    "        'save_strategy': 'steps',\n",
    "        'save_steps': 1000,\n",
    "        'save_total_limit': 1,\n",
    "        'load_best_model_at_end': True,\n",
    "        'metric_for_best_model': 'MCC',\n",
    "        'fp16': torch.cuda.is_available(),\n",
    "        'optim': 'adamw_torch',\n",
    "        'logging_steps': 100,\n",
    "        'logging_first_step': True,\n",
    "        'group_by_length': True,\n",
    "        'seed': 42,\n",
    "    }\n",
    "    \n",
    "    # Set trial output directory\n",
    "    trial_dir = SAVE_DIR / f\"trial_{trial.number}\"\n",
    "    \n",
    "    try:\n",
    "        # Train with current hyperparameters\n",
    "        eval_results = train_model(\n",
    "            model,\n",
    "            train_dataset,\n",
    "            dev_dataset,\n",
    "            trial_dir,\n",
    "            tokenizer,\n",
    "            **training_params\n",
    "        )\n",
    "        \n",
    "        # Log the hyperparameters and results\n",
    "        params = {\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"warmup_ratio\": warmup_ratio,\n",
    "        }\n",
    "        \n",
    "        with open(trial_dir / \"hyperparameters.json\", \"w\") as f:\n",
    "            json.dump({**params, **eval_results}, f, indent=2)\n",
    "        \n",
    "        # Return Matthews Correlation Coefficient as the objective value\n",
    "        return eval_results[\"eval_MCC\"]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number} failed with error: {e}\")\n",
    "        # Return very bad score for failed trials\n",
    "        return -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and run hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna_experiment():\n",
    "    \"\"\"Run Optuna hyperparameter optimization experiment.\"\"\"\n",
    "    print(\"Starting hyperparameter optimization with Optuna...\")\n",
    "    \n",
    "    # Create output directory for study\n",
    "    study_dir = SAVE_DIR / \"optuna_study\"\n",
    "    study_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create a pruner to terminate unpromising trials\n",
    "    pruner = optuna.pruners.MedianPruner()\n",
    "    \n",
    "    # Create a storage for the study\n",
    "    storage_name = f\"sqlite:///{study_dir}/optuna_study.db\"\n",
    "    \n",
    "    # Create TPE sampler for Bayesian optimization\n",
    "    sampler = TPESampler(seed=42)\n",
    "    \n",
    "    # Create the study\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        pruner=pruner,\n",
    "        storage=storage_name,\n",
    "        study_name=\"deberta_claim_evidence\",\n",
    "        load_if_exists=True,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    # Run optimization\n",
    "    study.optimize(objective, n_trials=N_TRIALS)\n",
    "    \n",
    "    # Get best trial\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    # Log additional information about the Bayesian optimization\n",
    "    print(f\"Using Bayesian optimization with TPE sampler\")\n",
    "    print(f\"Best trial: {best_trial.number}\")\n",
    "    print(f\"Best value: {best_trial.value}\")\n",
    "    print(\"Best hyperparameters:\")\n",
    "    \n",
    "    for param, value in best_trial.params.items():\n",
    "        print(f\"\\t{param}: {value}\")\n",
    "    \n",
    "    # Save best parameters\n",
    "    best_params = {\n",
    "        \"weight_decay\": best_trial.params[\"weight_decay\"],\n",
    "        \"warmup_ratio\": best_trial.params[\"warmup_ratio\"],\n",
    "    }\n",
    "    \n",
    "    with open(study_dir / \"best_params.json\", \"w\") as f:\n",
    "        json.dump(best_params, f, indent=2)\n",
    "    \n",
    "    # Plot optimization history\n",
    "    fig = optuna.visualization.plot_optimization_history(study)\n",
    "    fig.write_html(str(study_dir / \"optimization_history.html\"))\n",
    "    \n",
    "    # Plot parameter importance\n",
    "    fig = optuna.visualization.plot_param_importances(study)\n",
    "    fig.write_html(str(study_dir / \"param_importances.html\"))\n",
    "    \n",
    "    # Plot parameter relationships\n",
    "    fig = optuna.visualization.plot_parallel_coordinate(study)\n",
    "    fig.write_html(str(study_dir / \"parallel_coordinate.html\"))\n",
    "    \n",
    "    # Plot high-dimensional parameter relationships\n",
    "    fig = optuna.visualization.plot_contour(study)\n",
    "    fig.write_html(str(study_dir / \"contour.html\"))\n",
    "    \n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Commenting out as we have done this before and it takes a while\n",
    "    # best_params = run_optuna_experiment()\n",
    "    \n",
    "    # Free GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        r=8,  # Increased rank\n",
    "        lora_alpha=16,  # Higher scale\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"query_proj\", \"key_proj\", \"value_proj\", \"dense\"],  # Target both attention and FFN\n",
    "        init_lora_weights='pissa',\n",
    "        layers_to_transform=[i for i in range(6, 24)]\n",
    "    )\n",
    "\n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        BASE_MODEL, \n",
    "        num_labels=2,\n",
    "        hidden_dropout_prob=DROPOUT_RATE,\n",
    "        attention_probs_dropout_prob=DROPOUT_RATE,\n",
    "    )\n",
    "\n",
    "    hidden_size = model.classifier.in_features\n",
    "    # Replace default classification head with custom FFN\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(hidden_size, hidden_size),\n",
    "        nn.GELU(),\n",
    "        nn.LayerNorm(hidden_size),\n",
    "        nn.Dropout(FF_DROPOUT_RATE),\n",
    "        nn.Linear(hidden_size, 2)\n",
    "    )\n",
    "    model.config.num_labels = 2\n",
    "\n",
    "    model = get_peft_model(model, peft_config)    \n",
    "    model.to(device)\n",
    "\n",
    "    # Load data\n",
    "    train_dataset, dev_dataset, dev_df = load_data(tokenizer, MAX_SEQ_LENGTH)\n",
    "    \n",
    "    # Training parameters with focus on preventing overfitting\n",
    "    training_params = {\n",
    "        'per_device_train_batch_size': BATCH_SIZE,\n",
    "        'per_device_eval_batch_size': BATCH_SIZE,\n",
    "        'learning_rate': 5e-5,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'num_train_epochs': NUM_EPOCHS,\n",
    "        'warmup_ratio': WARMUP_RATIO,\n",
    "        'lr_scheduler_type': 'cosine',\n",
    "        'evaluation_strategy': 'steps',\n",
    "        'eval_steps': 1000,\n",
    "        'save_strategy': 'steps',\n",
    "        'save_steps': 1000,\n",
    "        'save_total_limit': 1,\n",
    "        'load_best_model_at_end': True,\n",
    "        'metric_for_best_model': 'MCC',\n",
    "        'fp16': torch.cuda.is_available(),\n",
    "        'optim': 'adamw_torch',\n",
    "        'logging_steps': 100,\n",
    "        'logging_first_step': True,\n",
    "        'group_by_length': True,\n",
    "        'seed': 42,\n",
    "    }\n",
    "    \n",
    "    # Train with default parameters\n",
    "    model_save_path = SAVE_DIR / BASE_MODEL.split('/')[-1]\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dataset,\n",
    "        dev_dataset,\n",
    "        model_save_path,\n",
    "        tokenizer,\n",
    "        **training_params\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7013730,
     "sourceId": 11351353,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7115562,
     "sourceId": 11367378,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
