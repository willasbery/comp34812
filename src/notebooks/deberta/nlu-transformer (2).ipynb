{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11351353,"sourceType":"datasetVersion","datasetId":7013730}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets huggingface_hub optuna tensorboard peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:54:40.598107Z","iopub.execute_input":"2025-04-11T10:54:40.598473Z","iopub.status.idle":"2025-04-11T10:54:45.248315Z","shell.execute_reply.started":"2025-04-11T10:54:40.598441Z","shell.execute_reply":"2025-04-11T10:54:45.247503Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.0)\nRequirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.2.1)\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.17.1)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.1)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.68.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.7)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (75.1.0)\nRequirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.17.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.1.3)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\nRequirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import logging\nimport os\nimport pandas as pd\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport optuna\nfrom optuna.samplers import TPESampler\nimport json\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    AutoConfig,\n    TrainingArguments,\n    Trainer,\n    EarlyStoppingCallback,\n    DataCollatorWithPadding\n)\nfrom peft import get_peft_model, LoraConfig, TaskType, PeftModel\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_recall_fscore_support,\n    matthews_corrcoef,\n    confusion_matrix,\n)\nfrom datasets import Dataset as HFDataset\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nfrom tqdm import tqdm\n\n# Configure logging\nlogging.basicConfig(\n    format='%(asctime)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    level=logging.INFO\n)\n\n# Disable wandb\nos.environ['WANDB_DISABLED'] = 'true'\n\n# Path configuration\nDATA_DIR = Path(\"/kaggle/working/\")\nTRAIN_FILE = \"/kaggle/input/ed-uom/train.csv\"\nDEV_FILE = \"/kaggle/input/ed-uom/dev.csv\"\nAUG_TRAIN_FILE = \"/kaggle/input/ed-uom/train_augmented.csv\"\nNEW_AUG = \"/kaggle/input/ed-uom/train_augmented_new.csv\"\nAUG_TRAIN_HIGH_REPLACEMENT_FILE = DATA_DIR / \"train_augmented_high_replacement_fraction.csv\"\nSAVE_DIR = DATA_DIR / \"results\" / \"transformer\"\nSAVE_DIR.mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:55:18.348172Z","iopub.execute_input":"2025-04-11T10:55:18.348810Z","iopub.status.idle":"2025-04-11T10:55:18.355480Z","shell.execute_reply.started":"2025-04-11T10:55:18.348780Z","shell.execute_reply":"2025-04-11T10:55:18.354479Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Training parameters\nBATCH_SIZE = 8\nNUM_EPOCHS = 3\nLEARNING_RATE = 5e-5\nWEIGHT_DECAY = 0.03\nWARMUP_RATIO = 0.11\nDROPOUT_RATE = 0.05\nFF_DROPOUT_RATE = 0.05\nMAX_SEQ_LENGTH = 512\nBASE_MODEL = 'microsoft/deberta-v3-large'\n\n\n# Optuna parameters\nN_TRIALS = 10\n\nWEIGHT_DECAYS = [0.001, 0.1]\nWARMUP_RATIOS = [0.05, 0.15]\nDROPOUT_RATES = [0.05]\nFF_DROPOUT_RATES = [0.05]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:55:44.884141Z","iopub.execute_input":"2025-04-11T10:55:44.884512Z","iopub.status.idle":"2025-04-11T10:55:44.889304Z","shell.execute_reply.started":"2025-04-11T10:55:44.884482Z","shell.execute_reply":"2025-04-11T10:55:44.888368Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def get_device() -> torch.device:\n    \"\"\"Determine the device to use for computations.\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n        return torch.device('mps')\n    return torch.device('cpu')\n\ndef preprocess_function(examples, tokenizer, max_seq_length):\n    \"\"\"Process examples for BERT/DeBERTa classification.\"\"\"\n    # Combine claim and evidence\n    claims = []\n    evidences = []\n\n    # Create inputs and targets\n    for claim, evidence in zip(examples['Claim'], examples['Evidence']):\n        formatted_claim = f\"Claim: {claim}\"\n        formatted_evidence = f\"Evidence: {evidence}\"\n        claims.append(formatted_claim)\n        evidences.append(formatted_evidence)\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(\n        claims,\n        evidences,\n        max_length=max_seq_length,\n        padding=False,\n        truncation=True,\n    )\n    \n    # Add labels (binary classification)\n    model_inputs[\"labels\"] = examples['label']\n    return model_inputs\n\ndef convert_to_hf_dataset(dataframe):\n    \"\"\"Convert pandas dataframe to HuggingFace dataset format.\"\"\"\n    return HFDataset.from_pandas(dataframe)\n\ndef load_data(tokenizer, max_seq_length):\n    \"\"\"Load and prepare the training and development datasets.\"\"\"\n    logging.info(\"Loading datasets...\")\n    \n    # Load CSV files into pandas dataframes\n    train_df = pd.read_csv(AUG_TRAIN_FILE)\n    dev_df = pd.read_csv(DEV_FILE)\n\n    # try:\n    #     train_augmented_df = pd.read_csv(AUG_TRAIN_FILE)\n    #     another_aug_df = pd.read_csv(ANOTHER_AUG_FILE)\n    #     train_df = pd.concat([train_df, train_augmented_df, another_aug_df])\n    # except Exception as e:\n    #     logging.error(f\"Error loading or concatenating augmented training data: {e}\")\n    #     raise\n    \n    print(f\"Training data shape: {train_df.shape}\")\n    print(f\"Development data shape: {dev_df.shape}\")\n    \n    # Check and report class distribution\n    train_positive = (train_df['label'] == 1).sum()\n    train_negative = (train_df['label'] == 0).sum()\n    dev_positive = (dev_df['label'] == 1).sum()\n    dev_negative = (dev_df['label'] == 0).sum()\n    \n    print(f\"Training data distribution: Positive: {train_positive} ({train_positive/len(train_df)*100:.1f}%), \"\n                 f\"Negative: {train_negative} ({train_negative/len(train_df)*100:.1f}%)\")\n    print(f\"Dev data distribution: Positive: {dev_positive} ({dev_positive/len(dev_df)*100:.1f}%), \"\n                 f\"Negative: {dev_negative} ({dev_negative/len(dev_df)*100:.1f}%)\")\n    \n    # Add a sequential index to keep track of original order (if not already present)\n    if 'original_index' not in dev_df.columns:\n        dev_df['original_index'] = list(range(len(dev_df)))\n    \n    # Convert to HuggingFace datasets\n    train_dataset = convert_to_hf_dataset(train_df)\n    dev_dataset = convert_to_hf_dataset(dev_df)\n    \n    # Apply preprocessing (tokenization)\n    train_dataset = train_dataset.map(\n        lambda examples: preprocess_function(examples, tokenizer, max_seq_length),\n        batched=True,\n        batch_size=1000,\n        remove_columns=['Claim', 'Evidence', 'label']\n    )\n    \n    # For dev dataset, keep track of original indices but remove other columns\n    columns_to_remove = [col for col in dev_df.columns if col not in ['original_index']]\n    dev_dataset = dev_dataset.map(\n        lambda examples: preprocess_function(examples, tokenizer, max_seq_length),\n        batched=True,\n        batch_size=1000,\n        remove_columns=columns_to_remove\n    )\n    \n    # Set format for pytorch\n    train_dataset.set_format(type='torch')\n    dev_dataset.set_format(type='torch')\n    \n    return train_dataset, dev_dataset, dev_df\n\ndef compute_metrics(eval_pred):\n    \"\"\"Calculate evaluation metrics for classification.\"\"\"\n    predictions, labels = eval_pred\n    \n    # For binary classification, get the predicted class (0 or 1)\n    predictions = predictions.argmax(axis=1)\n    \n    # Calculate metrics with more focus on positive class\n    accuracy = accuracy_score(labels, predictions)\n    \n    # Get more detailed metrics for both classes\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, predictions, average=None, zero_division=0\n    )\n    \n    # Weighted metrics\n    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n        labels, predictions, average='weighted', zero_division=0\n    )\n    \n    # Matthews Correlation Coefficient\n    mcc = matthews_corrcoef(labels, predictions)\n    \n    # Return both class-specific and overall metrics\n    metrics = {\n        'Accuracy': accuracy,\n        'Positive_Precision': precision[1] if len(precision) > 1 else 0,\n        'Positive_Recall': recall[1] if len(recall) > 1 else 0,\n        'Positive_F1': f1[1] if len(f1) > 1 else 0,\n        'W Macro-P': weighted_precision,\n        'W Macro-R': weighted_recall,\n        'W Macro-F1': weighted_f1,\n        'MCC': mcc\n    }\n    \n    return metrics\n\ndef plot_confusion_matrix(y_true, y_pred, save_path):\n    \"\"\"Plot and save confusion matrix.\"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    \n    classes = ['Negative', 'Positive']\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    \n    # Normalize confusion matrix\n    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    # Add text annotations\n    thresh = cm.max() / 2.\n    for i, j in np.ndindex(cm.shape):\n        plt.text(j, i, f'{cm[i, j]}\\n({cm_norm[i, j]:.2f})',\n                horizontalalignment=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\")\n    \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.savefig(save_path)\n    plt.close()\n\ndef train_model(\n    model,\n    train_dataset,\n    eval_dataset,\n    output_dir,\n    tokenizer,\n    **kwargs\n):\n    \"\"\"Train the classification model.\"\"\"\n    logging.info(\"Starting training...\")\n    \n    # Free up CUDA memory before training\n    torch.cuda.empty_cache()\n    \n    # Create data collator for dynamic padding\n    data_collator = DataCollatorWithPadding(\n        tokenizer=tokenizer,\n        padding='longest'\n    )\n    \n    training_args = TrainingArguments(\n        output_dir=str(output_dir),\n        greater_is_better=True,\n        **kwargs\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        processing_class=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=8)]\n    )\n    \n    trainer.train()\n    \n    # Get the final model from the trainer\n    model = trainer.model\n\n    # If using PEFT, merge adapters before saving\n    if isinstance(model, PeftModel):\n        logging.info(\"Merging PEFT adapters into the base model...\")\n        model = model.merge_and_unload()\n        logging.info(\"Adapters merged.\")\n\n    # Save the potentially merged model and tokenizer\n    logging.info(f\"Saving final model to {output_dir}\")\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    logging.info(f\"Model and tokenizer saved to {output_dir}\")\n\n    eval_results = trainer.evaluate(eval_dataset)\n    dev_preds = trainer.predict(eval_dataset)\n    y_true = dev_preds.label_ids\n    y_pred = dev_preds.predictions.argmax(axis=1)\n\n    # Save predictions to a CSV file with original dev data for alignment\n    # First, load the original dev CSV to maintain alignment\n    dev_df = pd.read_csv(DEV_FILE)\n    \n    # Create a dataframe with predictions\n    predictions_df = pd.DataFrame({'prediction': y_pred})\n\n    \n    \n    # Check if the evaluation dataset has original indices\n    if hasattr(eval_dataset, 'original_index') or 'original_index' in eval_dataset.features:\n        # Get original indices if present\n        try:\n            original_indices = [item['original_index'] for item in eval_dataset]\n            # Sort predictions by original index\n            predictions_df['original_index'] = original_indices\n            predictions_df = predictions_df.sort_values('original_index')\n            del predictions_df['original_index']  # Remove after sorting\n        except Exception as e:\n            logging.warning(f\"Couldn't use original indices: {e}\")\n    \n    # Ensure the predictions align with the original data\n    if len(dev_df) == len(predictions_df):\n        # Add predictions to the original dev dataframe\n        dev_df['prediction'] = predictions_df['prediction'].values\n        predictions_csv_path = os.path.join(output_dir, \"predictions_with_data.csv\")\n        dev_df.to_csv(predictions_csv_path, index=False)\n        print(f\"Predictions with original data saved to {predictions_csv_path}\")\n        \n        # Also save just the predictions for convenience\n        predictions_only_path = os.path.join(output_dir, \"predictions.csv\")\n        predictions_df.to_csv(predictions_only_path, index=False)\n    else:\n        print(f\"Prediction count ({len(predictions_df)}) doesn't match dev data count ({len(dev_df)})\")\n        # Save just the predictions\n        predictions_csv_path = os.path.join(output_dir, \"predictions.csv\")\n        predictions_df.to_csv(predictions_csv_path, index=False)\n        print(f\"Predictions saved to {predictions_csv_path}\")\n    \n    # Plot and save confusion matrix\n    cm_save_path = os.path.join(output_dir, \"confusion_matrix.png\")\n    plot_confusion_matrix(y_true, y_pred, cm_save_path)\n\n    print(eval_results)\n    \n    return eval_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:55:46.894076Z","iopub.execute_input":"2025-04-11T10:55:46.894440Z","iopub.status.idle":"2025-04-11T10:55:46.914155Z","shell.execute_reply.started":"2025-04-11T10:55:46.894400Z","shell.execute_reply":"2025-04-11T10:55:46.913374Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def objective(trial):\n    \"\"\"Optuna objective function for hyperparameter optimization.\"\"\"\n    # Get hyperparameters from trial\n    weight_decay = trial.suggest_float(\"weight_decay\", WEIGHT_DECAYS[0], WEIGHT_DECAYS[1], log=True)\n    warmup_ratio = trial.suggest_float(\"warmup_ratio\", WARMUP_RATIOS[0], WARMUP_RATIOS[1])\n    \n    device = get_device()\n    logging.info(f\"Trial {trial.number}: Using device: {device}\")\n    \n    # Free GPU memory\n    torch.cuda.empty_cache()\n\n    peft_config = LoraConfig(\n        task_type=TaskType.SEQ_CLS,\n        inference_mode=False,\n        r=8,  # Increased rank\n        lora_alpha=16,  # Higher scale\n        lora_dropout=0.1,\n        target_modules=[\"query_proj\", \"key_proj\", \"value_proj\", \"dense\"],  # Target both attention and FFN\n        init_lora_weights='pissa',\n        layers_to_transform=[i for i in range(6, 24)]\n    )\n\n    # Initialize tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        BASE_MODEL, \n        num_labels=2,\n        hidden_dropout_prob=DROPOUT_RATE,\n        attention_probs_dropout_prob=DROPOUT_RATE,\n    )\n\n    hidden_size = model.classifier.in_features\n    model.classifier = nn.Sequential(\n        nn.Linear(hidden_size, hidden_size),\n        nn.GELU(),\n        nn.LayerNorm(hidden_size),\n        nn.Dropout(FF_DROPOUT_RATE),\n        nn.Linear(hidden_size, 2)\n    )\n    model.config.num_labels = 2\n\n    model = get_peft_model(model, peft_config)\n    \n    model.to(device)\n    \n    # Load data with current max_seq_length\n    train_dataset, dev_dataset, dev_df = load_data(tokenizer, MAX_SEQ_LENGTH)\n    \n    # Training parameters\n    training_params = {\n        'per_device_train_batch_size': BATCH_SIZE,\n        'per_device_eval_batch_size': BATCH_SIZE,\n        'learning_rate': LEARNING_RATE,\n        'weight_decay': weight_decay,\n        'num_train_epochs': NUM_EPOCHS,\n        'warmup_ratio': warmup_ratio,\n        'lr_scheduler_type': 'cosine',\n        'evaluation_strategy': 'steps',\n        'eval_steps': 1000,\n        'save_strategy': 'steps',\n        'save_steps': 1000,\n        'save_total_limit': 1,\n        'load_best_model_at_end': True,\n        'metric_for_best_model': 'MCC',\n        'fp16': torch.cuda.is_available(),\n        'optim': 'adamw_torch',\n        'logging_steps': 100,\n        'logging_first_step': True,\n        'group_by_length': True,\n        'seed': 42,\n        'label_smoothing_factor': 0.1,\n    }\n    \n    # Set trial output directory\n    trial_dir = SAVE_DIR / f\"trial_{trial.number}\"\n    \n    try:\n        # Train with current hyperparameters\n        eval_results = train_model(\n            model,\n            train_dataset,\n            dev_dataset,\n            trial_dir,\n            tokenizer,\n            **training_params\n        )\n        \n        # Log the hyperparameters and results\n        params = {\n            \"weight_decay\": weight_decay,\n            \"warmup_ratio\": warmup_ratio,\n        }\n        \n        with open(trial_dir / \"hyperparameters.json\", \"w\") as f:\n            json.dump({**params, **eval_results}, f, indent=2)\n        \n        # Return Matthews Correlation Coefficient as the objective value\n        return eval_results[\"eval_MCC\"]\n    \n    except Exception as e:\n        print(f\"Trial {trial.number} failed with error: {e}\")\n        # Return very bad score for failed trials\n        return -1.0\n\ndef run_optuna_experiment():\n    \"\"\"Run Optuna hyperparameter optimization experiment.\"\"\"\n    logging.info(\"Starting hyperparameter optimization with Optuna...\")\n    \n    # Create output directory for study\n    study_dir = SAVE_DIR / \"optuna_study\"\n    study_dir.mkdir(exist_ok=True)\n    \n    # Create a pruner to terminate unpromising trials\n    pruner = optuna.pruners.MedianPruner()\n    \n    # Create a storage for the study\n    storage_name = f\"sqlite:///{study_dir}/optuna_study.db\"\n    \n    # Create TPE sampler for Bayesian optimization\n    sampler = TPESampler(seed=42)\n    \n    # Create the study\n    study = optuna.create_study(\n        direction=\"maximize\",\n        pruner=pruner,\n        storage=storage_name,\n        study_name=\"deberta_claim_evidence\",\n        load_if_exists=True,\n        sampler=sampler\n    )\n    \n    # Run optimization\n    study.optimize(objective, n_trials=N_TRIALS)\n    \n    # Get best trial\n    best_trial = study.best_trial\n    \n    # Log additional information about the Bayesian optimization\n    logging.info(f\"Using Bayesian optimization with TPE sampler\")\n    logging.info(f\"Best trial: {best_trial.number}\")\n    logging.info(f\"Best value: {best_trial.value}\")\n    logging.info(\"Best hyperparameters:\")\n    \n    for param, value in best_trial.params.items():\n        logging.info(f\"\\t{param}: {value}\")\n    \n    # Save best parameters\n    best_params = {\n        \"weight_decay\": best_trial.params[\"weight_decay\"],\n        \"warmup_ratio\": best_trial.params[\"warmup_ratio\"],\n    }\n    \n    with open(study_dir / \"best_params.json\", \"w\") as f:\n        json.dump(best_params, f, indent=2)\n    \n    # Plot optimization history\n    fig = optuna.visualization.plot_optimization_history(study)\n    fig.write_html(str(study_dir / \"optimization_history.html\"))\n    \n    # Plot parameter importance\n    fig = optuna.visualization.plot_param_importances(study)\n    fig.write_html(str(study_dir / \"param_importances.html\"))\n    \n    # Plot parameter relationships\n    fig = optuna.visualization.plot_parallel_coordinate(study)\n    fig.write_html(str(study_dir / \"parallel_coordinate.html\"))\n    \n    # Plot high-dimensional parameter relationships\n    fig = optuna.visualization.plot_contour(study)\n    fig.write_html(str(study_dir / \"contour.html\"))\n    \n    return best_params\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:55:51.430205Z","iopub.execute_input":"2025-04-11T10:55:51.430571Z","iopub.status.idle":"2025-04-11T10:55:51.443175Z","shell.execute_reply.started":"2025-04-11T10:55:51.430539Z","shell.execute_reply":"2025-04-11T10:55:51.442241Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def main():\n    \"\"\"Main execution function.\"\"\"\n    device = get_device()\n    logging.info(f\"Using device: {device}\")\n\n    # best_params = run_optuna_experiment()\n    \n    # Free GPU memory\n    torch.cuda.empty_cache()\n\n    peft_config = LoraConfig(\n        task_type=TaskType.SEQ_CLS,\n        inference_mode=False,\n        r=8,  # Increased rank\n        lora_alpha=16,  # Higher scale\n        lora_dropout=0.1,\n        target_modules=[\"query_proj\", \"key_proj\", \"value_proj\", \"dense\"],  # Target both attention and FFN\n        init_lora_weights='pissa',\n        layers_to_transform=[i for i in range(6, 24)]\n    )\n\n    # Initialize tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        BASE_MODEL, \n        num_labels=2,\n        hidden_dropout_prob=DROPOUT_RATE,\n        attention_probs_dropout_prob=DROPOUT_RATE,\n    )\n\n    hidden_size = model.classifier.in_features\n    model.classifier = nn.Sequential(\n        nn.Linear(hidden_size, hidden_size),\n        nn.GELU(),\n        nn.LayerNorm(hidden_size),\n        nn.Dropout(FF_DROPOUT_RATE),\n        nn.Linear(hidden_size, 2)\n    )\n    model.config.num_labels = 2\n\n    # model = get_peft_model(model, peft_config)\n    \n    model.to(device)\n\n    # Load data\n    train_dataset, dev_dataset, dev_df = load_data(tokenizer, MAX_SEQ_LENGTH)\n    \n    # Training parameters with focus on preventing overfitting\n    training_params = {\n        'per_device_train_batch_size': BATCH_SIZE,\n        'per_device_eval_batch_size': BATCH_SIZE,\n        'learning_rate': 5e-5,\n        'weight_decay': WEIGHT_DECAY,\n        'num_train_epochs': 2,\n        'warmup_ratio': WARMUP_RATIO,\n        'lr_scheduler_type': 'cosine',\n        'evaluation_strategy': 'steps',\n        'eval_steps': 1000,\n        'save_strategy': 'steps',\n        'save_steps': 1000,\n        'save_total_limit': 1,\n        'load_best_model_at_end': True,\n        'metric_for_best_model': 'MCC',\n        'fp16': torch.cuda.is_available(),\n        'optim': 'adamw_torch',\n        'logging_steps': 100,\n        'logging_first_step': True,\n        'group_by_length': True,\n        'seed': 42,\n        'label_smoothing_factor': 0.1,\n    }\n    \n    # Train with default parameters\n    model_save_path = SAVE_DIR / BASE_MODEL.split('/')[-1]\n    train_model(\n        model,\n        train_dataset,\n        dev_dataset,\n        model_save_path,\n        tokenizer,\n        **training_params\n    )\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:55:54.545928Z","iopub.execute_input":"2025-04-11T10:55:54.546264Z","iopub.status.idle":"2025-04-11T11:45:53.643259Z","shell.execute_reply.started":"2025-04-11T10:55:54.546233Z","shell.execute_reply":"2025-04-11T11:45:53.642403Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1537290e2cd345e38041e1308d350360"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a696fbcbd4a844fbae3606b381b21d90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"556e84faf51b4cc3b1a95592da13f7dc"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce1d8b24ae0e4c29af7d0298d1e0f315"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Training data shape: (28927, 3)\nDevelopment data shape: (5926, 3)\nTraining data distribution: Positive: 11708 (40.5%), Negative: 17219 (59.5%)\nDev data distribution: Positive: 1640 (27.7%), Negative: 4286 (72.3%)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/28927 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eac4ffb115a40bfa76c384e58ef2917"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5926 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc5de216fbc64093b0079529be86efe9"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7232' max='7232' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7232/7232 47:14, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Positive Precision</th>\n      <th>Positive Recall</th>\n      <th>Positive F1</th>\n      <th>W macro-p</th>\n      <th>W macro-r</th>\n      <th>W macro-f1</th>\n      <th>Mcc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>0.585300</td>\n      <td>0.497093</td>\n      <td>0.802565</td>\n      <td>0.745303</td>\n      <td>0.435366</td>\n      <td>0.549654</td>\n      <td>0.794704</td>\n      <td>0.802565</td>\n      <td>0.783926</td>\n      <td>0.459904</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.684400</td>\n      <td>0.617761</td>\n      <td>0.733716</td>\n      <td>0.620155</td>\n      <td>0.097561</td>\n      <td>0.168599</td>\n      <td>0.706027</td>\n      <td>0.733716</td>\n      <td>0.655256</td>\n      <td>0.163764</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.679500</td>\n      <td>0.620229</td>\n      <td>0.733547</td>\n      <td>0.615970</td>\n      <td>0.098780</td>\n      <td>0.170257</td>\n      <td>0.704957</td>\n      <td>0.733547</td>\n      <td>0.655584</td>\n      <td>0.163401</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.688600</td>\n      <td>0.618846</td>\n      <td>0.723253</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.523096</td>\n      <td>0.723253</td>\n      <td>0.607102</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.679400</td>\n      <td>0.641289</td>\n      <td>0.723253</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.523096</td>\n      <td>0.723253</td>\n      <td>0.607102</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.682000</td>\n      <td>0.638811</td>\n      <td>0.723253</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.523096</td>\n      <td>0.723253</td>\n      <td>0.607102</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.682300</td>\n      <td>0.631794</td>\n      <td>0.723253</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.523096</td>\n      <td>0.723253</td>\n      <td>0.607102</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"name":"stdout","text":"Predictions with original data saved to /kaggle/working/results/transformer/deberta-v3-large/predictions_with_data.csv\n{'eval_loss': 0.4970925450325012, 'eval_Accuracy': 0.8025649679379008, 'eval_Positive_Precision': 0.7453027139874739, 'eval_Positive_Recall': 0.4353658536585366, 'eval_Positive_F1': 0.5496535796766744, 'eval_W Macro-P': 0.794704086860165, 'eval_W Macro-R': 0.8025649679379008, 'eval_W Macro-F1': 0.7839259397561248, 'eval_MCC': 0.45990408263906535, 'eval_runtime': 61.5547, 'eval_samples_per_second': 96.272, 'eval_steps_per_second': 12.038, 'epoch': 2.0}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from safetensors.torch import load_file as load_safetensors_file\n\n\ndef prepare_input(claim: str, evidence: str, tokenizer, max_length: int, device: torch.device):\n    \"\"\"Formats and tokenizes a single claim-evidence pair.\"\"\"\n    # --- Reuses the formatting logic from preprocess_function ---\n    formatted_claim = f\"Claim: {claim}\"\n    formatted_evidence = f\"Evidence: {evidence}\"\n\n    # --- Reuses the tokenization logic ---\n    inputs = tokenizer(\n        formatted_claim,\n        formatted_evidence,\n        max_length=max_length,\n        padding=\"max_length\", # Or another appropriate padding strategy\n        truncation=True,\n        return_tensors=\"pt\"  # Return PyTorch tensors\n    )\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    return inputs\n\n# --- Main Prediction Logic ---\ndef run_predictions(model_path: str, input_csv_path: str, output_csv_path: str):\n    \"\"\"Loads model MANUALLY, reads CSV, makes predictions, and saves results.\"\"\"\n\n    # 1. Load Tokenizer and Config (as before)\n    print(f\"Loading tokenizer from: {model_path}\")\n    device = get_device()\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    config = AutoConfig.from_pretrained(model_path) # Load config separately\n\n    # 2. *** Manually Construct the Model Architecture *** (same as before)\n    print(\"Constructing model architecture...\")\n    model = AutoModelForSequenceClassification.from_config(config)\n    hidden_size = model.classifier.in_features\n    model.classifier = nn.Sequential(\n        nn.Linear(hidden_size, hidden_size),\n        nn.GELU(),\n        nn.LayerNorm(hidden_size),\n        nn.Dropout(FF_DROPOUT_RATE),\n        nn.Linear(hidden_size, config.num_labels)\n    )\n    print(\"Custom classifier head applied.\")\n\n    # 3. *** Load the Saved Weights (State Dictionary) - MODIFIED ***\n    safetensors_path = os.path.join(model_path, \"model.safetensors\")\n    pytorch_bin_path = os.path.join(model_path, \"pytorch_model.bin\")\n\n    state_dict = None\n    weights_loaded_from = None\n\n    if os.path.exists(safetensors_path):\n        print(f\"Loading weights from SafeTensors file: {safetensors_path}...\")\n        try:\n            state_dict = load_safetensors_file(safetensors_path, device='cpu') # Load using safetensors library\n            weights_loaded_from = safetensors_path\n        except Exception as e:\n            print(f\"Error loading safetensors file: {e}\")\n            # Optionally, try pytorch_model.bin if safetensors fails\n            if os.path.exists(pytorch_bin_path):\n                 print(f\"Attempting to load pytorch_model.bin instead...\")\n            else:\n                 return # Stop if neither format seems to work\n\n    if state_dict is None and os.path.exists(pytorch_bin_path):\n        print(f\"Loading weights from PyTorch bin file: {pytorch_bin_path}...\")\n        try:\n            # Use weights_only=True for security as recommended by the warning\n            state_dict = torch.load(pytorch_bin_path, map_location='cpu', weights_only=True)\n            weights_loaded_from = pytorch_bin_path\n        except Exception as e:\n            print(f\"Error loading pytorch_model.bin file: {e}. This might indicate corruption.\")\n            print(\"Please ensure the model saving process completed successfully.\")\n            return # Stop if loading fails\n\n    if state_dict is None:\n        print(f\"Error: No weight file (model.safetensors or pytorch_model.bin) found or loaded successfully in {model_path}\")\n        return\n\n    print(f\"Weights loaded successfully from {weights_loaded_from}\")\n\n    # Load the state dict into the manually constructed model\n    try:\n        model.load_state_dict(state_dict)\n    except RuntimeError as e:\n        print(f\"Error loading state dict into model: {e}\")\n        print(\"This often means the manually constructed architecture doesn't match the keys in the weights file.\")\n        print(\"Ensure the custom classifier definition EXACTLY matches the one used during training.\")\n        return\n\n\n    model.to(device) # Move the complete model to the target device\n    model.eval() # Set the model to evaluation mode\n    print(f\"Model constructed and weights loaded. Using device: {device}\")\n\n    # 4. Read Input CSV (same as before)\n    print(f\"Reading input CSV: {input_csv_path}\")\n    try:\n        input_df = pd.read_csv(input_csv_path)\n        if 'Claim' not in input_df.columns or 'Evidence' not in input_df.columns:\n            raise ValueError(\"Input CSV must contain 'Claim' and 'Evidence' columns.\")\n        print(f\"Loaded {len(input_df)} rows from {input_csv_path}\")\n    except FileNotFoundError:\n        print(f\"Error: Input CSV file not found at {input_csv_path}\")\n        return\n    except Exception as e:\n        print(f\"Error reading CSV file: {e}\")\n        return\n\n    # 5. Make Predictions (same as before)\n    predictions = []\n    print(\"Making predictions...\")\n    for index, row in tqdm(input_df.iterrows(), total=input_df.shape[0], desc=\"Predicting\"):\n        claim = str(row['Claim'])\n        evidence = str(row['Evidence'])\n        if not claim or not evidence:\n             print(f\"Warning: Skipping row {index} due to empty Claim or Evidence.\")\n             predictions.append(None)\n             continue\n        inputs = prepare_input(claim, evidence, tokenizer, MAX_SEQ_LENGTH, device)\n        with torch.no_grad():\n            outputs = model(**inputs)\n            logits = outputs.logits\n            predicted_class_id = torch.argmax(logits, dim=-1).item()\n            predictions.append(predicted_class_id)\n\n    # 6. Save Predictions (same as before)\n    output_df = pd.DataFrame({'prediction': predictions})\n    print(f\"Saving predictions to: {output_csv_path}\")\n    try:\n        output_df.to_csv(output_csv_path, index=False)\n        print(\"Predictions saved successfully.\")\n    except Exception as e:\n        print(f\"Error saving predictions: {e}\")\n\ntorch.cuda.empty_cache()\nmodel_save_path = \"/kaggle/working/results/transformer/deberta-v3-large\"\n\nrun_predictions(model_save_path, DEV_FILE, 'predictions.csv')\n\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T12:04:23.647662Z","iopub.execute_input":"2025-04-11T12:04:23.647994Z"}},"outputs":[{"name":"stdout","text":"Loading tokenizer from: /kaggle/working/results/transformer/deberta-v3-large\nConstructing model architecture...\nCustom classifier head applied.\nLoading weights from SafeTensors file: /kaggle/working/results/transformer/deberta-v3-large/model.safetensors...\nWeights loaded successfully from /kaggle/working/results/transformer/deberta-v3-large/model.safetensors\nModel constructed and weights loaded. Using device: cuda\nReading input CSV: /kaggle/input/ed-uom/dev.csv\nLoaded 5926 rows from /kaggle/input/ed-uom/dev.csv\nMaking predictions...\n","output_type":"stream"},{"name":"stderr","text":"Predicting:  46%|████▌     | 2736/5926 [04:43<05:31,  9.63it/s]","output_type":"stream"}],"execution_count":null}]}