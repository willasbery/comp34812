{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.11/site-packages (from nltk) (1.3.1)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import cloudpickle\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from typing import Tuple, Set, Optional\n",
    "\n",
    "# PLEAE CHANGE TO PROJECT ROOT OTHERWISE THE PICKLE WILL NOT WORK\n",
    "project_root = \"<ABSOLUTE PATH TO PROJECT ROOT>/comp34812\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseConfig:\n",
    "    DATA_DIR = Path(\"Path to data directory\") # comp34812/data\n",
    "    TRAIN_FILE = Path(\"Path to train file\") # train.csv\n",
    "    DEV_FILE = Path(\"Path to dev file\") # dev.csv\n",
    "    TEST_FILE = Path(\"Path to test file\") # test.csv\n",
    "    AUG_TRAIN_FILE = Path(\"Path to augmented train file\") # train_augmented.csv\n",
    "    SAVE_DIR = Path(\"Path to save directory\") # comp34812/data/results\n",
    "    CACHE_DIR = Path(\"Path to cache directory\") # comp34812/cache\n",
    "\n",
    "    # Augmentation config\n",
    "    AUGMENTATION_CONFIG = {\n",
    "        \"0\": {\n",
    "            \"replace\": 0.0,\n",
    "            \"add\": 0.1, # 10%\n",
    "            \"translate\":{\n",
    "                \"percentage\": 1.0,\n",
    "                \"split\": {\n",
    "                    \"Claim\": 0.15,\n",
    "                    \"Evidence\": 0.7,\n",
    "                    \"Both\": 0.15\n",
    "                },\n",
    "                \"src\": \"en\",\n",
    "                \"intermediates\": {\n",
    "                    \"fr\": 0.5,\n",
    "                    \"de\": 0.4,\n",
    "                    \"ja\": 0.1\n",
    "                }\n",
    "            },\n",
    "            \"synonym_replacement\": {\n",
    "                \"percentage\": 0.7,\n",
    "                \"replacement_fraction\": 0.3,\n",
    "                \"min_similarity\": 0.85,\n",
    "                \"min_word_length\": 4,\n",
    "                \"word_frequency_threshold\": 3,\n",
    "                \"synonym_selection_strategy\": \"random\",\n",
    "                \"enable_random_synonym_insertion\": True,\n",
    "                \"synonym_insertion_probability\": 0.03,\n",
    "                \"enable_random_word_insertion\": True,\n",
    "                \"word_insertion_probability\": 0.01,\n",
    "                \"enable_random_deletion\": True,\n",
    "                \"deletion_probability\": 0.01,\n",
    "            },\n",
    "            \"x_or_y\": {\n",
    "                \"percentage\": 0.08,\n",
    "                \"max_choices\": 4,\n",
    "                \"num_words_to_augment\": {\n",
    "                    \"Claim\": 1,\n",
    "                    \"Evidence\": 2\n",
    "                },\n",
    "                \"split\": {\n",
    "                    \"Claim\": 0.90,\n",
    "                    \"Evidence\": 0.05,\n",
    "                    \"Both\": 0.05\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"1\": {\n",
    "            \"replace\": 0.0,\n",
    "            \"add\": 1.0,\n",
    "            \"translate\":{\n",
    "                \"percentage\": 0.8,\n",
    "                \"split\": {\n",
    "                    \"Claim\": 0.15,\n",
    "                    \"Evidence\": 0.7,\n",
    "                    \"Both\": 0.15\n",
    "                },\n",
    "                \"src\": \"en\",\n",
    "                \"intermediates\": {\n",
    "                    \"fr\": 0.5,\n",
    "                    \"de\": 0.4,\n",
    "                    \"ja\": 0.1\n",
    "                }\n",
    "            },\n",
    "            \"synonym_replacement\": {\n",
    "                \"percentage\": 0.7,\n",
    "                \"replacement_fraction\": 0.3,\n",
    "                \"min_similarity\": 0.85,\n",
    "                \"min_word_length\": 4,\n",
    "                \"word_frequency_threshold\": 3,\n",
    "                \"synonym_selection_strategy\": \"random\",\n",
    "                \"enable_random_synonym_insertion\": True,\n",
    "                \"synonym_insertion_probability\": 0.03,\n",
    "                \"enable_random_word_insertion\": True,\n",
    "                \"word_insertion_probability\": 0.01,\n",
    "                \"enable_random_deletion\": True,\n",
    "                \"deletion_probability\": 0.01,\n",
    "            },\n",
    "            \"x_or_y\": {\n",
    "                \"percentage\": 0.02,\n",
    "                \"max_choices\": 4,\n",
    "                \"num_words_to_augment\": {\n",
    "                    \"Claim\": 1,\n",
    "                    \"Evidence\": 2\n",
    "                },\n",
    "                \"split\": {\n",
    "                    \"Claim\": 0.90,\n",
    "                    \"Evidence\": 0.05,\n",
    "                    \"Both\": 0.05\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "def get_config() -> BaseConfig:\n",
    "    return BaseConfig()\n",
    "\n",
    "\n",
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "params = {\n",
    "    \"vocab_size\": 12000,\n",
    "    \"n_gram_range\": (1, 2),\n",
    "    \"embedding_dim\": 300,\n",
    "    \"pca_components\": 540,\n",
    "    \"C\": 1.96,\n",
    "    \"tfidf_weighting\": True,\n",
    "    \"min_df\": 1,\n",
    "    \"max_df\": 0.95,\n",
    "    \"kernel\": 'rbf',\n",
    "    \"gamma\": 'scale'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_svm_data(data: pd.DataFrame, \n",
    "                    remove_stopwords: bool = True, \n",
    "                    lemmatize: bool = True, \n",
    "                    min_freq: int = 2, \n",
    "                    vocab_size: Optional[int] = None) -> Tuple[pd.DataFrame, np.ndarray, Set[str]]:\n",
    "    \"\"\"\n",
    "    Prepare text data for SVM training by cleaning, normalizing and vocabulary management.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame containing 'Claim' and 'Evidence' columns\n",
    "        remove_stopwords: Whether to remove common stopwords\n",
    "        lemmatize: Whether to apply lemmatization\n",
    "        min_freq: Minimum frequency for words to be included in vocabulary\n",
    "        vocab_size: Maximum vocabulary size (most frequent words kept)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - Processed DataFrame with added 'text' column\n",
    "            - NumPy array of labels\n",
    "            - Set of vocabulary words\n",
    "    \"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize text by lowercasing, removing punctuation,\n",
    "        and optionally removing stopwords and lemmatizing.\n",
    "        \"\"\"\n",
    "        text = text.lower().translate(translator)\n",
    "        # Normalize whitespace\n",
    "        text = \" \".join(text.split())\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            try:\n",
    "                # Keep important discourse markers and modal verbs\n",
    "                keep_words = {\n",
    "                    'because', 'since', 'therefore', 'hence', 'thus', 'although',\n",
    "                    'however', 'but', 'not', 'should', 'must', 'might', 'may',\n",
    "                    'could', 'would', 'against', 'between', 'before', 'after'\n",
    "                }\n",
    "                custom_stopwords = set(stopwords.words(\"english\")) - keep_words\n",
    "                \n",
    "                text = \" \".join([word for word in text.split() \n",
    "                               if word not in custom_stopwords])\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "        if lemmatize:\n",
    "            try:\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                words = text.split()\n",
    "                text = \" \".join([lemmatizer.lemmatize(word) for word in words])\n",
    "            except Exception:\n",
    "                pass\n",
    "        return text\n",
    "\n",
    "    # Build vocabulary from training data\n",
    "    train_samples = pd.concat([data['Claim'], data['Evidence']]).apply(clean_text)\n",
    "    all_words = [word for text in train_samples for word in text.split()]\n",
    "    word_counts = Counter(all_words)\n",
    "\n",
    "    # Filter words by minimum frequency and sort by frequency\n",
    "    filtered_words = [(word, count) for word, count in word_counts.items() if count >= min_freq]\n",
    "    sorted_words = sorted(filtered_words, key=lambda x: (-x[1], x[0]))\n",
    "    \n",
    "    # Apply vocabulary size limit if specified\n",
    "    if vocab_size is not None:\n",
    "        sorted_words = sorted_words[:vocab_size]\n",
    "    \n",
    "    vocab = {word for word, _ in sorted_words}\n",
    "\n",
    "    def replace_rare_words(text: str) -> str:\n",
    "        \"\"\"Replace words not in vocabulary with <UNK> token.\"\"\"\n",
    "        return ' '.join([word if word in vocab else '<UNK>' for word in text.split()])\n",
    "\n",
    "    # Process the data with UNK replacement\n",
    "    data['text'] = (\"Claim: \" + data['Claim'].apply(clean_text).apply(replace_rare_words) + \n",
    "                    \" [SEP] \" + \"Evidence: \" + data['Evidence'].apply(clean_text).apply(replace_rare_words))\n",
    "\n",
    "    # Extract labels\n",
    "    if 'label' in data.columns:\n",
    "        labels = data['label'].values\n",
    "    else:\n",
    "        labels = [None] * len(data)\n",
    "\n",
    "    return data, labels, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_saved_model(\n",
    "    pipeline_path: Path, \n",
    "    input_csv_path: Path, \n",
    "    output_csv_path: Path\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Loads a saved SVM pipeline, makes predictions on data from an input CSV, \n",
    "    and saves the predictions to an output CSV.\n",
    "\n",
    "    Args:\n",
    "        pipeline_path: Path to the saved .pkl pipeline file.\n",
    "        input_csv_path: Path to the input CSV file (must contain 'Evidence' column).\n",
    "        output_csv_path: Path where the predictions CSV will be saved.\n",
    "    \"\"\"\n",
    "    logger.info(\"\\n\" + \"=\"*70)\n",
    "    logger.info(f\"MAKING PREDICTIONS FROM {input_csv_path}\")\n",
    "    logger.info(\"=\"*70)\n",
    "\n",
    "    # --- Input Validation ---\n",
    "    if not pipeline_path.exists():\n",
    "        logger.error(f\"Pipeline file not found at {pipeline_path}. Cannot make predictions.\")\n",
    "        return\n",
    "    if not input_csv_path.exists():\n",
    "        logger.error(f\"Input CSV file not found at {input_csv_path}. Cannot make predictions.\")\n",
    "        return\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    output_csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # --- Load Pipeline --- \n",
    "        with open(pipeline_path, \"rb\") as f:\n",
    "            loaded_pipeline = cloudpickle.load(f)\n",
    "        logger.info(f\"Pipeline loaded successfully from {pipeline_path}\")\n",
    "\n",
    "        # --- Load and Prepare Input Data ---\n",
    "        input_df = pd.read_csv(input_csv_path)\n",
    "        logger.info(f\"Loaded {len(input_df)} rows from {input_csv_path}\")\n",
    "\n",
    "        if 'Evidence' not in input_df.columns or 'Claim' not in input_df.columns:\n",
    "            logger.error(f\"Input CSV {input_csv_path} must contain 'Evidence' and 'Claim' columns.\")\n",
    "            return\n",
    "            \n",
    "\n",
    "        # Determine training parameters needed for preprocessing\n",
    "        training_vocab_size = params.get('vocab_size', 12000) \n",
    "        logger.info(f\"Using parameters for preprocessing: vocab_size={training_vocab_size}\")\n",
    "\n",
    "\n",
    "        # Apply the *exact same* preprocessing as used during training\n",
    "        processed_data_df, _, _ = prepare_svm_data(\n",
    "            input_df, \n",
    "            remove_stopwords=True,\n",
    "            lemmatize=True,        \n",
    "            min_freq=2, \n",
    "            vocab_size=training_vocab_size\n",
    "        )\n",
    "        processed_texts = processed_data_df['text'].tolist()\n",
    "        logger.info(f\"Preprocessing complete for {len(processed_texts)} texts.\")\n",
    "\n",
    "        # --- Make Predictions --- \n",
    "        predictions = loaded_pipeline.predict(processed_texts)\n",
    "        logger.info(f\"Generated {len(predictions)} predictions.\")\n",
    "\n",
    "        # --- Save Predictions --- \n",
    "        predictions_df = pd.DataFrame({'prediction': predictions})\n",
    "        predictions_df.to_csv(output_csv_path, index=False)\n",
    "        logger.info(f\"Predictions saved successfully to {output_csv_path}\")\n",
    "\n",
    "    except ModuleNotFoundError as e:\n",
    "         logger.error(f\"Error loading pickle: A module required by the pickled object was not found: {e}\")\n",
    "         logger.error(\"Ensure all necessary libraries and custom classes (GloveVectorizer, etc.) are importable.\")\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Error: A required file was not found: {e}\")\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"Error: Missing expected column in input data: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during prediction: {e}\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_pickle_path = Path(\"/Users/harvey/School/Year 3/NLU/comp34812/data/results/svm/svm_pipeline.pkl\")\n",
    "\n",
    "try:\n",
    "    prediction_input_file = config.TEST_FILE\n",
    "    prediction_output_file = config.DATA_DIR / \"svm_predictions.csv\"\n",
    "    \n",
    "    # Ensure the predictions directory exists\n",
    "    prediction_output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    predict_with_saved_model(\n",
    "        pipeline_path=pipeline_pickle_path,\n",
    "        input_csv_path=prediction_input_file, \n",
    "        output_csv_path=prediction_output_file\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error predicting with saved model: {e}\", exc_info=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
