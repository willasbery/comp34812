{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 26 - Traditional Machine Learning Approach\n",
    "## SVM\n",
    "\n",
    "#### Harvey Dennis and William Asbery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the instructions in this notebook to successfully carry out predictions on the test file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "__PLEASE RUN THE CELL BELOW__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\python311\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: nltk in c:\\python311\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: optuna in c:\\python311\\lib\\site-packages (4.2.1)\n",
      "Requirement already satisfied: plotly in c:\\python311\\lib\\site-packages (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\python311\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\tools\\manim\\lib\\site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python311\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python311\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\tools\\manim\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python311\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\tools\\manim\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\python311\\lib\\site-packages (from optuna) (1.14.1)\n",
      "Requirement already satisfied: colorlog in c:\\python311\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python311\\lib\\site-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\python311\\lib\\site-packages (from optuna) (2.0.38)\n",
      "Requirement already satisfied: PyYAML in c:\\python311\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\python311\\lib\\site-packages (from plotly) (1.34.1)\n",
      "Requirement already satisfied: Mako in c:\\python311\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\python311\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python311\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\tools\\manim\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\python311\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~animgl (C:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~animgl (C:\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn nltk optuna plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willi\\Desktop\\Uni\\3rd_year\\NLU\\comp38412-not-broken\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpruners\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MedianPruner\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msamplers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TPESampler\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import psutil\n",
    "import string\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from plotly.io import show\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from pathlib import Path\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from contextlib import contextmanager\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple, Set, Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config\n",
    "\n",
    "This config specifies the configuration for the paths of the files being used to train, validate and test the model.\n",
    "Also, the config contains the characteristics of the augmented file generated from the train data.\n",
    "\n",
    "Please add the relative or absolute paths to the train, dev, test and augmented train files. We provide you with an augmented train file as the augmentation pipeline takes about 2 hours to run.\n",
    "\n",
    "Once all the paths are added, \n",
    "\n",
    "__PLEASE RUN THE CELL BELOW.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseConfig:\n",
    "    DATA_DIR = Path(__file__) / \"data\"\n",
    "    TRAIN_FILE = Path(\"train.csv\")\n",
    "    DEV_FILE = Path(\"dev.csv\")\n",
    "    TEST_FILE = Path(\"test.csv\")\n",
    "    AUG_TRAIN_FILE = Path(\"train_augmented.csv\")\n",
    "    SAVE_DIR = Path(__file__) / \"data\" / \"results\"\n",
    "    CACHE_DIR = Path(__file__) / \"cache\"\n",
    "\n",
    "    # Augmentation config\n",
    "    AUGMENTATION_CONFIG = {\n",
    "        \"0\": {\n",
    "            \"replace\": 0.0,\n",
    "            \"add\": 0.1, # 10%\n",
    "            \"translate\":{\n",
    "                \"percentage\": 1.0,\n",
    "                \"split\": {\n",
    "                    \"Claim\": 0.15,\n",
    "                    \"Evidence\": 0.7,\n",
    "                    \"Both\": 0.15\n",
    "                },\n",
    "                \"src\": \"en\",\n",
    "                \"intermediates\": {\n",
    "                    \"fr\": 0.5,\n",
    "                    \"de\": 0.4,\n",
    "                    \"ja\": 0.1\n",
    "                }\n",
    "            },\n",
    "            \"synonym_replacement\": {\n",
    "                \"percentage\": 0.7,\n",
    "                \"replacement_fraction\": 0.3,\n",
    "                \"min_similarity\": 0.85,\n",
    "                \"min_word_length\": 4,\n",
    "                \"word_frequency_threshold\": 3,\n",
    "                \"synonym_selection_strategy\": \"random\",\n",
    "                \"enable_random_synonym_insertion\": True,\n",
    "                \"synonym_insertion_probability\": 0.03,\n",
    "                \"enable_random_word_insertion\": True,\n",
    "                \"word_insertion_probability\": 0.01,\n",
    "                \"enable_random_deletion\": True,\n",
    "                \"deletion_probability\": 0.01,\n",
    "            },\n",
    "            \"x_or_y\": {\n",
    "                \"percentage\": 0.08,\n",
    "                \"max_choices\": 4,\n",
    "                \"num_words_to_augment\": {\n",
    "                    \"Claim\": 1,\n",
    "                    \"Evidence\": 2\n",
    "                },\n",
    "                \"split\": {\n",
    "                    \"Claim\": 0.90,\n",
    "                    \"Evidence\": 0.05,\n",
    "                    \"Both\": 0.05\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"1\": {\n",
    "            \"replace\": 0.0,\n",
    "            \"add\": 1.0,\n",
    "            \"translate\":{\n",
    "                \"percentage\": 0.8,\n",
    "                \"split\": {\n",
    "                    \"Claim\": 0.15,\n",
    "                    \"Evidence\": 0.7,\n",
    "                    \"Both\": 0.15\n",
    "                },\n",
    "                \"src\": \"en\",\n",
    "                \"intermediates\": {\n",
    "                    \"fr\": 0.5,\n",
    "                    \"de\": 0.4,\n",
    "                    \"ja\": 0.1\n",
    "                }\n",
    "            },\n",
    "            \"synonym_replacement\": {\n",
    "                \"percentage\": 0.7,\n",
    "                \"replacement_fraction\": 0.3,\n",
    "                \"min_similarity\": 0.85,\n",
    "                \"min_word_length\": 4,\n",
    "                \"word_frequency_threshold\": 3,\n",
    "                \"synonym_selection_strategy\": \"random\",\n",
    "                \"enable_random_synonym_insertion\": True,\n",
    "                \"synonym_insertion_probability\": 0.03,\n",
    "                \"enable_random_word_insertion\": True,\n",
    "                \"word_insertion_probability\": 0.01,\n",
    "                \"enable_random_deletion\": True,\n",
    "                \"deletion_probability\": 0.01,\n",
    "            },\n",
    "            \"x_or_y\": {\n",
    "                \"percentage\": 0.02,\n",
    "                \"max_choices\": 4,\n",
    "                \"num_words_to_augment\": {\n",
    "                    \"Claim\": 1,\n",
    "                    \"Evidence\": 2\n",
    "                },\n",
    "                \"split\": {\n",
    "                    \"Claim\": 0.90,\n",
    "                    \"Evidence\": 0.05,\n",
    "                    \"Both\": 0.05\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "def get_config() -> BaseConfig:\n",
    "    return BaseConfig()\n",
    "\n",
    "\n",
    "config = get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Params from tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "params = {\n",
    "    \"vocab_size\": 12000,\n",
    "    \"n_gram_range\": (1, 2),\n",
    "    \"embedding_dim\": 300,\n",
    "    \"pca_components\": 540,\n",
    "    \"C\": 1.96,\n",
    "    \"tfidf_weighting\": True,\n",
    "    \"min_df\": 1,\n",
    "    \"max_df\": 0.95,\n",
    "    \"kernel\": 'rbf',\n",
    "    \"gamma\": 'scale'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_svm_data(data: pd.DataFrame, \n",
    "                    remove_stopwords: bool = True, \n",
    "                    lemmatize: bool = True, \n",
    "                    min_freq: int = 2, \n",
    "                    vocab_size: Optional[int] = None) -> Tuple[pd.DataFrame, np.ndarray, Set[str]]:\n",
    "    \"\"\"\n",
    "    Prepare text data for SVM training by cleaning, normalizing and vocabulary management.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame containing 'Claim' and 'Evidence' columns\n",
    "        remove_stopwords: Whether to remove common stopwords\n",
    "        lemmatize: Whether to apply lemmatization\n",
    "        min_freq: Minimum frequency for words to be included in vocabulary\n",
    "        vocab_size: Maximum vocabulary size (most frequent words kept)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - Processed DataFrame with added 'text' column\n",
    "            - NumPy array of labels\n",
    "            - Set of vocabulary words\n",
    "    \"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize text by lowercasing, removing punctuation,\n",
    "        and optionally removing stopwords and lemmatizing.\n",
    "        \"\"\"\n",
    "        text = text.lower().translate(translator)\n",
    "        # Normalize whitespace\n",
    "        text = \" \".join(text.split())\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            try:\n",
    "                # Keep important discourse markers and modal verbs\n",
    "                keep_words = {\n",
    "                    'because', 'since', 'therefore', 'hence', 'thus', 'although',\n",
    "                    'however', 'but', 'not', 'should', 'must', 'might', 'may',\n",
    "                    'could', 'would', 'against', 'between', 'before', 'after'\n",
    "                }\n",
    "                custom_stopwords = set(stopwords.words(\"english\")) - keep_words\n",
    "                \n",
    "                text = \" \".join([word for word in text.split() \n",
    "                               if word not in custom_stopwords])\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "        if lemmatize:\n",
    "            try:\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                words = text.split()\n",
    "                text = \" \".join([lemmatizer.lemmatize(word) for word in words])\n",
    "            except Exception:\n",
    "                pass\n",
    "        return text\n",
    "\n",
    "    # Build vocabulary from training data\n",
    "    train_samples = pd.concat([data['Claim'], data['Evidence']]).apply(clean_text)\n",
    "    all_words = [word for text in train_samples for word in text.split()]\n",
    "    word_counts = Counter(all_words)\n",
    "\n",
    "    # Filter words by minimum frequency and sort by frequency\n",
    "    filtered_words = [(word, count) for word, count in word_counts.items() if count >= min_freq]\n",
    "    sorted_words = sorted(filtered_words, key=lambda x: (-x[1], x[0]))\n",
    "    \n",
    "    # Apply vocabulary size limit if specified\n",
    "    if vocab_size is not None:\n",
    "        sorted_words = sorted_words[:vocab_size]\n",
    "    \n",
    "    vocab = {word for word, _ in sorted_words}\n",
    "\n",
    "    def replace_rare_words(text: str) -> str:\n",
    "        \"\"\"Replace words not in vocabulary with <UNK> token.\"\"\"\n",
    "        return ' '.join([word if word in vocab else '<UNK>' for word in text.split()])\n",
    "\n",
    "    # Process the data with UNK replacement\n",
    "    data['text'] = (\"Claim: \" + data['Claim'].apply(clean_text).apply(replace_rare_words) + \n",
    "                    \" [SEP] \" + \"Evidence: \" + data['Evidence'].apply(clean_text).apply(replace_rare_words))\n",
    "\n",
    "    # Extract labels\n",
    "    labels = data['label'].values\n",
    "\n",
    "    return data, labels, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_saved_model(\n",
    "    pipeline_path: Path, \n",
    "    input_csv_path: Path, \n",
    "    output_csv_path: Path\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Loads a saved SVM pipeline, makes predictions on data from an input CSV, \n",
    "    and saves the predictions to an output CSV.\n",
    "\n",
    "    Args:\n",
    "        pipeline_path: Path to the saved .pkl pipeline file.\n",
    "        input_csv_path: Path to the input CSV file (must contain 'Evidence' column).\n",
    "        output_csv_path: Path where the predictions CSV will be saved.\n",
    "    \"\"\"\n",
    "    logger.info(\"\\n\" + \"=\"*70)\n",
    "    logger.info(f\"MAKING PREDICTIONS FROM {input_csv_path}\")\n",
    "    logger.info(\"=\"*70)\n",
    "\n",
    "    # --- Input Validation ---\n",
    "    if not pipeline_path.exists():\n",
    "        logger.error(f\"Pipeline file not found at {pipeline_path}. Cannot make predictions.\")\n",
    "        return\n",
    "    if not input_csv_path.exists():\n",
    "        logger.error(f\"Input CSV file not found at {input_csv_path}. Cannot make predictions.\")\n",
    "        return\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    output_csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # --- Load Pipeline --- \n",
    "        with open(pipeline_path, \"rb\") as f:\n",
    "            loaded_pipeline = pickle.load(f)\n",
    "        logger.info(f\"Pipeline loaded successfully from {pipeline_path}\")\n",
    "\n",
    "        # --- Load and Prepare Input Data ---\n",
    "        input_df = pd.read_csv(input_csv_path)\n",
    "        logger.info(f\"Loaded {len(input_df)} rows from {input_csv_path}\")\n",
    "\n",
    "        if 'Evidence' not in input_df.columns or 'Claim' not in input_df.columns:\n",
    "            logger.error(f\"Input CSV {input_csv_path} must contain 'Evidence' and 'Claim' columns.\")\n",
    "            return\n",
    "            \n",
    "\n",
    "        # Determine training parameters needed for preprocessing\n",
    "        training_vocab_size = params.get('vocab_size', 12000) \n",
    "        logger.info(f\"Using parameters for preprocessing: vocab_size={training_vocab_size}\")\n",
    "\n",
    "\n",
    "        # Apply the *exact same* preprocessing as used during training\n",
    "        processed_data_df, _, _ = prepare_svm_data(\n",
    "            input_df, \n",
    "            remove_stopwords=True,\n",
    "            lemmatize=True,        \n",
    "            min_freq=2, \n",
    "            vocab_size=training_vocab_size\n",
    "        )\n",
    "        processed_texts = processed_data_df['text'].tolist()\n",
    "        logger.info(f\"Preprocessing complete for {len(processed_texts)} texts.\")\n",
    "\n",
    "        # --- Make Predictions --- \n",
    "        predictions = loaded_pipeline.predict(processed_texts)\n",
    "        logger.info(f\"Generated {len(predictions)} predictions.\")\n",
    "\n",
    "        # --- Save Predictions --- \n",
    "        predictions_df = pd.DataFrame({'prediction': predictions})\n",
    "        predictions_df.to_csv(output_csv_path, index=False)\n",
    "        logger.info(f\"Predictions saved successfully to {output_csv_path}\")\n",
    "\n",
    "    except ModuleNotFoundError as e:\n",
    "         logger.error(f\"Error loading pickle: A module required by the pickled object was not found: {e}\")\n",
    "         logger.error(\"Ensure all necessary libraries and custom classes (GloveVectorizer, etc.) are importable.\")\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Error: A required file was not found: {e}\")\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"Error: Missing expected column in input data: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during prediction: {e}\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_pickle_path = \"\"\n",
    "\n",
    "try:\n",
    "    prediction_input_file = config.DEV_FILE\n",
    "    prediction_output_file = config.DATA_DIR / \"svm_predictions.csv\"\n",
    "    \n",
    "    # Ensure the predictions directory exists\n",
    "    prediction_output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    predict_with_saved_model(\n",
    "        pipeline_path=pipeline_pickle_path,\n",
    "        input_csv_path=prediction_input_file, \n",
    "        output_csv_path=prediction_output_file\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error predicting with saved model: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code cells below show how we Augmented the data, trained the model and evaluated the final model\n",
    "### These cells don't need to be run (and probably wont work if run, as we didn't use notebooks to complete the svm). They are to show the process of the svm creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Translation\n",
    "\n",
    "This code performs back translation to create paraphrased versions of text data by translating it to another language and back. This augments the dataset, helping the model by increasing the variety of training examples, improving generalisation and robustness, especially when labeled data is limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Back translation module for data augmentation.\n",
    "\n",
    "This module implements back translation, a technique that translates text to an\n",
    "intermediate language and then back to the original language to create paraphrased\n",
    "versions of the original text, used for data augmentation.\n",
    "\"\"\"\n",
    "import asyncio\n",
    "from googletrans import Translator\n",
    "\n",
    "\n",
    "async def back_translate_batch(data: pd.DataFrame, column: str, src='en', intermediate='fr') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply back-translation to a batch of text in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame containing the text to translate\n",
    "        column: Column name to translate (or \"Both\" for \"Claim\" and \"Evidence\")\n",
    "        src: Source language code\n",
    "        intermediate: Intermediate language code\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with translated text\n",
    "    \"\"\"\n",
    "    async with Translator() as translator:\n",
    "        async def translate_text(text):\n",
    "            \"\"\"\n",
    "            Translate a single text with retry mechanism.\n",
    "            \n",
    "            Makes up to 3 attempts to translate the text, waiting 0.5s between attempts.\n",
    "            Returns original text if all attempts fail.\n",
    "            \"\"\"\n",
    "            for attempt in range(3):\n",
    "                try:\n",
    "                    translation = await translator.translate(text, src=src, dest=intermediate)\n",
    "                    back_to_source = await translator.translate(translation.text, src=intermediate, dest=src)\n",
    "                    return back_to_source.text\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                    await asyncio.sleep(0.5)\n",
    "            return text\n",
    "        \n",
    "        if column == \"Both\":\n",
    "            data.loc[:, \"Claim\"] = [await translate_text(text) for text in data[\"Claim\"]]\n",
    "            data.loc[:, \"Evidence\"] = [await translate_text(text) for text in data[\"Evidence\"]]\n",
    "        else:\n",
    "            data.loc[:, column] = [await translate_text(text) for text in data[column]]\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonym Replacement, Addition, Deletion\n",
    "\n",
    "The code is able to perform synonym replacement while maintaining the text's semantic meaning. It can also perferm random synonym addition and random word deletion. These methods increase the variety of the training data and should help with generalisation.\n",
    "\n",
    "Warning: it's quite a lot of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "class AdvancedSynonymReplacer:\n",
    "    \"\"\"\n",
    "    Advanced data augmentation class that performs synonym replacement with semantic similarity control.\n",
    "    \n",
    "    This class enhances text data by replacing words with their synonyms while maintaining\n",
    "    semantic meaning of the original text. It uses Sentence Transformers to verify \n",
    "    that augmented sentences remain semantically similar to the original.\n",
    "    \n",
    "    Attributes:\n",
    "        params (dict): Configuration parameters for augmentation.\n",
    "        train_df (pd.DataFrame): Training data containing 'Evidence', 'Claim', and 'label' columns.\n",
    "        device (str): Computing device ('cpu' or 'cuda').\n",
    "        st_model (SentenceTransformer): Model for semantic similarity measurement.\n",
    "        stop_words (set): Set of English stop words to ignore during augmentation.\n",
    "        word_frequencies (Counter): Word frequency counter from training data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params: dict, train_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize the synonym replacement augmentation with specified parameters.\n",
    "        \n",
    "        Args:\n",
    "            params (dict): Configuration parameters for the augmentation process.\n",
    "            train_df (pd.DataFrame): Training data with 'Evidence', 'Claim', and 'label' columns.\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.device = get_device()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # Load Sentence Transformer Model\n",
    "        self.st_model_name = params.get('sentence_transformer_model', 'sentence-transformers/all-MiniLM-L6-v2')\n",
    "        logging.info(f\"Loading Sentence Transformer model: {self.st_model_name} onto device: {self.device}\")\n",
    "        self.st_model = SentenceTransformer(self.st_model_name, device=self.device)\n",
    "        logging.info(\"Sentence Transformer model loaded.\")\n",
    "\n",
    "        # Configuration parameters\n",
    "        self.min_sentence_similarity = params.get(\"min_sentence_similarity\", 0.85)\n",
    "        self.replacement_fraction = params.get(\"replacement_fraction\", 0.5)\n",
    "        self.batch_size = params.get(\"batch_size\", 1000)\n",
    "        self.add_original_evidence_to_results = params.get(\"add_original_evidence_to_results\", True)\n",
    "        self.results_file_name = params.get(\"output_file\", config.DATA_DIR / \"advanced_synonym_replacement_results.csv\")\n",
    "        self.min_word_length = params.get(\"min_word_length\", 4)\n",
    "        self.synonym_selection_strategy = params.get(\"synonym_selection_strategy\", \"random\")\n",
    "        self.allow_multi_word_synonyms = params.get(\"allow_multi_word_synonyms\", False)\n",
    "        self.word_frequency_threshold = params.get(\"word_frequency_threshold\", 5)\n",
    "        \n",
    "        # Advanced augmentation settings\n",
    "        self.enable_random_synonym_insertion = params.get(\"enable_random_synonym_insertion\", False)\n",
    "        self.synonym_insertion_probability = params.get(\"synonym_insertion_probability\", 0.05)\n",
    "        \n",
    "        self.enable_random_word_insertion = params.get(\"enable_random_word_insertion\", False)\n",
    "        self.word_insertion_probability = params.get(\"word_insertion_probability\", 0.05)\n",
    "        \n",
    "        self.enable_random_deletion = params.get(\"enable_random_synonym_deletion\", False)\n",
    "        self.deletion_probability = params.get(\"deletion_probability\", 0.05)\n",
    "\n",
    "        # Store original DataFrame and prepare data\n",
    "        self.train_df = train_df.copy()\n",
    "        self._prepare_data()\n",
    "\n",
    "        logging.info(\"Starting advanced data augmentation with the following parameters:\")\n",
    "        for key, value in params.items():\n",
    "            logging.info(f\" - {key}: {value}\")\n",
    "\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        \"\"\"\n",
    "        Prepare the training data by adding POS tags and calculating word frequencies.\n",
    "        \n",
    "        This method tokenizes evidence sentences, adds part-of-speech tags, and\n",
    "        builds a word frequency dictionary for later use in the augmentation process.\n",
    "        \"\"\"\n",
    "        if 'POS' not in self.train_df.columns:\n",
    "            self.train_df['POS_Evidence'] = self.train_df['Evidence'].apply(\n",
    "                lambda x: nltk.pos_tag(nltk.word_tokenize(x))\n",
    "            )\n",
    "\n",
    "        self.original_evidences_pos = self.train_df['POS_Evidence'].tolist()\n",
    "        self.original_evidences = self.train_df['Evidence'].tolist()\n",
    "        self.preprocessed_evidences = self.train_df['Evidence'].apply(remove_stopwords).tolist()\n",
    "        self.corresponding_claim = self.train_df['Claim'].apply(remove_stopwords).tolist()\n",
    "\n",
    "        # Calculate word frequencies\n",
    "        all_words = []\n",
    "        for text in self.train_df['Evidence']:\n",
    "            all_words.extend(nltk.word_tokenize(text.lower()))\n",
    "        self.word_frequencies = Counter(all_words)\n",
    "\n",
    "\n",
    "    def calculate_sentence_similarity(self, sentence_1: str, sentence_2: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the semantic similarity between two sentences.\n",
    "        \n",
    "        Uses the Sentence Transformer model to generate embeddings and compute\n",
    "        the cosine similarity between them.\n",
    "        \n",
    "        Args:\n",
    "            sentence_1: First sentence to compare\n",
    "            sentence_2: Second sentence to compare\n",
    "            \n",
    "        Returns:\n",
    "            float: Similarity score between 0 and 1, where 1 indicates identical meaning\n",
    "        \"\"\"\n",
    "        embeddings = self.st_model.encode([sentence_1, sentence_2], convert_to_tensor=True, device=self.device, verbose=False)\n",
    "        cosine_scores = util.cos_sim(embeddings[0], embeddings[1])\n",
    "        return cosine_scores.item()\n",
    "\n",
    "\n",
    "    def _get_wordnet_pos(self, tag):\n",
    "        \"\"\"\n",
    "        Map NLTK POS tags to WordNet POS tags.\n",
    "        \n",
    "        Args:\n",
    "            tag: NLTK part-of-speech tag\n",
    "            \n",
    "        Returns:\n",
    "            WordNet POS constant or None if no matching tag found\n",
    "        \"\"\"\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def _process_text(\n",
    "        self,\n",
    "        text_tokens: list[str],\n",
    "        pos_tags: list[tuple[str, str]],\n",
    "        claim_words: set = None,\n",
    "        is_claim: bool = False\n",
    "    ) -> list[str]:\n",
    "        \"\"\"\n",
    "        Process text to identify candidate words eligible for replacement.\n",
    "        \n",
    "        Analyzes the tokens and their POS tags to determine which words can be\n",
    "        replaced with synonyms based on various criteria such as word length,\n",
    "        frequency, and part of speech.\n",
    "        \n",
    "        Args:\n",
    "            text_tokens: List of tokenized words from the text\n",
    "            pos_tags: List of (word, POS tag) tuples\n",
    "            claim_words: Set of words in the claim (if processing evidence)\n",
    "            is_claim: Whether the text being processed is a claim\n",
    "            \n",
    "        Returns:\n",
    "            List of words eligible for synonym replacement\n",
    "        \"\"\"\n",
    "        potential_replacements = []\n",
    "        safe_pos_tags = {\n",
    "            'NN', 'NNS', 'JJ', 'JJR', 'JJS',\n",
    "            'RB', 'RBR', 'RBS',\n",
    "            'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "        }\n",
    "\n",
    "        pos_tags_dict = defaultdict(list)\n",
    "        for word, tag in pos_tags:\n",
    "            pos_tags_dict[word.lower()].append(tag)\n",
    "\n",
    "        common_words = set()\n",
    "        if claim_words is not None and not is_claim:\n",
    "            common_words = set(text_tokens) & claim_words\n",
    "\n",
    "        for word in text_tokens:\n",
    "            lower_word = word.lower()\n",
    "\n",
    "            # Skip if stop word\n",
    "            if lower_word in self.stop_words:\n",
    "                continue\n",
    "\n",
    "            # Skip based on frequency\n",
    "            if self.word_frequencies.get(lower_word, 0) < self.word_frequency_threshold:\n",
    "                continue\n",
    "\n",
    "            # Skip if word is in both claim and evidence (if applicable and not processing claim)\n",
    "            # or if the word is a substring of any claim word (or vice versa)\n",
    "            if (not is_claim and (word in common_words or\n",
    "                    any(word in cw or cw in word for cw in claim_words))):\n",
    "                continue\n",
    "                \n",
    "            # Skip if word not in pos_tags_dict\n",
    "            if lower_word not in pos_tags_dict:\n",
    "                continue\n",
    "                \n",
    "            # Skip if word is too short\n",
    "            if len(word) < self.min_word_length:\n",
    "                continue\n",
    "                \n",
    "            # Skip if the word's POS tags are not in the safe list\n",
    "            if not any(tag in safe_pos_tags for tag in pos_tags_dict[lower_word]):\n",
    "                continue\n",
    "                \n",
    "            potential_replacements.append(word)\n",
    "\n",
    "        return potential_replacements\n",
    "\n",
    "\n",
    "    def get_synonyms(self, word: str, pos_tag: str = None, topn: int = 10) -> list[str]:\n",
    "        \"\"\"\n",
    "        Retrieve synonyms for a word using WordNet.\n",
    "        \n",
    "        Finds synonyms that match the part of speech of the original word\n",
    "        and applies filtering based on configuration parameters.\n",
    "        \n",
    "        Args:\n",
    "            word: Target word to find synonyms for\n",
    "            pos_tag: Part-of-speech tag of the word\n",
    "            topn: Maximum number of synonyms to return\n",
    "            \n",
    "        Returns:\n",
    "            List of potential synonyms (empty if none found)\n",
    "        \"\"\"\n",
    "        synonyms = set()\n",
    "        wordnet_pos = self._get_wordnet_pos(pos_tag) if pos_tag else None\n",
    "\n",
    "        synsets = wordnet.synsets(word, pos=wordnet_pos)\n",
    "        if not synsets:\n",
    "            return []\n",
    "\n",
    "        for syn in synsets:\n",
    "            for lemma in syn.lemmas():\n",
    "                synonym = lemma.name().replace('_', ' ')\n",
    "                # Only add if not the same as original word\n",
    "                if synonym.lower() != word.lower():\n",
    "                    # Filter multi-word synonyms based on configuration\n",
    "                    if self.allow_multi_word_synonyms or ' ' not in synonym:\n",
    "                        synonyms.add(synonym)\n",
    "                        \n",
    "                    if len(synonyms) >= topn:\n",
    "                        break\n",
    "                    \n",
    "            if len(synonyms) >= topn:\n",
    "                break\n",
    "\n",
    "        synonym_list = list(synonyms)\n",
    "        \n",
    "        # Apply synonym selection strategy\n",
    "        if self.synonym_selection_strategy == 'frequent':\n",
    "            # Prioritize synonyms that appear more frequently in the training data\n",
    "            synonym_list.sort(key=lambda s: self.word_frequencies.get(s.lower(), 0), reverse=True)\n",
    "        elif self.synonym_selection_strategy == 'random':\n",
    "            random.shuffle(synonym_list)\n",
    "\n",
    "        return synonym_list[:topn]\n",
    "\n",
    "    def get_random_word(self) -> str:\n",
    "        \"\"\"\n",
    "        Get a random word from the training data vocabulary.\n",
    "        \n",
    "        Returns:\n",
    "            A random word from the training data vocabulary\n",
    "        \"\"\"\n",
    "        return random.choice(list(self.word_frequencies.keys()))\n",
    "\n",
    "\n",
    "    def find_valid_replacements(\n",
    "        self,\n",
    "        word_to_replace: str,\n",
    "        synonyms: list[str],\n",
    "        original_text: str,\n",
    "        original_pos_tags: dict\n",
    "    ) -> tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Find a valid synonym replacement that maintains semantic similarity.\n",
    "        \n",
    "        Tests each synonym to see if it:\n",
    "        1. Maintains the same part of speech\n",
    "        2. Keeps the sentence semantically similar to the original\n",
    "        \n",
    "        Args:\n",
    "            word_to_replace: The original word to be replaced\n",
    "            synonyms: List of potential synonym candidates\n",
    "            original_text: The complete original text\n",
    "            original_pos_tags: Dictionary mapping words to their POS tags\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (success_flag, replacement_word)\n",
    "        \"\"\"\n",
    "        lower_word = word_to_replace.lower()\n",
    "        original_word_pos_tags = original_pos_tags.get(lower_word, [])\n",
    "        \n",
    "        if not original_word_pos_tags:\n",
    "            return False, \"\"\n",
    "            \n",
    "        primary_pos = original_word_pos_tags[0]\n",
    "\n",
    "        for synonym in synonyms:\n",
    "            # Create pattern to replace only whole words (not substrings)\n",
    "            pattern = r'\\b' + re.escape(word_to_replace) + r'\\b'\n",
    "            \n",
    "            try:\n",
    "                new_text = re.sub(pattern, synonym, original_text, flags=re.IGNORECASE)\n",
    "            except re.error:\n",
    "                logging.warning(f\"Regex error replacing '{word_to_replace}' with '{synonym}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Skip if no replacement was made\n",
    "            if new_text == original_text:\n",
    "                continue\n",
    "\n",
    "            # Check if the POS tag is preserved\n",
    "            new_text_pos = nltk.pos_tag(nltk.word_tokenize(new_text))\n",
    "            synonym_pos_tags = [tag for (w, tag) in new_text_pos if w.lower() == synonym.lower()]\n",
    "            \n",
    "            if not synonym_pos_tags:\n",
    "                continue\n",
    "\n",
    "            # Verify that the WordNet POS category is maintained\n",
    "            if self._get_wordnet_pos(synonym_pos_tags[0]) != self._get_wordnet_pos(primary_pos):\n",
    "                continue\n",
    "\n",
    "            # Check semantic similarity to ensure meaning is preserved\n",
    "            similarity = self.calculate_sentence_similarity(original_text, new_text)\n",
    "            if similarity >= self.min_sentence_similarity:\n",
    "                return True, synonym\n",
    "\n",
    "        return False, \"\"\n",
    "\n",
    "\n",
    "    def _random_insertion(self, tokens: list[str], pos_tags: list[tuple[str, str]], add_a_synonym: bool = True) -> list[str]:\n",
    "        \"\"\"\n",
    "        Randomly insert a word or synonym into the token list.\n",
    "        \n",
    "        Args:\n",
    "            tokens: List of tokens to augment\n",
    "            pos_tags: List of (word, POS tag) tuples\n",
    "            add_a_synonym: Whether to insert a synonym (True) or random word (False)\n",
    "            \n",
    "        Returns:\n",
    "            Augmented list of tokens with the insertion\n",
    "        \"\"\"\n",
    "        augmented_tokens = list(tokens)\n",
    "        \n",
    "        if not augmented_tokens:\n",
    "            return []\n",
    "        \n",
    "        # Choose random position and word\n",
    "        insert_index = random.randint(0, len(augmented_tokens))\n",
    "        word_to_augment = random.choice(augmented_tokens)\n",
    "        lower_word = word_to_augment.lower()\n",
    "        \n",
    "        # Get POS tag for the word\n",
    "        word_pos_dict = dict(pos_tags)\n",
    "        original_word_pos_tag = word_pos_dict.get(lower_word)\n",
    "        \n",
    "        if not original_word_pos_tag:\n",
    "            return augmented_tokens\n",
    "        \n",
    "        # Get either a synonym or random word\n",
    "        if add_a_synonym:\n",
    "            candidates = self.get_synonyms(word_to_augment, original_word_pos_tag, topn=5)\n",
    "        else:\n",
    "            candidates = [self.get_random_word()]\n",
    "            \n",
    "        if not candidates:\n",
    "            return augmented_tokens\n",
    "        \n",
    "        # Insert the selected word\n",
    "        word_to_insert = random.choice(candidates)\n",
    "        augmented_tokens.insert(insert_index, word_to_insert)\n",
    "                    \n",
    "        return augmented_tokens\n",
    "\n",
    "\n",
    "    def _random_deletion(self, tokens: list[str]) -> list[str]:\n",
    "        \"\"\"\n",
    "        Randomly delete words from the token list.\n",
    "        \n",
    "        Deletes up to 10% of words from the input tokens.\n",
    "        \n",
    "        Args:\n",
    "            tokens: List of tokens to augment\n",
    "            \n",
    "        Returns:\n",
    "            Augmented list of tokens with deletions\n",
    "        \"\"\"\n",
    "        if not tokens:\n",
    "            return []\n",
    "        \n",
    "        # Create a copy of the tokens list\n",
    "        augmented_tokens = tokens.copy()\n",
    "        \n",
    "        # Choose random tokens to delete (up to 10%)\n",
    "        max_deletions = max(1, int(len(augmented_tokens) * 0.1))\n",
    "        num_to_delete = random.randint(1, max_deletions)\n",
    "        indices_to_delete = random.sample(range(len(augmented_tokens)), num_to_delete)\n",
    "        \n",
    "        # Create new list without the deleted tokens\n",
    "        augmented_tokens = [token for i, token in enumerate(augmented_tokens) \n",
    "                           if i not in indices_to_delete]\n",
    "        \n",
    "        return augmented_tokens\n",
    "\n",
    "\n",
    "    def augment_data(self):\n",
    "        \"\"\"\n",
    "        Perform data augmentation on the training dataset.\n",
    "        \n",
    "        This method applies synonym replacement, insertion, and deletion operations\n",
    "        to generate augmented versions of the evidence texts. The augmentation\n",
    "        preserves the semantic similarity above the specified threshold.\n",
    "        \n",
    "        The augmented data is saved in batches to the specified output file.\n",
    "        \n",
    "        Returns:\n",
    "            int: Number of successful augmentations performed\n",
    "        \"\"\"\n",
    "        # Check if results file exists and confirm overwrite\n",
    "        results_path = Path(self.results_file_name)\n",
    "        if results_path.exists():\n",
    "            overwrite = input(\n",
    "                f\"Results file {self.results_file_name} already exists. Overwrite? (y/n) \"\n",
    "            ).strip().lower()\n",
    "            if overwrite != 'y':\n",
    "                logging.info(\"Augmentation aborted by user.\")\n",
    "                return 0\n",
    "        else:\n",
    "            results_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Prepare output dataframe columns\n",
    "        cols = [\"Claim\", \"Evidence\", \"label\"]\n",
    "        if self.add_original_evidence_to_results:\n",
    "            cols.append(\"Original Evidence\")\n",
    "            cols.append(\"Similarity Score\")\n",
    "\n",
    "        # Initialize tracking variables\n",
    "        synonym_replaced_df = pd.DataFrame(columns=cols)\n",
    "        batch_counter = 0\n",
    "        successful_augmentations = 0\n",
    "        attempted_augmentations = 0\n",
    "        total_words_augmented = 0\n",
    "\n",
    "        # Get data from original dataframe\n",
    "        original_claims = self.train_df['Claim'].tolist()\n",
    "        labels = self.train_df['label'].tolist()\n",
    "\n",
    "        # Process each evidence text\n",
    "        for idx, original_evidence_text in tqdm(\n",
    "            enumerate(self.original_evidences),\n",
    "            desc=\"Augmenting data\",\n",
    "            total=len(self.original_evidences)\n",
    "        ):\n",
    "            attempted_augmentations += 1\n",
    "            original_claim_text = original_claims[idx]\n",
    "\n",
    "            # Get POS tagging for the evidence\n",
    "            evidence_pos_tags = self.original_evidences_pos[idx]\n",
    "            evidence_pos_tags_dict = defaultdict(list)\n",
    "            for word, tag in evidence_pos_tags:\n",
    "                evidence_pos_tags_dict[word.lower()].append(tag)\n",
    "            evidence_tokens = nltk.word_tokenize(original_evidence_text)\n",
    "\n",
    "            # Start with original tokens\n",
    "            augmented_evidence_tokens = list(evidence_tokens)\n",
    "            \n",
    "            # Apply random augmentations based on probabilities\n",
    "            # 1. Insert synonyms\n",
    "            should_insert_synonyms = random.random() < self.synonym_insertion_probability\n",
    "            if self.enable_random_synonym_insertion and should_insert_synonyms:\n",
    "                augmented_evidence_tokens = self._random_insertion(\n",
    "                    augmented_evidence_tokens, evidence_pos_tags, add_a_synonym=True)\n",
    "            \n",
    "            # 2. Insert random words    \n",
    "            should_insert_words = random.random() < self.word_insertion_probability\n",
    "            if self.enable_random_word_insertion and should_insert_words:\n",
    "                augmented_evidence_tokens = self._random_insertion(\n",
    "                    augmented_evidence_tokens, evidence_pos_tags, add_a_synonym=False)\n",
    "            \n",
    "            # 3. Delete words    \n",
    "            should_delete_words = random.random() < self.deletion_probability\n",
    "            if self.enable_random_deletion and should_delete_words:\n",
    "                augmented_evidence_tokens = self._random_deletion(augmented_evidence_tokens)\n",
    "\n",
    "            # Find candidate words for synonym replacement\n",
    "            potential_evidence_replacements = self._process_text(\n",
    "                augmented_evidence_tokens,\n",
    "                evidence_pos_tags,\n",
    "                claim_words=set()\n",
    "            )\n",
    "\n",
    "            # Perform the synonym replacements\n",
    "            num_evidence_replacements = max(0, int(len(potential_evidence_replacements) * self.replacement_fraction))\n",
    "            \n",
    "            if potential_evidence_replacements and num_evidence_replacements > 0:\n",
    "                words_to_replace = random.sample(potential_evidence_replacements, k=num_evidence_replacements)\n",
    "                current_evidence = \" \".join(augmented_evidence_tokens)\n",
    "                final_word_replacement_map_evidence = {}\n",
    "\n",
    "                for word in words_to_replace:\n",
    "                    lower_word = word.lower()\n",
    "                    if lower_word not in evidence_pos_tags_dict or not evidence_pos_tags_dict[lower_word]:\n",
    "                        continue\n",
    "\n",
    "                    word_pos_tag = evidence_pos_tags_dict[lower_word][0]\n",
    "                    synonyms = self.get_synonyms(word, word_pos_tag, topn=10)\n",
    "                    if not synonyms:\n",
    "                        continue\n",
    "\n",
    "                    found, replacement = self.find_valid_replacements(\n",
    "                        word,\n",
    "                        synonyms,\n",
    "                        current_evidence,\n",
    "                        evidence_pos_tags_dict\n",
    "                    )\n",
    "                    if found:\n",
    "                        pattern = r'\\b' + re.escape(word) + r'\\b'\n",
    "                        try:\n",
    "                            current_evidence = re.sub(pattern, replacement, current_evidence, flags=re.IGNORECASE)\n",
    "                            final_word_replacement_map_evidence[word] = replacement\n",
    "                            total_words_augmented += 1\n",
    "                        except re.error:\n",
    "                            logging.warning(f\"Regex error applying replacement for '{word}' with '{replacement}'.\")\n",
    "                \n",
    "                augmented_evidence_text = current_evidence\n",
    "            else:\n",
    "                augmented_evidence_text = \" \".join(augmented_evidence_tokens)\n",
    "                \n",
    "            # Clean up text formatting and spacing\n",
    "            augmented_evidence_text = self._clean_text_formatting(augmented_evidence_text)\n",
    "\n",
    "            # Validate the final augmented evidence\n",
    "            final_similarity_score = self.calculate_sentence_similarity(original_evidence_text, augmented_evidence_text)\n",
    "            if final_similarity_score >= self.min_sentence_similarity:\n",
    "                # Add the augmented example to the results\n",
    "                new_row_data = {\n",
    "                    \"Claim\": original_claim_text,\n",
    "                    \"Evidence\": augmented_evidence_text,\n",
    "                    \"label\": labels[idx]\n",
    "                }\n",
    "                \n",
    "                if self.add_original_evidence_to_results:\n",
    "                    new_row_data[\"Original Evidence\"] = original_evidence_text\n",
    "                    new_row_data[\"Similarity Score\"] = final_similarity_score\n",
    "\n",
    "                synonym_replaced_df = pd.concat([\n",
    "                    synonym_replaced_df,\n",
    "                    pd.DataFrame([new_row_data])\n",
    "                ], ignore_index=True)\n",
    "                successful_augmentations += 1\n",
    "\n",
    "            # Save batch if it reaches batch size\n",
    "            if len(synonym_replaced_df) >= self.batch_size:\n",
    "                self._save_batch(synonym_replaced_df, batch_counter)\n",
    "                synonym_replaced_df = pd.DataFrame(columns=cols)\n",
    "                batch_counter += 1\n",
    "\n",
    "        # Save any remaining augmented data\n",
    "        if not synonym_replaced_df.empty:\n",
    "            self._save_batch(synonym_replaced_df, batch_counter)\n",
    "\n",
    "        # Log final statistics\n",
    "        self._log_augmentation_stats(successful_augmentations, attempted_augmentations, total_words_augmented)\n",
    "\n",
    "        return successful_augmentations\n",
    "        \n",
    "    def _clean_text_formatting(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean up text spacing and punctuation.\n",
    "        \n",
    "        Args:\n",
    "            text: Text to clean\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned text with proper spacing and punctuation\n",
    "        \"\"\"\n",
    "        # Trim whitespace\n",
    "        cleaned_text = text.strip()\n",
    "        \n",
    "        # Ensure symbols like '$' and '' have a space before them if not at the start\n",
    "        cleaned_text = re.sub(r'(?<!\\s)([$])', r' \\1', cleaned_text)\n",
    "        \n",
    "        # Ensure hyphens are attached with no spaces around them\n",
    "        cleaned_text = re.sub(r'\\s*-\\s*', '-', cleaned_text)\n",
    "        \n",
    "        # Remove extra spaces before punctuation\n",
    "        cleaned_text = re.sub(r'\\s+([,.;?!])', r'\\1', cleaned_text)\n",
    "        \n",
    "        # Adjust spacing around quotes\n",
    "        cleaned_text = re.sub(r'(?<!\\s)([\"\\\"])', r' \\1', cleaned_text)\n",
    "        cleaned_text = re.sub(r'([\"\\\"])(\\s+)', r'\\1', cleaned_text)\n",
    "        \n",
    "        # Collapse multiple spaces into one\n",
    "        cleaned_text = re.sub(r'\\s{2,}', ' ', cleaned_text)\n",
    "        \n",
    "        return cleaned_text\n",
    "        \n",
    "    def _save_batch(self, df: pd.DataFrame, batch_counter: int):\n",
    "        \"\"\"\n",
    "        Save a batch of augmented data to the output file.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing the batch data\n",
    "            batch_counter: Current batch number\n",
    "        \"\"\"\n",
    "        mode = 'w' if batch_counter == 0 else 'a'\n",
    "        header = (batch_counter == 0)\n",
    "        logging.info(f\"Saving batch {batch_counter} to {self.results_file_name}\")\n",
    "        df.to_csv(self.results_file_name, index=False, mode=mode, header=header)\n",
    "        \n",
    "    def _log_augmentation_stats(self, successful: int, attempted: int, words_augmented: int):\n",
    "        \"\"\"\n",
    "        Log statistics about the augmentation process.\n",
    "        \n",
    "        Args:\n",
    "            successful: Number of successful augmentations\n",
    "            attempted: Number of attempted augmentations\n",
    "            words_augmented: Total number of words augmented\n",
    "        \"\"\"\n",
    "        logging.info(f\"Augmentation completed. {successful} sentences successfully augmented \"\n",
    "                     f\"out of {attempted} attempts.\")\n",
    "        logging.info(f\"Total words augmented: {words_augmented}\")\n",
    "        \n",
    "        if attempted > 0:\n",
    "            success_rate = (successful / attempted) * 100\n",
    "            logging.info(f\"Success rate: {success_rate:.2f}%\")\n",
    "        else:\n",
    "            logging.info(\"No augmentation attempts were made.\")\n",
    "\n",
    "\n",
    "class AdvancedSynonymReplacerDF(AdvancedSynonymReplacer):\n",
    "    \"\"\"\n",
    "    In-place synonym replacement for DataFrame augmentation.\n",
    "    \n",
    "    This class extends AdvancedSynonymReplacer to modify the input DataFrame directly\n",
    "    without creating a separate output file. Useful for integration with transformer\n",
    "    model pipelines where the entire DataFrame needs to be processed in-memory.\n",
    "    \n",
    "    The original evidence texts are replaced with their augmented versions\n",
    "    while maintaining the same DataFrame structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params: dict, train_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize the in-place DataFrame augmenter.\n",
    "        \n",
    "        Args:\n",
    "            params: Configuration parameters for the augmentation process\n",
    "            train_df: Training data with 'Evidence', 'Claim', and 'label' columns\n",
    "        \"\"\"\n",
    "        super().__init__(params, train_df)\n",
    "        self.train_df = train_df\n",
    "        self._prepare_data()\n",
    "        \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"\n",
    "        Prepare the training data by adding POS tags and calculating word frequencies.\n",
    "        \n",
    "        Unlike the parent class, this implementation doesn't remove stopwords\n",
    "        since the full text is directly modified.\n",
    "        \"\"\"\n",
    "        if 'POS' not in self.train_df.columns:\n",
    "            self.train_df['POS_Evidence'] = self.train_df['Evidence'].apply(\n",
    "                lambda x: nltk.pos_tag(nltk.word_tokenize(x))\n",
    "            )\n",
    "\n",
    "        # Calculate word frequencies\n",
    "        all_words = []\n",
    "        for text in self.train_df['Evidence']:\n",
    "            all_words.extend(nltk.word_tokenize(text.lower()))\n",
    "        self.word_frequencies = Counter(all_words)\n",
    "    \n",
    "    def augment_data(self):\n",
    "        \"\"\"\n",
    "        Perform in-place data augmentation on the DataFrame.\n",
    "        \n",
    "        Modifies the 'Evidence' column of the input DataFrame directly with\n",
    "        augmented versions of the texts.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Reference to the modified input DataFrame\n",
    "        \"\"\"\n",
    "        successful_augmentations = 0\n",
    "        attempted_augmentations = 0\n",
    "        total_words_augmented = 0\n",
    "\n",
    "        # Iterate through each row in the DataFrame\n",
    "        for idx, row in tqdm(\n",
    "            self.train_df.iterrows(),\n",
    "            desc=\"Augmenting data\",\n",
    "            total=len(self.train_df)\n",
    "        ):\n",
    "            attempted_augmentations += 1\n",
    "            original_evidence_text = row['Evidence']\n",
    "            original_claim_text = row['Claim']\n",
    "\n",
    "            # Get POS tagging for the evidence\n",
    "            evidence_pos_tags = row['POS_Evidence']\n",
    "            evidence_pos_tags_dict = defaultdict(list)\n",
    "            for word, tag in evidence_pos_tags:\n",
    "                evidence_pos_tags_dict[word.lower()].append(tag)\n",
    "            evidence_tokens = nltk.word_tokenize(original_evidence_text)\n",
    "\n",
    "            # Start with original tokens\n",
    "            augmented_evidence_tokens = list(evidence_tokens)\n",
    "            \n",
    "            # Apply random augmentations based on probabilities\n",
    "            # 1. Insert synonyms\n",
    "            should_insert_synonyms = random.random() < self.synonym_insertion_probability\n",
    "            if self.enable_random_synonym_insertion and should_insert_synonyms:\n",
    "                augmented_evidence_tokens = self._random_insertion(\n",
    "                    augmented_evidence_tokens, evidence_pos_tags, add_a_synonym=True)\n",
    "            \n",
    "            # 2. Insert random words    \n",
    "            should_insert_words = random.random() < self.word_insertion_probability\n",
    "            if self.enable_random_word_insertion and should_insert_words:\n",
    "                augmented_evidence_tokens = self._random_insertion(\n",
    "                    augmented_evidence_tokens, evidence_pos_tags, add_a_synonym=False)\n",
    "            \n",
    "            # 3. Delete words    \n",
    "            should_delete_words = random.random() < self.deletion_probability\n",
    "            if self.enable_random_deletion and should_delete_words:\n",
    "                augmented_evidence_tokens = self._random_deletion(augmented_evidence_tokens)\n",
    "\n",
    "            # Find candidate words for synonym replacement\n",
    "            potential_evidence_replacements = self._process_text(\n",
    "                augmented_evidence_tokens,\n",
    "                evidence_pos_tags,\n",
    "                claim_words=set()\n",
    "            )\n",
    "\n",
    "            # Perform the synonym replacements\n",
    "            num_evidence_replacements = max(0, int(len(potential_evidence_replacements) * self.replacement_fraction))\n",
    "            \n",
    "            if potential_evidence_replacements and num_evidence_replacements > 0:\n",
    "                words_to_replace = random.sample(potential_evidence_replacements, k=num_evidence_replacements)\n",
    "                current_evidence = \" \".join(augmented_evidence_tokens)\n",
    "                final_word_replacement_map_evidence = {}\n",
    "\n",
    "                for word in words_to_replace:\n",
    "                    lower_word = word.lower()\n",
    "                    if lower_word not in evidence_pos_tags_dict or not evidence_pos_tags_dict[lower_word]:\n",
    "                        continue\n",
    "\n",
    "                    word_pos_tag = evidence_pos_tags_dict[lower_word][0]\n",
    "                    synonyms = self.get_synonyms(word, word_pos_tag, topn=10)\n",
    "                    if not synonyms:\n",
    "                        continue\n",
    "\n",
    "                    found, replacement = self.find_valid_replacements(\n",
    "                        word,\n",
    "                        synonyms,\n",
    "                        current_evidence,\n",
    "                        evidence_pos_tags_dict\n",
    "                    )\n",
    "                    if found:\n",
    "                        pattern = r'\\b' + re.escape(word) + r'\\b'\n",
    "                        try:\n",
    "                            current_evidence = re.sub(pattern, replacement, current_evidence, flags=re.IGNORECASE)\n",
    "                            final_word_replacement_map_evidence[word] = replacement\n",
    "                            total_words_augmented += 1\n",
    "                        except re.error:\n",
    "                            logging.warning(f\"Regex error applying replacement for '{word}' with '{replacement}'.\")\n",
    "                \n",
    "                augmented_evidence_text = current_evidence\n",
    "            else:\n",
    "                augmented_evidence_text = \" \".join(augmented_evidence_tokens)\n",
    "\n",
    "            # Clean up text formatting and spacing\n",
    "            augmented_evidence_text = self._clean_text_formatting(augmented_evidence_text)\n",
    "\n",
    "            # Validate the final augmented evidence\n",
    "            final_similarity_score = self.calculate_sentence_similarity(original_evidence_text, augmented_evidence_text)\n",
    "            if final_similarity_score >= self.min_sentence_similarity:\n",
    "                # Update the Evidence text directly in the input DataFrame\n",
    "                self.train_df.at[idx, 'Evidence'] = augmented_evidence_text\n",
    "                successful_augmentations += 1\n",
    "\n",
    "        # Log final statistics\n",
    "        self._log_augmentation_stats(successful_augmentations, attempted_augmentations, total_words_augmented)\n",
    "\n",
    "        return self.train_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X or Y augmentation\n",
    "\n",
    "This augmenter finds candidate words in a text and replaces them with a format like \"word/synonym1/synonym2\", creating variations of the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XorYAugmenter:\n",
    "    \"\"\"\n",
    "    A text augmentation class that replaces words with alternatives in X/Y format.\n",
    "    \n",
    "    This augmenter finds candidate words in a text and replaces them with a format\n",
    "    like \"word/synonym1/synonym2\", creating variations of the original text.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_df: pd.DataFrame, similarity_threshold: float = 0.6, \n",
    "                 max_choices: int = 2, num_words_to_augment: int = 1):\n",
    "        \"\"\"\n",
    "        Initialize the XorYAugmenter.\n",
    "        \n",
    "        Args:\n",
    "            train_df: Training dataframe containing text to analyze\n",
    "            similarity_threshold: Threshold for word similarity (0.0-1.0)\n",
    "            max_choices: Maximum number of alternative words to include\n",
    "            num_words_to_augment: Number of words to augment in each text\n",
    "        \"\"\"\n",
    "        self.train_df = train_df\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.max_choices = max_choices\n",
    "        self.num_words_to_augment = num_words_to_augment\n",
    "        \n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.glove_embeddings = glove_embeddings\n",
    "    \n",
    "    def _find_candidates(self, claim: str) -> list[tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Find candidate words for augmentation in the given text.\n",
    "        \n",
    "        Args:\n",
    "            claim: The text to analyze for augmentation candidates\n",
    "            \n",
    "        Returns:\n",
    "            List of (word, POS tag) tuples that are candidates for augmentation\n",
    "        \"\"\"\n",
    "        tokens = nltk.word_tokenize(claim)\n",
    "        pos = nltk.pos_tag(tokens)\n",
    "        \n",
    "        candidates = []\n",
    "        \n",
    "        for word, tag in pos:\n",
    "            # Skip stopwords and words not in our embedding vocabulary\n",
    "            if word.lower() in self.stop_words or word.lower() not in self.glove_embeddings:\n",
    "                continue\n",
    "            \n",
    "            candidates.append((word, tag))\n",
    "            \n",
    "        return candidates\n",
    "                \n",
    "    def _get_wordnet_pos(self, nltk_tag: str) -> str:\n",
    "        \"\"\"\n",
    "        Map NLTK POS tags to WordNet POS tags.\n",
    "        \n",
    "        Args:\n",
    "            nltk_tag: POS tag from NLTK tagger\n",
    "            \n",
    "        Returns:\n",
    "            Corresponding WordNet POS tag\n",
    "        \"\"\"\n",
    "        tag_map = {\n",
    "            'JJ': wordnet.ADJ,\n",
    "            'NN': wordnet.NOUN,\n",
    "            'VB': wordnet.VERB,\n",
    "            'RB': wordnet.ADV,\n",
    "            'MD': wordnet.VERB\n",
    "        }\n",
    "        return tag_map.get(nltk_tag[:2], wordnet.NOUN)\n",
    "        \n",
    "    def _get_similar_words(self, word: str, pos_tag: str = None) -> list[str]:\n",
    "        \"\"\"\n",
    "        Find similar words using WordNet.\n",
    "        \n",
    "        Args:\n",
    "            word: The word to find synonyms for\n",
    "            pos_tag: Part of speech tag to constrain synonyms\n",
    "            \n",
    "        Returns:\n",
    "            List of similar words suitable for augmentation\n",
    "        \"\"\"     \n",
    "        topn = max(4, self.max_choices * 3)\n",
    "        \n",
    "        candidates = set()\n",
    "        wordnet_pos = self._get_wordnet_pos(pos_tag) if pos_tag else None\n",
    "\n",
    "        synsets = wordnet.synsets(word, pos=wordnet_pos)\n",
    "        if not synsets:\n",
    "            return []\n",
    "\n",
    "        # Collect synonyms from WordNet\n",
    "        for syn in synsets:\n",
    "            for lemma in syn.lemmas():\n",
    "                synonym = lemma.name().replace('_', ' ')\n",
    "                if synonym.lower() != word.lower() and ' ' not in synonym:\n",
    "                    candidates.add(synonym)\n",
    "                    if len(candidates) >= topn:\n",
    "                        break\n",
    "            if len(candidates) >= topn:\n",
    "                break\n",
    "\n",
    "        # Preserve original capitalization\n",
    "        synonym_list = list(candidates)\n",
    "        if word[0].isupper():\n",
    "            synonym_list = [s.capitalize() for s in synonym_list]\n",
    "            \n",
    "        # Sample a subset of synonyms\n",
    "        synonyms_to_return = random.sample(synonym_list, min(topn, len(synonym_list)))\n",
    "\n",
    "        return synonyms_to_return\n",
    "    \n",
    "    def _augment_text(self, text: str) -> str | None:\n",
    "        \"\"\"\n",
    "        Augment a single text by replacing words with X/Y alternatives.\n",
    "        \n",
    "        Args:\n",
    "            text: The text to augment\n",
    "            \n",
    "        Returns:\n",
    "            Augmented text or None if augmentation was not possible\n",
    "        \"\"\"\n",
    "        candidates = self._find_candidates(text)\n",
    "        if not candidates:\n",
    "            return None\n",
    "\n",
    "        # Determine the number of candidates to use\n",
    "        num_candidates = min(len(candidates), random.randint(1, self.num_words_to_augment))\n",
    "        candidates = candidates[:num_candidates]\n",
    "\n",
    "        for candidate in candidates:\n",
    "            similar_words = self._get_similar_words(candidate[0], candidate[1])\n",
    "            if not similar_words:\n",
    "                continue\n",
    "\n",
    "            # Select a random number of similar words\n",
    "            num_words = min(len(similar_words), random.randint(1, self.max_choices - 1))\n",
    "            similar_words = random.sample(similar_words, num_words)\n",
    "            similar_words.append(candidate[0])\n",
    "            random.shuffle(similar_words)\n",
    "\n",
    "            text = text.replace(candidate[0], '/'.join(similar_words))\n",
    "\n",
    "        return text\n",
    "\n",
    "    def augment_data(self, data: pd.DataFrame, augment_claim: bool = True, augment_evidence: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Augment a dataset by adding X/Y alternatives to selected fields.\n",
    "        \n",
    "        This method modifies the dataframe in-place, adding alternatives to either\n",
    "        claims, evidence, or both depending on the parameters.\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame containing text to augment\n",
    "            augment_claim: Whether to augment the 'Claim' column\n",
    "            augment_evidence: Whether to augment the 'Evidence' column\n",
    "        \"\"\"\n",
    "        for index, row in tqdm(data.iterrows(), total=len(data), desc=\"Augmenting dataset\"):\n",
    "            if augment_claim:\n",
    "                new_claim = self._augment_text(row['Claim'])\n",
    "                if new_claim:\n",
    "                    data.at[index, 'Claim'] = new_claim\n",
    "            \n",
    "            if augment_evidence:\n",
    "                new_evidence = self._augment_text(row['Evidence'])\n",
    "                if new_evidence:\n",
    "                    data.at[index, 'Evidence'] = new_evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_augmented_samples(df: pd.DataFrame, label_counts: np.int64, num_samples: int) -> list[pd.DataFrame.index]:\n",
    "    \"\"\"\n",
    "    Generate a list of indices of the samples to augment.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to augment\n",
    "        label_counts (np.int64): The number of samples for the label\n",
    "        num_samples (int): The number of samples to augment\n",
    "\n",
    "    Returns:\n",
    "        list[pd.DataFrame.index]: The list of indices of the samples to augment\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    \n",
    "    if num_samples > label_counts:\n",
    "        full_repeats = num_samples // label_counts\n",
    "        indices.extend(df.index.repeat(full_repeats))\n",
    "        num_samples %= label_counts\n",
    "        \n",
    "    if num_samples > 0:\n",
    "        indices.extend(df.sample(num_samples).index)\n",
    "        \n",
    "    return indices\n",
    "\n",
    "\n",
    "async def back_translate_samples(aug_df: pd.DataFrame, label: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Back translate the samples for the specified label.\n",
    "\n",
    "    Args:\n",
    "        aug_df (pd.DataFrame): The dataframe to augment\n",
    "        label (str): The label to augment\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The augmented dataframe\n",
    "    \"\"\"\n",
    "    src = config.AUGMENTATION_CONFIG[label][\"translate\"][\"src\"]\n",
    "\n",
    "    percentage_to_translate = config.AUGMENTATION_CONFIG[label][\"translate\"][\"percentage\"]\n",
    "    samples = aug_df.sample(frac=percentage_to_translate)\n",
    "\n",
    "    splits = config.AUGMENTATION_CONFIG[label][\"translate\"][\"split\"]\n",
    "    languages = config.AUGMENTATION_CONFIG[label][\"translate\"][\"intermediates\"]\n",
    "\n",
    "    claim_count = int(len(samples) * splits[\"Claim\"])\n",
    "    evidence_count = int(len(samples) * splits[\"Evidence\"])\n",
    "\n",
    "    split_samples = {\n",
    "        \"Claim\": samples.iloc[:claim_count],\n",
    "        \"Evidence\": samples.iloc[claim_count: claim_count + evidence_count],\n",
    "        \"Both\": samples.iloc[claim_count + evidence_count:]\n",
    "    }\n",
    "\n",
    "    for text_type, sample in split_samples.items():\n",
    "        count = 0\n",
    "        \n",
    "        for lang, percentage in languages.items():\n",
    "            # Calculate number of samples for this language\n",
    "            num_samples = int(len(sample) * percentage)\n",
    "            \n",
    "            # Handle the remaining samples if we're at the end\n",
    "            if count + num_samples >= len(sample):\n",
    "                aug_df.update(await back_translate_batch(sample.iloc[count:], text_type, src, lang))\n",
    "                break\n",
    "            \n",
    "            # Process the current batch\n",
    "            current_batch = sample.iloc[count:count + num_samples]\n",
    "            aug_df.update(await back_translate_batch(current_batch, text_type, src, lang))\n",
    "            count += num_samples\n",
    "\n",
    "    return aug_df\n",
    "    \n",
    "    \n",
    "def synonym_replace_samples(aug_df: pd.DataFrame, label: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply synonym replacement augmentation to the specified samples.\n",
    "    Modifies the input DataFrame in-place.\n",
    "    \n",
    "    Args:\n",
    "        aug_df (pd.DataFrame): DataFrame containing samples to augment\n",
    "        label (str): Label identifier (\"0\" or \"1\") to get config parameters\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The modified input DataFrame\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting synonym replacement for label {label}\")\n",
    "    \n",
    "    params = config.AUGMENTATION_CONFIG[label][\"synonym_replacement\"]\n",
    "    percentage_to_translate = config.AUGMENTATION_CONFIG[label][\"synonym_replacement\"][\"percentage\"]\n",
    "    samples = aug_df.sample(frac=percentage_to_translate)\n",
    "    \n",
    "    replacer = AdvancedSynonymReplacerDF(params, samples)\n",
    "    replacer.augment_data()  # This now modifies aug_df directly\n",
    "    \n",
    "    logging.info(f\"Completed synonym replacement for label {label}\")\n",
    "    aug_df.update(samples)\n",
    "    return aug_df  # Return the modified DataFrame\n",
    "\n",
    "\n",
    "def x_or_y_augment_samples(aug_df: pd.DataFrame, label: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply x or y augmentation to the specified samples.\n",
    "\n",
    "    Args:\n",
    "        aug_df (pd.DataFrame): The dataframe to augment\n",
    "        label (str): The label to augment\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The augmented dataframe\n",
    "    \"\"\"\n",
    "    percentage_to_augment = config.AUGMENTATION_CONFIG[label][\"x_or_y\"][\"percentage\"]\n",
    "    samples = aug_df.sample(frac=percentage_to_augment)\n",
    "\n",
    "    splits = config.AUGMENTATION_CONFIG[label][\"x_or_y\"][\"split\"]\n",
    "    claim_count = int(len(samples) * splits[\"Claim\"])\n",
    "    evidence_count = int(len(samples) * splits[\"Evidence\"])\n",
    "\n",
    "    split_samples = {\n",
    "        \"Claim\": samples.iloc[:claim_count],\n",
    "        \"Evidence\": samples.iloc[claim_count: claim_count + evidence_count],\n",
    "        \"Both\": samples.iloc[claim_count + evidence_count:]\n",
    "    }\n",
    "\n",
    "    max_choices = config.AUGMENTATION_CONFIG[label][\"x_or_y\"][\"max_choices\"]\n",
    "    claim_num_words_to_augment = config.AUGMENTATION_CONFIG[label][\"x_or_y\"][\"num_words_to_augment\"][\"Claim\"]\n",
    "    evidence_num_words_to_augment = config.AUGMENTATION_CONFIG[label][\"x_or_y\"][\"num_words_to_augment\"][\"Evidence\"]\n",
    "\n",
    "    for text_type, sample in split_samples.items():\n",
    "        if text_type == \"Claim\":\n",
    "            augmenter = XorYAugmenter(sample, max_choices=max_choices, num_words_to_augment=claim_num_words_to_augment)\n",
    "            augmenter.augment_data(sample, augment_claim=True, augment_evidence=False)\n",
    "        elif text_type == \"Evidence\":\n",
    "            augmenter = XorYAugmenter(sample, max_choices=max_choices, num_words_to_augment=evidence_num_words_to_augment)\n",
    "            augmenter.augment_data(sample, augment_claim=False, augment_evidence=True)\n",
    "        elif text_type == \"Both\":\n",
    "            augmenter = XorYAugmenter(sample, max_choices=max_choices, num_words_to_augment=min(claim_num_words_to_augment, evidence_num_words_to_augment))\n",
    "            augmenter.augment_data(sample, augment_claim=True, augment_evidence=True)\n",
    "\n",
    "    aug_df.update(samples)\n",
    "    return aug_df\n",
    "\n",
    "\n",
    "async def main():\n",
    "    aug_df = pd.read_csv(config.TRAIN_FILE)\n",
    "    aug_path = config.AUG_TRAIN_FILE\n",
    "\n",
    "    # get label counts\n",
    "    label_counts = aug_df['label'].value_counts()\n",
    "    logging.info(f\"Label counts: {label_counts}\")\n",
    "\n",
    "    zeros_to_replace = int(label_counts[0] * config.AUGMENTATION_CONFIG[\"0\"][\"replace\"])\n",
    "    ones_to_replace = int(label_counts[1] * config.AUGMENTATION_CONFIG[\"1\"][\"replace\"])\n",
    "\n",
    "    zeros_to_add = int(label_counts[0] * config.AUGMENTATION_CONFIG[\"0\"][\"add\"])\n",
    "    ones_to_add = int(label_counts[1] * config.AUGMENTATION_CONFIG[\"1\"][\"add\"])\n",
    "\n",
    "    logging.info(f\"Zeros to replace: {zeros_to_replace}\")\n",
    "    logging.info(f\"Ones to replace: {ones_to_replace}\")\n",
    "    logging.info(f\"Zeros to add: {zeros_to_add}\")\n",
    "    logging.info(f\"Ones to add: {ones_to_add}\")\n",
    "\n",
    "    # get the indices of the zeros and ones to replace\n",
    "    zeros_to_replace_indices = generate_augmented_samples(aug_df[aug_df['label'] == 0], label_counts[0], zeros_to_replace)\n",
    "    ones_to_replace_indices = generate_augmented_samples(aug_df[aug_df['label'] == 1], label_counts[1], ones_to_replace)\n",
    "    zeros_to_add_indices = generate_augmented_samples(aug_df[aug_df['label'] == 0], label_counts[0], zeros_to_add)\n",
    "    ones_to_add_indices = generate_augmented_samples(aug_df[aug_df['label'] == 1], label_counts[1], ones_to_add)\n",
    "\n",
    "    #generate addition df\n",
    "    ones_to_add_df = aug_df.iloc[ones_to_add_indices].reset_index(drop=True).copy()\n",
    "    zeros_to_add_df = aug_df.iloc[zeros_to_add_indices].reset_index(drop=True).copy()\n",
    "\n",
    "    # back translation\n",
    "    await back_translate_samples(zeros_to_add_df, \"0\")\n",
    "    await back_translate_samples(ones_to_add_df, \"1\")\n",
    "\n",
    "    # synonym replacement\n",
    "    synonym_replace_samples(zeros_to_add_df, \"0\")\n",
    "    synonym_replace_samples(ones_to_add_df, \"1\")\n",
    "\n",
    "    # x or y augmentation\n",
    "    x_or_y_augment_samples(zeros_to_add_df, \"0\")\n",
    "    x_or_y_augment_samples(ones_to_add_df, \"1\")    \n",
    "\n",
    "    aug_df = pd.concat([aug_df, zeros_to_add_df, ones_to_add_df])\n",
    "\n",
    "    aug_df.to_csv(aug_path, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.downloader import load as glove_embeddings_loader\n",
    "\n",
    "\n",
    "def get_memory_usage() -> float:\n",
    "    \"\"\"\n",
    "    Get current memory usage of the process.\n",
    "    \n",
    "    Returns:\n",
    "        Memory usage in megabytes (MB)\n",
    "    \"\"\"\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "@contextmanager\n",
    "def timer(name: str, logger):\n",
    "    \"\"\"\n",
    "    Context manager for timing code execution.\n",
    "    \n",
    "    Args:\n",
    "        name: Descriptive name for the operation being timed\n",
    "        logger: Logger object to output timing information\n",
    "    \n",
    "    Example:\n",
    "        with timer(\"Data processing\", logger):\n",
    "            process_data()\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        logger.info(f\"{name} completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Define cache directory and path\n",
    "CACHE_DIR = config.DATA_DIR.parent / \"cache\"\n",
    "EMBEDDINGS_CACHE_PATH = CACHE_DIR / 'glove_embeddings.pkl'\n",
    "\n",
    "def load_cached_embeddings(embedding_dim=300):\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings of specified dimension from cache if available, otherwise download and cache them.\n",
    "    \n",
    "    Args:\n",
    "        embedding_dim (int): Desired dimension for GloVe embeddings (50, 100, 200, or 300). Defaults to 300.\n",
    "    \n",
    "    Returns:\n",
    "        dict: GloVe word embeddings dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create cache directory if it doesn't exist\n",
    "    CACHE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    cache_path = CACHE_DIR / f'glove_embeddings_{embedding_dim}.pkl'\n",
    "    if cache_path.exists():\n",
    "        logging.info(f\"Loading GloVe embeddings from cache: {cache_path}\")\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            glove_embeddings = pickle.load(f)\n",
    "    else:\n",
    "        model_name = f'glove-wiki-gigaword-{embedding_dim}'\n",
    "        logging.info(f\"Downloading GloVe embeddings with model {model_name} (this might take a while)...\")\n",
    "        glove_embeddings = glove_embeddings_loader(model_name)\n",
    "        \n",
    "        # Cache the embeddings for future use\n",
    "        logging.info(f\"Caching GloVe embeddings to: {cache_path}\")\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(glove_embeddings, f)\n",
    "    \n",
    "    return glove_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_svm_data(data: pd.DataFrame, \n",
    "                    remove_stopwords: bool = True, \n",
    "                    lemmatize: bool = True, \n",
    "                    min_freq: int = 2, \n",
    "                    vocab_size: Optional[int] = None) -> Tuple[pd.DataFrame, np.ndarray, Set[str]]:\n",
    "    \"\"\"\n",
    "    Prepare text data for the SVM by cleaning, normalizing and vocabulary management.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame containing 'Claim' and 'Evidence' columns\n",
    "        remove_stopwords: Whether to remove common stopwords\n",
    "        lemmatize: Whether to apply lemmatization\n",
    "        min_freq: Minimum frequency for words to be included in vocabulary\n",
    "        vocab_size: Maximum vocabulary size (most frequent words kept)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - Processed DataFrame with added 'text' column\n",
    "            - NumPy array of labels\n",
    "            - Set of vocabulary words\n",
    "    \"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize text by lowercasing, removing punctuation,\n",
    "        and optionally removing stopwords and lemmatizing.\n",
    "        \"\"\"\n",
    "        text = text.lower().translate(translator)\n",
    "        # Normalize whitespace\n",
    "        text = \" \".join(text.split())\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            try:\n",
    "                # Keep important discourse markers and modal verbs\n",
    "                keep_words = {\n",
    "                    'because', 'since', 'therefore', 'hence', 'thus', 'although',\n",
    "                    'however', 'but', 'not', 'should', 'must', 'might', 'may',\n",
    "                    'could', 'would', 'against', 'between', 'before', 'after'\n",
    "                }\n",
    "                custom_stopwords = set(stopwords.words(\"english\")) - keep_words\n",
    "                \n",
    "                text = \" \".join([word for word in text.split() \n",
    "                               if word not in custom_stopwords])\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "        if lemmatize:\n",
    "            try:\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                words = text.split()\n",
    "                text = \" \".join([lemmatizer.lemmatize(word) for word in words])\n",
    "            except Exception:\n",
    "                pass\n",
    "        return text\n",
    "\n",
    "    # Build vocabulary from training data\n",
    "    train_samples = pd.concat([data['Claim'], data['Evidence']]).apply(clean_text)\n",
    "    all_words = [word for text in train_samples for word in text.split()]\n",
    "    word_counts = Counter(all_words)\n",
    "\n",
    "    # Filter words by minimum frequency and sort by frequency\n",
    "    filtered_words = [(word, count) for word, count in word_counts.items() if count >= min_freq]\n",
    "    sorted_words = sorted(filtered_words, key=lambda x: (-x[1], x[0]))\n",
    "    \n",
    "    # Apply vocabulary size limit if specified\n",
    "    if vocab_size is not None:\n",
    "        sorted_words = sorted_words[:vocab_size]\n",
    "    \n",
    "    vocab = {word for word, _ in sorted_words}\n",
    "\n",
    "    def replace_rare_words(text: str) -> str:\n",
    "        \"\"\"Replace words not in vocabulary with <UNK> token.\"\"\"\n",
    "        return ' '.join([word if word in vocab else '<UNK>' for word in text.split()])\n",
    "\n",
    "    # Process the data with UNK replacement\n",
    "    data['text'] = (\"Claim: \" + data['Claim'].apply(clean_text).apply(replace_rare_words) + \n",
    "                    \" [SEP] \" + \"Evidence: \" + data['Evidence'].apply(clean_text).apply(replace_rare_words))\n",
    "\n",
    "    # Extract labels\n",
    "    labels = data['label'].values\n",
    "\n",
    "    return data, labels, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class GloveVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A vectorizer that combines GloVe word embeddings with positional encoding.\n",
    "    \n",
    "    This vectorizer transforms text into fixed-size vectors by:\n",
    "    1. Converting words to GloVe embeddings\n",
    "    2. Applying TF-IDF weighting (optional)\n",
    "    3. Adding positional encoding information\n",
    "    4. Computing interaction features between claim and evidence\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sep_token: str = '[SEP]', use_tfidf_weighting=True, vocabulary=None, \n",
    "                 embedding_dim=300, ngram_range=(1,1), min_df=2, max_df=0.95):\n",
    "        \"\"\"\n",
    "        Initialize the GloveVectorizer.\n",
    "        \n",
    "        Args:\n",
    "            sep_token: Token used to separate claim and evidence. Defaults to '[SEP]'.\n",
    "            use_tfidf_weighting: Whether to use TF-IDF weights for word embeddings. Defaults to True.\n",
    "            vocabulary: Set of words to include in the vocabulary.\n",
    "            embedding_dim: Desired embedding dimension.\n",
    "            ngram_range: The lower and upper boundary of the n-grams to be extracted.\n",
    "            min_df: Minimum document frequency for TF-IDF.\n",
    "            max_df: Maximum document frequency for TF-IDF.\n",
    "        \"\"\"\n",
    "        self.glove = load_cached_embeddings(embedding_dim)\n",
    "        self.vector_size = embedding_dim\n",
    "        self.sep_token = sep_token\n",
    "        self.use_tfidf_weighting = use_tfidf_weighting\n",
    "        self.vocabulary = vocabulary or set()\n",
    "        self.ngram_range = ngram_range\n",
    "        \n",
    "        # Initialize TF-IDF vectorizer if weighting is enabled\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "            min_df=min_df, \n",
    "            max_df=max_df,\n",
    "            vocabulary=self.vocabulary,\n",
    "            max_features=len(self.vocabulary) if self.vocabulary else None,\n",
    "            ngram_range=self.ngram_range\n",
    "        ) if use_tfidf_weighting else None\n",
    "        \n",
    "    @staticmethod\n",
    "    def _pre_process(doc: str) -> str:\n",
    "        \"\"\"\n",
    "        Pre-process text by removing unrepresentable characters and quotes.\n",
    "        \n",
    "        Args:\n",
    "            doc: Input text document\n",
    "            \n",
    "        Returns:\n",
    "            Pre-processed text with ASCII-only characters and no leading/trailing quotes\n",
    "        \"\"\"\n",
    "        # Remove any unrepresentable characters\n",
    "        doc = doc.encode('ascii', 'ignore').decode('ascii')\n",
    "        # Remove any double quotes at the beginning and end of the document\n",
    "        doc = doc.strip('\"')\n",
    "        return doc\n",
    "    \n",
    "    def _get_weighted_vector(self, text: str, tfidf_weights=None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute weighted average of word vectors using vocabulary words.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            tfidf_weights: Dictionary mapping words to their TF-IDF weights\n",
    "            \n",
    "        Returns:\n",
    "            Weighted average vector of the input text's words\n",
    "        \"\"\"\n",
    "        # Replace OOV words with UNK before processing\n",
    "        words = [word if word in self.vocabulary else '<UNK>' for word in text.split()]\n",
    "        \n",
    "        if not words:\n",
    "            return np.zeros(self.vector_size)\n",
    "            \n",
    "        # Restrict to vocabulary words\n",
    "        valid_words = [word for word in words if word in self.vocabulary]\n",
    "        \n",
    "        if self.use_tfidf_weighting and tfidf_weights:\n",
    "            vectors = []\n",
    "            weights = []\n",
    "            for word in valid_words:\n",
    "                if word in self.glove:\n",
    "                    vectors.append(self.glove[word])\n",
    "                    weights.append(tfidf_weights.get(word, 1.0))  # Default weight=1 if not in TF-IDF\n",
    "            \n",
    "            if vectors:\n",
    "                weights = np.array(weights) / np.sum(weights)  # Normalize weights\n",
    "                return np.average(vectors, axis=0, weights=weights)\n",
    "        \n",
    "        # Fallback to regular mean if no weights or no matching words\n",
    "        vectors = [self.glove[word] for word in valid_words if word in self.glove]\n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)\n",
    "            \n",
    "        return np.zeros(self.vector_size)\n",
    "    \n",
    "    def _get_positional_encoding(self, max_len: int, d_model: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate sinusoidal positional encoding matrix.\n",
    "        \n",
    "        Implements the positional encoding from \"Attention Is All You Need\" (Vaswani et al., 2017).\n",
    "        The encoding allows the model to learn to attend by relative positions.\n",
    "        \n",
    "        Args:\n",
    "            max_len: Maximum sequence length to encode\n",
    "            d_model: Dimensionality of the model/embeddings\n",
    "            \n",
    "        Returns:\n",
    "            A matrix of shape (max_len, d_model) with positional encodings.\n",
    "        \"\"\"\n",
    "        # Pre-compute position and dimension arrays\n",
    "        positions = np.arange(max_len)[:, np.newaxis]  # Shape: (max_len, 1)\n",
    "        \n",
    "        # Create the div_term with proper shape for broadcasting\n",
    "        div_term = np.exp(-(np.log(10000.0) / d_model) * np.arange(0, d_model, 2))\n",
    "        \n",
    "        # Initialize the encoding matrix\n",
    "        pe = np.zeros((max_len, d_model))\n",
    "        \n",
    "        # Set even indices to sine, odd indices to cosine\n",
    "        pe[:, 0::2] = np.sin(positions * div_term)\n",
    "        if d_model > 1:  # Handle case where d_model might be 1\n",
    "            pe[:, 1::2] = np.cos(positions * div_term[:pe.shape[1]//2])\n",
    "            \n",
    "        return pe\n",
    "\n",
    "    def _extract_positional_features(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract features using sinusoidal positional encoding.\n",
    "        \n",
    "        Applies positional encoding to word vectors to capture sequential information\n",
    "        in the input text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text string\n",
    "            \n",
    "        Returns:\n",
    "            A vector of size self.vector_size with positionally encoded features\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        if not words:\n",
    "            return np.zeros(self.vector_size)\n",
    "            \n",
    "        # Get word vectors for words that exist in the embeddings\n",
    "        word_vectors = []\n",
    "        for word in words:\n",
    "            if word in self.glove:\n",
    "                word_vectors.append(self.glove[word])\n",
    "        \n",
    "        if not word_vectors:\n",
    "            return np.zeros(self.vector_size)\n",
    "            \n",
    "        # Stack word vectors into a matrix: shape (sequence_length, embedding_dim)\n",
    "        word_vectors = np.stack(word_vectors)\n",
    "        sequence_length = word_vectors.shape[0]\n",
    "        \n",
    "        # Generate positional encoding of appropriate size\n",
    "        pe = self._get_positional_encoding(sequence_length, self.vector_size)\n",
    "        \n",
    "        # Apply positional encoding to word vectors\n",
    "        positionally_encoded = word_vectors + pe[:sequence_length]\n",
    "        \n",
    "        # Return mean of positionally encoded vectors\n",
    "        return np.mean(positionally_encoded, axis=0)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the vectorizer by preparing TF-IDF weights if enabled.\n",
    "        \n",
    "        Args:\n",
    "            X: Training data. Each element should be a string containing\n",
    "               claim and evidence separated by self.sep_token.\n",
    "            y: Target values. Not used in this vectorizer.\n",
    "            \n",
    "        Returns:\n",
    "            self: Returns the instance itself.\n",
    "        \"\"\"\n",
    "        if self.use_tfidf_weighting:\n",
    "            # Fit TF-IDF vectorizer on all texts\n",
    "            self.tfidf_vectorizer.fit([doc.replace(self.sep_token, \" \") for doc in X])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the input data into feature vectors.\n",
    "        \n",
    "        For each input text, this method:\n",
    "        1. Splits the text into claim and evidence\n",
    "        2. Computes weighted word embeddings for both parts\n",
    "        3. Adds positional encoding information\n",
    "        4. Computes interaction features between claim and evidence\n",
    "        \n",
    "        Args:\n",
    "            X: Input data. Each element should be a string containing\n",
    "               claim and evidence separated by self.sep_token.\n",
    "            \n",
    "        Returns:\n",
    "            Feature matrix with claim, evidence, positional, and interaction features\n",
    "        \"\"\"\n",
    "        doc_vectors = []\n",
    "        \n",
    "        # Compute TF-IDF for all documents if using weighting\n",
    "        tfidf_weights_dict = {}\n",
    "        if self.use_tfidf_weighting:\n",
    "            # Get TF-IDF vocabulary and weights\n",
    "            vocabulary = self.tfidf_vectorizer.vocabulary_\n",
    "            idf = self.tfidf_vectorizer.idf_\n",
    "            \n",
    "            # Create a lookup dictionary for word -> tfidf weight\n",
    "            tfidf_weights_dict = {word: idf[idx] for word, idx in vocabulary.items()}\n",
    "        \n",
    "        for doc in X:\n",
    "            # Split on [SEP] token to separate claim and evidence\n",
    "            try:\n",
    "                claim, evidence = doc.split(self.sep_token)\n",
    "            except ValueError as ve:\n",
    "                raise ValueError(f\"Document splitting error: Expected 2 parts separated by '{self.sep_token}', but got an error: {ve}\")\n",
    "            \n",
    "            # Pre-process the claim and evidence\n",
    "            claim = self._pre_process(claim)\n",
    "            evidence = self._pre_process(evidence)\n",
    "            \n",
    "            # Get weighted vectors for claim and evidence\n",
    "            claim_vector = self._get_weighted_vector(claim, tfidf_weights_dict)\n",
    "            evidence_vector = self._get_weighted_vector(evidence, tfidf_weights_dict)\n",
    "            \n",
    "            # Get positional features\n",
    "            claim_pos_features = self._extract_positional_features(claim)\n",
    "            evidence_pos_features = self._extract_positional_features(evidence)\n",
    "            \n",
    "            # Prepare interaction features\n",
    "            element_wise_product = claim_vector * evidence_vector\n",
    "            absolute_difference = np.abs(claim_vector - evidence_vector)\n",
    "            \n",
    "            # Concatenate all features\n",
    "            doc_vectors.append(np.concatenate([\n",
    "                claim_vector, \n",
    "                evidence_vector,\n",
    "                claim_pos_features,\n",
    "                evidence_pos_features,\n",
    "                element_wise_product,\n",
    "                absolute_difference\n",
    "            ]))\n",
    "            \n",
    "        return np.array(doc_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    A feature extractor that combines various text-based features for evidence detection.\n",
    "    \n",
    "    This extractor computes a rich set of features including:\n",
    "    1. Basic text statistics (lengths, word counts, etc.)\n",
    "    2. Sentiment analysis features using VADER\n",
    "    3. Text characteristics (capitalization, punctuation, digits)\n",
    "    4. TF-IDF based similarity between claim and evidence\n",
    "    \n",
    "    The features are designed to capture both semantic and structural aspects\n",
    "    of the text, which are important for evidence detection tasks.\n",
    "    \n",
    "    Attributes:\n",
    "        sentiment_analyzer (SentimentIntensityAnalyzer): VADER sentiment analyzer\n",
    "        tfidf (TfidfVectorizer): TF-IDF vectorizer for computing text similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the FeatureExtractor.\n",
    "        \n",
    "        Downloads required NLTK resources if not already present:\n",
    "        - vader_lexicon: For sentiment analysis\n",
    "        - punkt: For text tokenization\n",
    "        \"\"\"\n",
    "        nltk.download('vader_lexicon')\n",
    "        nltk.download('punkt')\n",
    "        self.sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "        self.tfidf = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform input texts into feature vectors.\n",
    "        \n",
    "        For each input text (containing claim and evidence), computes:\n",
    "        1. Basic text statistics:\n",
    "           - Word overlap between claim and evidence\n",
    "        \n",
    "        2. Sentiment features:\n",
    "           - Negative, neutral, positive scores for both claim and evidence\n",
    "           - Compound sentiment scores\n",
    "           - Absolute difference in sentiment between claim and evidence\n",
    "        \n",
    "        3. TF-IDF similarity between claim and evidence\n",
    "        \n",
    "        Args:\n",
    "            X (array-like): Input texts. Each element should be a string containing\n",
    "                          claim and evidence separated by '[SEP]'.\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Feature matrix with sentiment and similarity features.\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for text in X:\n",
    "            claim, evidence = text.split(\"[SEP]\")\n",
    "            \n",
    "            # Extract sentiment features\n",
    "            claim_sentiments = self.sentiment_analyzer.polarity_scores(claim)\n",
    "            evidence_sentiments = self.sentiment_analyzer.polarity_scores(evidence)\n",
    "            \n",
    "            # Create feature dictionary\n",
    "            feature_dict = {\n",
    "                'word_overlap': len(set(claim.split()) & set(evidence.split())),\n",
    "                'claim_sentiment_neg': claim_sentiments['neg'],\n",
    "                'claim_sentiment_neu': claim_sentiments['neu'],\n",
    "                'claim_sentiment_pos': claim_sentiments['pos'],\n",
    "                'claim_sentiment_compound': claim_sentiments['compound'],\n",
    "                'evidence_sentiment_neg': evidence_sentiments['neg'],\n",
    "                'evidence_sentiment_neu': evidence_sentiments['neu'],\n",
    "                'evidence_sentiment_pos': evidence_sentiments['pos'],\n",
    "                'evidence_sentiment_compound': evidence_sentiments['compound'],\n",
    "                'sentiment_diff': abs(claim_sentiments['compound'] - evidence_sentiments['compound'])\n",
    "            }\n",
    "            \n",
    "            # Calculate TF-IDF similarity\n",
    "            claim_tfidf = self.tfidf.transform([claim]).toarray()[0]\n",
    "            evidence_tfidf = self.tfidf.transform([evidence]).toarray()[0]\n",
    "            \n",
    "            # Calculate cosine similarity only if vectors are non-zero\n",
    "            if np.sum(claim_tfidf) > 0 and np.sum(evidence_tfidf) > 0:\n",
    "                tfidf_similarity = 1 - cosine(claim_tfidf, evidence_tfidf)\n",
    "            else:\n",
    "                tfidf_similarity = 0\n",
    "                \n",
    "            feature_dict['tfidf_similarity'] = tfidf_similarity\n",
    "            features.append(feature_dict)\n",
    "            \n",
    "        return pd.DataFrame(features)    \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the feature extractor by preparing TF-IDF weights.\n",
    "        \n",
    "        This method fits the TF-IDF vectorizer on all claims and evidence texts\n",
    "        to prepare for computing similarity features during transform.\n",
    "        \n",
    "        Args:\n",
    "            X (array-like): Training data. Each element should be a string containing\n",
    "                          claim and evidence separated by '[SEP]'.\n",
    "            y (array-like, optional): Target values. Not used in this extractor.\n",
    "            \n",
    "        Returns:\n",
    "            self: Returns the instance itself.\n",
    "        \"\"\"\n",
    "        # Extract all texts for TF-IDF fitting\n",
    "        all_texts = []\n",
    "        for text in X:\n",
    "            claim, evidence = text.split(\"[SEP]\")\n",
    "            all_texts.append(claim)\n",
    "            all_texts.append(evidence)\n",
    "        \n",
    "        # Fit TF-IDF on all texts\n",
    "        self.tfidf.fit(all_texts)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning/Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration Constants\n",
    "NUM_TRIALS = 100\n",
    "TRAIN_SUBSET_FRACTION = 0.4  # Use 40% of training data for faster iteration\n",
    "\n",
    "# Load and prepare initial data\n",
    "initial_memory = get_memory_usage()\n",
    "logger.info(f\"Initial memory usage: {initial_memory:.2f} MB\")\n",
    "\n",
    "train_df_raw = pd.read_csv(config.AUG_TRAIN_FILE)\n",
    "dev_df_raw = pd.read_csv(config.DEV_FILE)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_all_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate comprehensive evaluation metrics for classification.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Array of true labels\n",
    "        y_pred: Array of predicted labels\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing accuracy, precision, recall, F1-score, and MCC metrics\n",
    "    \"\"\"\n",
    "    # Basic accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate precision, recall, f1 (macro)\n",
    "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro'\n",
    "    )\n",
    "    \n",
    "    # Calculate precision, recall, f1 (weighted)\n",
    "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted'\n",
    "    )\n",
    "    \n",
    "    # Matthews Correlation Coefficient\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Macro-P': macro_precision,\n",
    "        'Macro-R': macro_recall,\n",
    "        'Macro-F1': macro_f1,\n",
    "        'W Macro-P': weighted_precision,\n",
    "        'W Macro-R': weighted_recall,\n",
    "        'W Macro-F1': weighted_f1,\n",
    "        'MCC': mcc\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Create training subset for faster iteration if needed\n",
    "if TRAIN_SUBSET_FRACTION < 1.0:\n",
    "    train_samples = int(len(train_df_raw) * TRAIN_SUBSET_FRACTION)\n",
    "    logger.info(f\"Using {TRAIN_SUBSET_FRACTION*100:.0f}% of training data ({train_samples} samples)\")\n",
    "    \n",
    "    train_df_subset, _ = train_test_split(\n",
    "        train_df_raw, \n",
    "        train_size=TRAIN_SUBSET_FRACTION, \n",
    "        stratify=train_df_raw['label'],\n",
    "        random_state=42\n",
    "    )\n",
    "    logger.info(f\"Stratified subset created. Label distribution:\\n{train_df_subset['label'].value_counts(normalize=True)}\")\n",
    "else:\n",
    "    logger.info(\"Using 100% of the training data.\")\n",
    "    train_df_subset = train_df_raw\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Optuna objective function for hyperparameter optimization.\n",
    "    \n",
    "    Generates features, trains SVM model, and evaluates performance for each trial.\n",
    "    \n",
    "    Args:\n",
    "        trial: Current Optuna trial object\n",
    "        \n",
    "    Returns:\n",
    "        float: Weighted Macro F1 score on development set\n",
    "    \"\"\"\n",
    "    global train_df_subset, dev_df_raw\n",
    "    trial_number = trial.number\n",
    "    \n",
    "    logger.info(f\"\\n{'='*50}\\nStarting trial {trial_number}/{NUM_TRIALS}\\n{'='*50}\")\n",
    "    trial_start = time.time()\n",
    "    \n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        \"C\": trial.suggest_float(\"C\", 1.5, 2.5), # Focused range around C=2\n",
    "        \"vocab_size\": trial.suggest_int(\"vocab_size\", 10000, 20000, step=500), # Tune vocab size,\n",
    "        \"embedding_dim\": trial.suggest_categorical (\"embedding_dim\", [100, 200, 300]), # Tune embedding dimension\n",
    "        \"pca_components\": trial.suggest_int(\"pca_components\", 400, 600, step=10), # Tune PCA components\n",
    "        # \"min_df\": trial.suggest_int(\"min_df\", 1, 2),\n",
    "        # \"max_df\": trial.suggest_float(\"max_df\", 0.85, 1.0, step=0.05),\n",
    "    }\n",
    "        \n",
    "    # Data preparation for this trial\n",
    "    with timer(f\"Trial {trial_number} Data Prep\", logger):\n",
    "        train_df_trial, train_labels_trial, trial_vocab = prepare_svm_data(\n",
    "            train_df_subset.copy(), \n",
    "            remove_stopwords=True, \n",
    "            lemmatize=True, \n",
    "            min_freq=2, \n",
    "            vocab_size=params['vocab_size']\n",
    "        )\n",
    "        \n",
    "        dev_df_trial, dev_labels_trial, _ = prepare_svm_data(\n",
    "            dev_df_raw.copy(), \n",
    "            remove_stopwords=True, \n",
    "            lemmatize=True, \n",
    "            min_freq=2, \n",
    "            vocab_size=params['vocab_size']\n",
    "        ) \n",
    "        \n",
    "        train_texts_trial = train_df_trial['text'].tolist()\n",
    "        dev_texts_trial = dev_df_trial['text'].tolist()\n",
    "        logger.info(f\"  Trial vocab size: {len(trial_vocab)}\")\n",
    "    \n",
    "    # Create pipeline and train model\n",
    "    pipeline = create_pipeline_from_params(params, trial_vocab)\n",
    "    \n",
    "    with timer(f\"Trial {trial_number} training\", logger):\n",
    "        pipeline.fit(train_texts_trial, train_labels_trial)\n",
    "    \n",
    "    # Evaluate model\n",
    "    with timer(f\"Trial {trial_number} evaluation\", logger):\n",
    "        dev_preds = pipeline.predict(dev_texts_trial)\n",
    "        metrics = calculate_all_metrics(dev_labels_trial, dev_preds)\n",
    "    \n",
    "    # Save trial results\n",
    "    svm_dir = config.SAVE_DIR / \"svm\"\n",
    "    svm_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with (svm_dir / f'svm_{trial_number}.json').open('w') as f:\n",
    "        serializable_params = {k: (float(v) if isinstance(v, np.floating) else v) for k, v in params.items()}\n",
    "        serializable_metrics = {k: (float(v) if isinstance(v, np.floating) else v) for k, v in metrics.items()}\n",
    "        json.dump({**serializable_metrics, **serializable_params}, f)\n",
    "    \n",
    "    trial_duration = time.time() - trial_start\n",
    "    logger.info(f\"Trial {trial_number} completed in {trial_duration:.2f} seconds\")\n",
    "    logger.info(f\"Trial {trial_number} results: W Macro-F1 = {metrics['W Macro-F1']:.4f}\")\n",
    "    \n",
    "    # Free memory\n",
    "    gc.collect()\n",
    "    return metrics[\"W Macro-F1\"]\n",
    "\n",
    "\n",
    "def create_pipeline_from_params(params: Dict, vocabulary: List[str]) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Create scikit-learn pipeline for SVM classification.\n",
    "    \n",
    "    Builds a pipeline with feature extraction, scaling, dimensionality reduction,\n",
    "    and SVM classification components based on specified hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        params: Dictionary of hyperparameters\n",
    "        vocabulary: List of vocabulary terms to use in vectorization\n",
    "        \n",
    "    Returns:\n",
    "        Pipeline: Scikit-learn pipeline for text classification\n",
    "    \"\"\"\n",
    "    pipeline_steps = []\n",
    "    \n",
    "    # Feature extraction component\n",
    "    pipeline_steps.append(('glove_feature_union', FeatureUnion([\n",
    "        ('glove', GloveVectorizer(\n",
    "            use_tfidf_weighting=True,\n",
    "            vocabulary=vocabulary,\n",
    "            embedding_dim=params['embedding_dim'],\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=1,\n",
    "            max_df=0.95\n",
    "        )),\n",
    "        ('feature_extractor', FeatureExtractor())\n",
    "    ])))\n",
    "    \n",
    "    # Feature scaling and dimensionality reduction\n",
    "    pipeline_steps.append(('scaler', StandardScaler()))\n",
    "    pipeline_steps.append(('pca', PCA(n_components=params['pca_components'])))\n",
    "    \n",
    "    # SVM classifier with RBF kernel\n",
    "    pipeline_steps.append(('svm', SVC(\n",
    "        C=params['C'],\n",
    "        kernel='rbf',\n",
    "        gamma='scale',\n",
    "        probability=False,\n",
    "        random_state=42\n",
    "    )))\n",
    "    \n",
    "    return Pipeline(pipeline_steps)\n",
    "\n",
    "\n",
    "def hyperparameter_tuning(show_plots: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning using Optuna.\n",
    "    \n",
    "    Configures and runs an Optuna study to optimize hyperparameters for the SVM model.\n",
    "    \n",
    "    Args:\n",
    "        show_plots: Whether to display optimization visualizations\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Best hyperparameters found during optimization\n",
    "    \"\"\"\n",
    "    logger.info(f\"Running {NUM_TRIALS} hyperparameter optimization trials...\")\n",
    "    \n",
    "    # Configure Optuna sampler and pruner\n",
    "    sampler = TPESampler(\n",
    "        seed=42,\n",
    "        n_startup_trials=int(NUM_TRIALS / 10),\n",
    "        multivariate=True,\n",
    "        constant_liar=True\n",
    "    )\n",
    "    pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=5, interval_steps=2)\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=sampler,\n",
    "        pruner=pruner,\n",
    "        study_name='svm_evidence_detection_streamlined'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        with timer(\"Hyperparameter optimization\", logger):\n",
    "            study.optimize(objective, n_trials=NUM_TRIALS, n_jobs=8)\n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"Hyperparameter tuning interrupted by user.\")\n",
    "    \n",
    "    # Log best trial results\n",
    "    if not study.trials:\n",
    "        logger.error(\"No trials completed. Exiting.\")\n",
    "        return {}\n",
    "        \n",
    "    trial = study.best_trial\n",
    "    logger.info(\"\\nBest trial:\")\n",
    "    logger.info(f\"  Value (W Macro-F1): {trial.value:.4f}\")\n",
    "    logger.info(\"  Params:\")\n",
    "    \n",
    "    for key, value in trial.params.items():\n",
    "        logger.info(f\"    {key}: {value}\")\n",
    "\n",
    "    # Generate and display plots\n",
    "    if show_plots:\n",
    "        try:\n",
    "            logger.info(\"Generating Optuna trial plots...\")\n",
    "            history_fig = optuna.visualization.plot_optimization_history(study)\n",
    "            show(history_fig)\n",
    "            importance_fig = optuna.visualization.plot_param_importances(study)\n",
    "            show(importance_fig)\n",
    "            heatmap_fig = optuna.visualization.plot_contour(study)\n",
    "            show(heatmap_fig)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to generate plots: {e}\")\n",
    "\n",
    "    return trial.params\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main execution function for SVM model training and optimization.\n",
    "    \n",
    "    Performs hyperparameter tuning, trains the final model with optimal \n",
    "    parameters on full dataset, and saves the model.\n",
    "    \"\"\"\n",
    "    global train_df_raw, dev_df_raw\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\"*70)\n",
    "    logger.info(\"EVIDENCE DETECTION SVM MODEL TRAINING\")\n",
    "    logger.info(\"=\"*70)\n",
    "    \n",
    "    # Find optimal hyperparameters\n",
    "    params = hyperparameter_tuning(show_plots=True)\n",
    "    \n",
    "    # Process data with optimal parameters\n",
    "    train_df_processed, train_labels, best_vocab = prepare_svm_data(\n",
    "        train_df_raw, \n",
    "        remove_stopwords=True, \n",
    "        lemmatize=True, \n",
    "        min_freq=2, \n",
    "        vocab_size=params['vocab_size']\n",
    "    )\n",
    "    \n",
    "    dev_df_processed, dev_labels, _ = prepare_svm_data(\n",
    "        dev_df_raw, \n",
    "        remove_stopwords=True, \n",
    "        lemmatize=True, \n",
    "        min_freq=2, \n",
    "        vocab_size=params['vocab_size']\n",
    "    )\n",
    "\n",
    "    # Train and evaluate final model\n",
    "    logger.info(\"\\n\" + \"=\"*70)\n",
    "    logger.info(\"TRAINING FINAL FULL MODEL\")\n",
    "    logger.info(\"=\"*70)\n",
    "\n",
    "    pipeline = create_pipeline_from_params(params, best_vocab)\n",
    "    pipeline.fit(train_df_processed['text'], train_labels)\n",
    "    dev_preds = pipeline.predict(dev_df_processed['text'])\n",
    "    metrics = calculate_all_metrics(dev_labels, dev_preds)\n",
    "    logger.info(f\"Final model evaluation: {metrics}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    pipeline_pickle_path = config.SAVE_DIR / \"svm\" / \"svm_pipeline.pkl\"\n",
    "    try:\n",
    "        with open(pipeline_pickle_path, \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "        logger.info(f\"Pipeline successfully saved to {pipeline_pickle_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving pipeline: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    matthews_corrcoef,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics for classification.\n",
    "    \n",
    "    Args:\n",
    "        eval_pred: Tuple of predictions and labels\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of metrics including accuracy, precision, recall, F1, and MCC\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "    \n",
    "    metrics = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Positive_Precision': precision[1] if len(precision) > 1 else 0,\n",
    "        'Positive_Recall': recall[1] if len(recall) > 1 else 0,\n",
    "        'Positive_F1': f1[1] if len(f1) > 1 else 0,\n",
    "        'W Macro-P': weighted_precision,\n",
    "        'W Macro-R': weighted_recall,\n",
    "        'W Macro-F1': weighted_f1,\n",
    "        'MCC': mcc\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, save_path):\n",
    "    \"\"\"\n",
    "    Plot and save confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        save_path: Path to save the confusion matrix plot\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    classes = ['Negative', 'Positive']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, f'{cm[i, j]}\\n({cm_norm[i, j]:.2f})',\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6854185,
     "sourceId": 11009344,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
