{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willi\\Desktop\\Uni\\3rd_year\\NLU\\comp38412-not-broken\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Hyperparameter tuning\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# Embeddings\n",
    "import gensim.downloader as api\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path configuration\n",
    "DATA_DIR = Path('../../data')\n",
    "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
    "AUGMENTED_DATA_PATH = DATA_DIR / 'train_augmented.csv'\n",
    "DEV_PATH = DATA_DIR / 'dev.csv'\n",
    "MODEL_SAVE_PATH = Path('./models')\n",
    "MODEL_SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optuna config\n",
    "N_TRIALS = 50  # Number of Optuna trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 13:10:46 - loading projection weights from C:\\Users\\willi/gensim-data\\glove-wiki-gigaword-300\\glove-wiki-gigaword-300.gz\n",
      "2025-03-27 13:13:25 - KeyedVectors lifecycle event {'msg': 'loaded (400000, 300) matrix of type float32 from C:\\\\Users\\\\willi/gensim-data\\\\glove-wiki-gigaword-300\\\\glove-wiki-gigaword-300.gz', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-03-27T13:13:25.780208', 'gensim': '4.3.3', 'python': '3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "glove_embeddings = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GloveVectorizer(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self, sep_token: str = '[SEP]'):\n",
    "#         self.glove = glove_embeddings\n",
    "#         self.vector_size = 300\n",
    "#         self.sep_token = sep_token\n",
    "        \n",
    "#     @staticmethod\n",
    "#     def _pre_process(doc: str) -> str:\n",
    "#         # Remove any unrepresentable characters\n",
    "#         doc = doc.encode('ascii', 'ignore').decode('ascii')\n",
    "#         # Remove any double quotes at the beginning and end of the document\n",
    "#         doc = doc.strip('\"')\n",
    "        \n",
    "#         return doc\n",
    "    \n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "    \n",
    "#     def transform(self, X):\n",
    "#         doc_vectors = []\n",
    "        \n",
    "#         for doc in X:\n",
    "#             # Split on [SEP] token to separate claim and evidence\n",
    "#             try:\n",
    "#                 claim, evidence = doc.split(self.sep_token)\n",
    "#             except ValueError as ve:\n",
    "#                 raise ValueError(f\"Document splitting error: Expected 2 parts separated by '{self.sep_token}', but got an error: {ve}\")\n",
    "#             except Exception as e:\n",
    "#                 raise Exception(f\"An unexpected error occurred while splitting the document: {e}\")\n",
    "            \n",
    "#             # Pre-process the claim and evidence\n",
    "#             claim = self._pre_process(claim)\n",
    "#             evidence = self._pre_process(evidence)\n",
    "            \n",
    "#             # Get vectors for claim\n",
    "#             claim_vectors = [self.glove[word] for word in claim.split() \n",
    "#                            if word in self.glove]\n",
    "            \n",
    "#             # Get vectors for evidence\n",
    "#             evidence_vectors = [self.glove[word] for word in evidence.split() \n",
    "#                               if word in self.glove]\n",
    "            \n",
    "#             # Combine vectors (if either part has no vectors, use empty list)\n",
    "#             combined_vectors = []\n",
    "#             if claim_vectors:\n",
    "#                 combined_vectors.extend(claim_vectors)\n",
    "#             if evidence_vectors:\n",
    "#                 combined_vectors.extend(evidence_vectors)\n",
    "                \n",
    "#             # If no vectors found, use zero vector\n",
    "#             if not combined_vectors:\n",
    "#                 combined_vectors = [np.zeros(self.vector_size)]\n",
    "            \n",
    "#             doc_vectors.append(combined_vectors)\n",
    "            \n",
    "#         # Pad sequences to same length (use longest document as reference)\n",
    "#         max_length = max(len(vectors) for vectors in doc_vectors)\n",
    "#         padded_vectors = []\n",
    "        \n",
    "#         for vectors in doc_vectors:\n",
    "#             # Pad with zero vectors if needed\n",
    "#             padding = [np.zeros(self.vector_size)] * (max_length - len(vectors))\n",
    "#             padded_vectors.append(vectors + padding)\n",
    "            \n",
    "#         return np.array(padded_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, sep_token: str = '[SEP]'):\n",
    "        self.glove = glove_embeddings\n",
    "        self.vector_size = 300\n",
    "        self.sep_token = sep_token\n",
    "        \n",
    "    @staticmethod\n",
    "    def _pre_process(doc: str) -> str:\n",
    "        # Remove any unrepresentable characters\n",
    "        doc = doc.encode('ascii', 'ignore').decode('ascii')\n",
    "        # Remove any double quotes at the beginning and end of the document\n",
    "        doc = doc.strip('\"')\n",
    "        return doc\n",
    "    \n",
    "    def _get_mean_vector(self, text: str) -> np.ndarray:\n",
    "        # Get vectors for all words in text and return their mean\n",
    "        vectors = [self.glove[word] for word in text.split() \n",
    "                  if word in self.glove]\n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        return np.zeros(self.vector_size)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        doc_vectors = []\n",
    "        \n",
    "        for doc in X:\n",
    "            # Split on [SEP] token to separate claim and evidence\n",
    "            try:\n",
    "                claim, evidence = doc.split(self.sep_token)\n",
    "            except ValueError as ve:\n",
    "                raise ValueError(f\"Document splitting error: Expected 2 parts separated by '{self.sep_token}', but got an error: {ve}\")\n",
    "            \n",
    "            # Pre-process the claim and evidence\n",
    "            claim = self._pre_process(claim)\n",
    "            evidence = self._pre_process(evidence)\n",
    "            \n",
    "            # Get mean vectors for claim and evidence\n",
    "            claim_vector = self._get_mean_vector(claim)\n",
    "            evidence_vector = self._get_mean_vector(evidence)\n",
    "            \n",
    "            # Concatenate claim and evidence vectors\n",
    "            doc_vectors.append(np.concatenate([claim_vector, evidence_vector]))\n",
    "            \n",
    "        return np.array(doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_df, aug_train_df, dev_df):\n",
    "    \"\"\"Prepare data for XGBoost training.\"\"\"\n",
    "    # Combine claim and evidence into a single text feature for TF-IDF\n",
    "    train_df['text'] = train_df['Claim'] + \" [SEP] \" + train_df['Evidence']\n",
    "    aug_train_df['text'] = aug_train_df['Claim'] + \" [SEP] \" + aug_train_df['Evidence']\n",
    "    dev_df['text'] = dev_df['Claim'] + \" [SEP] \" + dev_df['Evidence']\n",
    "    \n",
    "    # Extract labels\n",
    "    train_labels = train_df['label'].values\n",
    "    aug_train_labels = aug_train_df['label'].values\n",
    "    dev_labels = dev_df['label'].values\n",
    "    \n",
    "    # Combine the augmented training data with the original training data\n",
    "    train_df = pd.concat([train_df, aug_train_df])\n",
    "    train_labels = np.concatenate([train_labels, aug_train_labels])\n",
    "    \n",
    "    return train_df, dev_df, train_labels, dev_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive evaluation metrics.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing all metrics\n",
    "    \"\"\"\n",
    "    # Basic accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate precision, recall, f1 (macro)\n",
    "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro'\n",
    "    )\n",
    "    \n",
    "    # Calculate precision, recall, f1 (weighted)\n",
    "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted'\n",
    "    )\n",
    "    \n",
    "    # Matthews Correlation Coefficient\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Macro-P': macro_precision,\n",
    "        'Macro-R': macro_recall,\n",
    "        'Macro-F1': macro_f1,\n",
    "        'W Macro-P': weighted_precision,\n",
    "        'W Macro-R': weighted_recall,\n",
    "        'W Macro-F1': weighted_f1,\n",
    "        'MCC': mcc\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "aug_train_df = pd.read_csv(AUGMENTED_DATA_PATH)\n",
    "dev_df = pd.read_csv(DEV_PATH)\n",
    "   \n",
    "train_df, dev_df, train_labels, dev_labels = prepare_data(train_df, aug_train_df, dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):    \n",
    "    # XGBoost hyperparameters\n",
    "    xgb_params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'use_label_encoder': False,\n",
    "        'tree_method': 'hist',\n",
    "        'max_bin': 256,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    logging.info(f\"Training XGBoost with hyperparameters: {xgb_params}\")\n",
    "    \n",
    "    CHUNK_SIZE = 1000\n",
    "    \n",
    "    # Get embeddings for training data in chunks\n",
    "    X_train_chunks = []\n",
    "    for i in range(0, len(train_df), CHUNK_SIZE):\n",
    "        \n",
    "        chunk = train_df['text'].iloc[i:i + CHUNK_SIZE]\n",
    "        X_chunk = GloveVectorizer().fit_transform(chunk)\n",
    "        X_chunk = StandardScaler(with_mean=False).fit_transform(X_chunk)\n",
    "        \n",
    "        X_train_chunks.append(X_chunk)\n",
    "    \n",
    "    X_train = np.vstack(X_train_chunks)\n",
    "    del X_train_chunks\n",
    "    \n",
    "    # Process dev data\n",
    "    X_dev_chunks = []\n",
    "    for i in range(0, len(dev_df), CHUNK_SIZE):\n",
    "        chunk = dev_df['text'].iloc[i:i + CHUNK_SIZE]\n",
    "        \n",
    "        X_chunk = GloveVectorizer().transform(chunk)\n",
    "        X_chunk = StandardScaler(with_mean=False).fit_transform(X_chunk)\n",
    "        \n",
    "        X_dev_chunks.append(X_chunk)\n",
    "        \n",
    "    X_dev = np.vstack(X_dev_chunks)\n",
    "    del X_dev_chunks\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=train_labels, \n",
    "                        enable_categorical=True,\n",
    "                        nthread=-1)\n",
    "    ddev = xgb.DMatrix(X_dev, label=dev_labels,\n",
    "                       enable_categorical=True,\n",
    "                       nthread=-1)\n",
    "    del X_train, X_dev\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.train(\n",
    "        xgb_params,\n",
    "        dtrain,\n",
    "        num_boost_round=xgb_params['n_estimators'],\n",
    "        evals=[(ddev, 'eval')],\n",
    "        early_stopping_rounds=25,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    dev_preds = (model.predict(ddev) >= 0.5).astype(int)    \n",
    "    metrics = calculate_all_metrics(dev_labels, dev_preds)\n",
    "    \n",
    "    # Report intermediate values for pruning\n",
    "    trial.report(metrics['W Macro-F1'], step=model.best_iteration)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return metrics['W Macro-F1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 13:13:26,182] A new study created in memory with name: xgboost_evidence_detection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HYPERPARAMETER TUNING\n",
      "=====================\n",
      "Running 50 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 13:19:14,403] Trial 7 finished with value: 0.7935391242811073 and parameters: {'max_depth': 3, 'learning_rate': 0.10229155080951147, 'n_estimators': 125, 'min_child_weight': 4, 'gamma': 1.8876263249109634e-08, 'subsample': 0.8245508706032039, 'colsample_bytree': 0.8709318045473746, 'reg_alpha': 0.022362500447227703, 'reg_lambda': 4.574416947170543e-08}. Best is trial 7 with value: 0.7935391242811073.\n",
      "[I 2025-03-27 13:19:18,176] Trial 15 finished with value: 0.7981341165677212 and parameters: {'max_depth': 6, 'learning_rate': 0.263546115378612, 'n_estimators': 142, 'min_child_weight': 3, 'gamma': 2.000319653606339e-08, 'subsample': 0.8895640669209626, 'colsample_bytree': 0.7062913286508585, 'reg_alpha': 0.00025935398203485, 'reg_lambda': 0.005711922093594107}. Best is trial 15 with value: 0.7981341165677212.\n",
      "[I 2025-03-27 13:20:48,994] Trial 2 finished with value: 0.8039039428668125 and parameters: {'max_depth': 4, 'learning_rate': 0.21725135682260818, 'n_estimators': 382, 'min_child_weight': 2, 'gamma': 4.08602780421424e-07, 'subsample': 0.7773223123526056, 'colsample_bytree': 0.5435980007771217, 'reg_alpha': 3.222808794479277e-05, 'reg_lambda': 0.0032389748945330183}. Best is trial 2 with value: 0.8039039428668125.\n",
      "[I 2025-03-27 13:21:23,485] Trial 6 finished with value: 0.7252491594663226 and parameters: {'max_depth': 5, 'learning_rate': 0.007699579274632575, 'n_estimators': 163, 'min_child_weight': 2, 'gamma': 4.430695759166323e-06, 'subsample': 0.5214372338560979, 'colsample_bytree': 0.8374501037712119, 'reg_alpha': 1.382637276143058e-08, 'reg_lambda': 0.021578857105645558}. Best is trial 2 with value: 0.8039039428668125.\n",
      "[I 2025-03-27 13:22:30,778] Trial 12 finished with value: 0.7964278394745365 and parameters: {'max_depth': 5, 'learning_rate': 0.032637133450820206, 'n_estimators': 219, 'min_child_weight': 4, 'gamma': 3.6601655734864165e-05, 'subsample': 0.6446161755655659, 'colsample_bytree': 0.9720341787407543, 'reg_alpha': 0.0009901410901598836, 'reg_lambda': 4.663080708675215e-07}. Best is trial 2 with value: 0.8039039428668125.\n",
      "[I 2025-03-27 13:22:55,860] Trial 3 finished with value: 0.8086506690921221 and parameters: {'max_depth': 6, 'learning_rate': 0.10682475462746788, 'n_estimators': 195, 'min_child_weight': 1, 'gamma': 0.00013615600858326484, 'subsample': 0.7859969845196482, 'colsample_bytree': 0.9421024055019578, 'reg_alpha': 2.45822411618268e-07, 'reg_lambda': 9.037076734568544e-06}. Best is trial 3 with value: 0.8086506690921221.\n",
      "[I 2025-03-27 13:23:48,207] Trial 9 pruned. \n",
      "[I 2025-03-27 13:24:07,792] Trial 10 finished with value: 0.7770877779403182 and parameters: {'max_depth': 5, 'learning_rate': 0.013761581294597506, 'n_estimators': 278, 'min_child_weight': 4, 'gamma': 0.0013443694835778791, 'subsample': 0.9525961657516862, 'colsample_bytree': 0.733036025421149, 'reg_alpha': 0.03166939885260415, 'reg_lambda': 0.02563438555551125}. Best is trial 3 with value: 0.8086506690921221.\n",
      "[I 2025-03-27 13:24:19,670] Trial 8 finished with value: 0.8097585272316887 and parameters: {'max_depth': 7, 'learning_rate': 0.07638750131187294, 'n_estimators': 305, 'min_child_weight': 1, 'gamma': 0.006067689183945673, 'subsample': 0.9826353636929852, 'colsample_bytree': 0.5060351856626177, 'reg_alpha': 1.7660559470674775e-06, 'reg_lambda': 0.0007730066082416498}. Best is trial 8 with value: 0.8097585272316887.\n",
      "[I 2025-03-27 13:24:27,005] Trial 11 finished with value: 0.8092242154984836 and parameters: {'max_depth': 8, 'learning_rate': 0.060534983470867824, 'n_estimators': 175, 'min_child_weight': 5, 'gamma': 0.002620211146315936, 'subsample': 0.8132560280589054, 'colsample_bytree': 0.9915239317958403, 'reg_alpha': 0.00154223520333089, 'reg_lambda': 3.8199290792905904e-07}. Best is trial 8 with value: 0.8097585272316887.\n",
      "[I 2025-03-27 13:24:36,472] Trial 4 finished with value: 0.7966780592162209 and parameters: {'max_depth': 7, 'learning_rate': 0.020070024295699887, 'n_estimators': 219, 'min_child_weight': 7, 'gamma': 2.1095099421410084e-05, 'subsample': 0.993108815508187, 'colsample_bytree': 0.6994929941841012, 'reg_alpha': 0.00025815502738389006, 'reg_lambda': 0.5990718631690038}. Best is trial 8 with value: 0.8097585272316887.\n",
      "[I 2025-03-27 13:24:42,524] Trial 16 finished with value: 0.8034063267199936 and parameters: {'max_depth': 3, 'learning_rate': 0.14488451251565337, 'n_estimators': 339, 'min_child_weight': 7, 'gamma': 1.6898894939738729e-07, 'subsample': 0.783004302709893, 'colsample_bytree': 0.7258661855005817, 'reg_alpha': 1.5240478778163424e-07, 'reg_lambda': 0.0988406396738316}. Best is trial 8 with value: 0.8097585272316887.\n",
      "[I 2025-03-27 13:24:46,716] Trial 5 finished with value: 0.785473614291225 and parameters: {'max_depth': 4, 'learning_rate': 0.015735349984039824, 'n_estimators': 399, 'min_child_weight': 2, 'gamma': 0.001495284982050419, 'subsample': 0.57278384654067, 'colsample_bytree': 0.8963791300804544, 'reg_alpha': 0.001597254121613213, 'reg_lambda': 0.27440034173704275}. Best is trial 8 with value: 0.8097585272316887.\n",
      "[I 2025-03-27 13:25:29,369] Trial 20 finished with value: 0.7984243238649219 and parameters: {'max_depth': 4, 'learning_rate': 0.23157929406186584, 'n_estimators': 450, 'min_child_weight': 1, 'gamma': 3.403260429871204e-05, 'subsample': 0.7403805542855665, 'colsample_bytree': 0.5764014378084819, 'reg_alpha': 1.1590577306563171e-05, 'reg_lambda': 5.539386041494989e-05}. Best is trial 8 with value: 0.8097585272316887.\n",
      "[I 2025-03-27 13:25:35,984] Trial 1 finished with value: 0.7407477727134346 and parameters: {'max_depth': 8, 'learning_rate': 0.004279226862154913, 'n_estimators': 212, 'min_child_weight': 6, 'gamma': 0.0003133225125622649, 'subsample': 0.5435106788529802, 'colsample_bytree': 0.7667697941101581, 'reg_alpha': 0.00013840711117435892, 'reg_lambda': 0.0005217534689414938}. Best is trial 8 with value: 0.8097585272316887.\n",
      "[I 2025-03-27 13:25:40,326] Trial 14 finished with value: 0.7219439846232052 and parameters: {'max_depth': 4, 'learning_rate': 0.003547761039961172, 'n_estimators': 475, 'min_child_weight': 2, 'gamma': 1.790018751161866e-05, 'subsample': 0.7530817740929819, 'colsample_bytree': 0.8264647340201823, 'reg_alpha': 4.670628937954111e-08, 'reg_lambda': 0.0003034546723143161}. Best is trial 8 with value: 0.8097585272316887.\n",
      "[I 2025-03-27 13:25:44,493] Trial 19 finished with value: 0.7942083534065522 and parameters: {'max_depth': 4, 'learning_rate': 0.04896879581235719, 'n_estimators': 226, 'min_child_weight': 3, 'gamma': 7.32431215550253e-07, 'subsample': 0.9689430801461781, 'colsample_bytree': 0.9624539781765922, 'reg_alpha': 1.0065387877332081e-07, 'reg_lambda': 6.580697782524624e-05}. Best is trial 8 with value: 0.8097585272316887.\n",
      "[I 2025-03-27 13:25:52,225] Trial 17 finished with value: 0.7638484168032971 and parameters: {'max_depth': 4, 'learning_rate': 0.009648210318220158, 'n_estimators': 381, 'min_child_weight': 5, 'gamma': 1.2195956291524964e-05, 'subsample': 0.710724205412079, 'colsample_bytree': 0.5039253820086839, 'reg_alpha': 5.829558093137889e-08, 'reg_lambda': 6.022570229903908e-08}. Best is trial 8 with value: 0.8097585272316887.\n",
      "[I 2025-03-27 13:27:06,235] Trial 0 finished with value: 0.8014068700958975 and parameters: {'max_depth': 7, 'learning_rate': 0.012528119138783282, 'n_estimators': 442, 'min_child_weight': 6, 'gamma': 5.442009477865163e-05, 'subsample': 0.6370887878294709, 'colsample_bytree': 0.6043714075113622, 'reg_alpha': 0.0198919920777458, 'reg_lambda': 0.01915852530168318}. Best is trial 8 with value: 0.8097585272316887.\n",
      "[I 2025-03-27 13:27:59,124] Trial 18 finished with value: 0.8084859323708339 and parameters: {'max_depth': 6, 'learning_rate': 0.02634146979359095, 'n_estimators': 392, 'min_child_weight': 6, 'gamma': 2.112338827194885e-08, 'subsample': 0.7297551214938275, 'colsample_bytree': 0.6016803299493119, 'reg_alpha': 0.05464392992398258, 'reg_lambda': 0.00012847402068755185}. Best is trial 8 with value: 0.8097585272316887.\n",
      "[I 2025-03-27 13:28:35,624] Trial 13 finished with value: 0.7876784354950753 and parameters: {'max_depth': 8, 'learning_rate': 0.005829674558538086, 'n_estimators': 427, 'min_child_weight': 6, 'gamma': 0.002607823423981078, 'subsample': 0.5520575875926689, 'colsample_bytree': 0.8093600118560595, 'reg_alpha': 2.2406872810708822e-07, 'reg_lambda': 0.0011633288988562758}. Best is trial 8 with value: 0.8097585272316887.\n",
      "[I 2025-03-27 13:28:36,182] Trial 21 finished with value: 0.8051179371277134 and parameters: {'max_depth': 7, 'learning_rate': 0.06244324910146983, 'n_estimators': 301, 'min_child_weight': 1, 'gamma': 6.8199763365863065e-06, 'subsample': 0.8943061759662165, 'colsample_bytree': 0.948771848306232, 'reg_alpha': 3.09025146708365e-05, 'reg_lambda': 8.707019637481746e-06}. Best is trial 8 with value: 0.8097585272316887.\n",
      "[I 2025-03-27 13:29:18,624] Trial 22 finished with value: 0.8015839558841595 and parameters: {'max_depth': 7, 'learning_rate': 0.21715029464703361, 'n_estimators': 168, 'min_child_weight': 3, 'gamma': 0.005045343000100183, 'subsample': 0.7630005765396934, 'colsample_bytree': 0.7651705774119149, 'reg_alpha': 1.2545046845732514e-08, 'reg_lambda': 0.0005194690859245992}. Best is trial 8 with value: 0.8097585272316887.\n",
      "[I 2025-03-27 13:29:34,393] Trial 24 finished with value: 0.8047866158338562 and parameters: {'max_depth': 7, 'learning_rate': 0.0454241585715849, 'n_estimators': 246, 'min_child_weight': 3, 'gamma': 0.23634628673923957, 'subsample': 0.8907904891913984, 'colsample_bytree': 0.5323383484837555, 'reg_alpha': 7.522538726675199e-07, 'reg_lambda': 0.023535235398738644}. Best is trial 8 with value: 0.8097585272316887.\n",
      "[I 2025-03-27 13:30:42,373] Trial 25 finished with value: 0.8040715311906416 and parameters: {'max_depth': 8, 'learning_rate': 0.12200054857321323, 'n_estimators': 324, 'min_child_weight': 1, 'gamma': 0.00010323784254919183, 'subsample': 0.9265320361669882, 'colsample_bytree': 0.5760572812148432, 'reg_alpha': 1.0256809209661201e-07, 'reg_lambda': 3.6662015950875205e-07}. Best is trial 8 with value: 0.8097585272316887.\n",
      "[I 2025-03-27 13:31:53,471] Trial 27 finished with value: 0.8017237186698075 and parameters: {'max_depth': 8, 'learning_rate': 0.12008646417522396, 'n_estimators': 164, 'min_child_weight': 6, 'gamma': 0.3242697051561732, 'subsample': 0.5412281375748266, 'colsample_bytree': 0.878526546810062, 'reg_alpha': 0.00039921046293034406, 'reg_lambda': 1.093846279646213e-06}. Best is trial 8 with value: 0.8097585272316887.\n",
      "[I 2025-03-27 13:32:09,405] Trial 28 finished with value: 0.7970132639923093 and parameters: {'max_depth': 8, 'learning_rate': 0.2111436017321559, 'n_estimators': 195, 'min_child_weight': 7, 'gamma': 0.0006883708877957508, 'subsample': 0.856903590640134, 'colsample_bytree': 0.893929189695395, 'reg_alpha': 0.0011894100688688797, 'reg_lambda': 5.984704248580911e-05}. Best is trial 8 with value: 0.8097585272316887.\n",
      "[I 2025-03-27 13:32:17,480] Trial 23 finished with value: 0.8005956367583439 and parameters: {'max_depth': 4, 'learning_rate': 0.07927926389395602, 'n_estimators': 224, 'min_child_weight': 1, 'gamma': 0.014428465786158029, 'subsample': 0.9478966931052762, 'colsample_bytree': 0.9189132376331038, 'reg_alpha': 1.0677797111530778e-07, 'reg_lambda': 1.9889658802504888e-07}. Best is trial 8 with value: 0.8097585272316887.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nHYPERPARAMETER TUNING\")\n",
    "    print(\"=====================\")\n",
    "    print(f\"Running {N_TRIALS} trials...\")\n",
    "    \n",
    "    # Create a study with TPE sampler and MedianPruner\n",
    "    sampler = TPESampler(seed=42, \n",
    "                         n_startup_trials=int(N_TRIALS / 10), # First 10% of trials are random, then TPE\n",
    "                         multivariate=True, \n",
    "                         constant_liar=True) # constant_liar = True as we are doing distributed optimisation\n",
    "\n",
    "    pruner = MedianPruner(n_startup_trials=5, \n",
    "                          n_warmup_steps=5, \n",
    "                          interval_steps=2)\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=sampler,\n",
    "        pruner=pruner,\n",
    "        study_name='xgboost_evidence_detection'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        study.optimize(objective, n_trials=N_TRIALS, n_jobs=-1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Hyperparameter tuning interrupted.\")\n",
    "    \n",
    "    print(\"\\nBest trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"  Value (Accuracy): {trial.value}\")\n",
    "    print(\"  Params:\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = 0.796359627026355\n",
    "best_params = {\n",
    "    \"max_features\": 5000,\n",
    "    \"min_df\": 3,\n",
    "    \"ngram_range\": (1, 3),\n",
    "    \"max_depth\": 8,\n",
    "    \"learning_rate\": 0.22260998418525665,\n",
    "    \"n_estimators\": 666,\n",
    "    \"min_child_weight\": 2,\n",
    "    \"gamma\": 0.005341979637780388,\n",
    "    \"subsample\": 0.9900049863839708,\n",
    "    \"colsample_bytree\": 0.5918673659371315,\n",
    "    \"reg_alpha\": 6.834558057096682e-08,\n",
    "    \"reg_lambda\": 0.008776003604199037,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
